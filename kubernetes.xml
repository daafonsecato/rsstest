<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Kubernetes – Production-Grade Container Orchestration</title><link>https://kubernetes.io/</link><description>The Kubernetes project blog</description><generator>Hugo -- gohugo.io</generator><image><url>https://raw.githubusercontent.com/kubernetes/kubernetes/master/logo/logo.png</url><title>Kubernetes.io</title><link>https://kubernetes.io/</link></image><atom:link href="https://kubernetes.io/feed.xml" rel="self" type="application/rss+xml"/><item><title>Blog: Server Side Apply Is Great And You Should Be Using It</title><link>https://kubernetes.io/blog/2022/10/20/advanced-server-side-apply/</link><pubDate>Thu, 20 Oct 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/10/20/advanced-server-side-apply/</guid><description> <p><strong>Author:</strong> Daniel Smith (Google)</p> <p><a href="https://kubernetes.io/docs/reference/using-api/server-side-apply/">Server-side apply</a> (SSA) has now been <a href="https://kubernetes.io/blog/2021/08/06/server-side-apply-ga/">GA for a few releases</a>, and I have found myself in a number of conversations, recommending that people / teams in various situations use it. So I’d like to write down some of those reasons.</p> <h2 id="benefits">Obvious (and not-so-obvious) benefits of SSA</h2> <p>A list of improvements / niceties you get from switching from various things to Server-side apply!</p> <ul> <li>Versus client-side-apply (that is, plain <code>kubectl apply</code>): <ul> <li>The system gives you conflicts when you accidentally fight with another actor over the value of a field!</li> <li>When combined with <code>--dry-run</code>, there’s no chance of accidentally running a client-side dry run instead of a server side dry run.</li> </ul> </li> <li>Versus hand-rolling patches: <ul> <li>The SSA patch format is extremely natural to write, with no weird syntax. It’s just a regular object, but you can (and should) omit any field you don’t care about.</li> <li>The old patch format (“strategic merge patch”) was ad-hoc and still has some bugs; JSON-patch and JSON merge-patch fail to handle some cases that are common in the Kubernetes API, namely lists with items that should be recursively merged based on a “name” or other identifying field.</li> <li>There’s also now great <a href="https://kubernetes.io/blog/2021/08/06/server-side-apply-ga/#using-server-side-apply-in-a-controller">go-language library support</a> for building apply calls programmatically!</li> <li>You can use SSA to explicitly delete fields you don’t “own” by setting them to <code>null</code>, which makes it a feature-complete replacement for all of the old patch formats.</li> </ul> </li> <li>Versus shelling out to kubectl: <ul> <li>You can use the <strong>apply</strong> API call from any language without shelling out to kubectl!</li> <li>As stated above, the <a href="https://kubernetes.io/blog/2021/08/06/server-side-apply-ga/#server-side-apply-support-in-client-go">Go library has dedicated mechanisms</a> to make this easy now.</li> </ul> </li> <li>Versus GET-modify-PUT: <ul> <li>(This one is more complicated and you can skip it if you've never written a controller!)</li> <li>To use GET-modify-PUT correctly, you have to handle and retry a write failure in the case that someone else has modified the object in any way between your GET and PUT. This is an “optimistic concurrency failure” when it happens.</li> <li>SSA offloads this task to the server– you only have to retry if there’s a conflict, and the conflicts you can get are all meaningful, like when you’re actually trying to take a field away from another actor in the system.</li> <li>To put it another way, if 10 actors do a GET-modify-PUT cycle at the same time, 9 will get an optimistic concurrency failure and have to retry, then 8, etc, for up to 50 total GET-PUT attempts in the worst case (that’s .5N^2 GET and PUT calls for N actors making simultaneous changes). If the actors are using SSA instead, and the changes don’t actually conflict over specific fields, then all the changes can go in in any order. Additionally, SSA changes can often be done without a GET call at all. That’s only N <strong>apply</strong> requests for N actors, which is a drastic improvement!</li> </ul> </li> </ul> <h2 id="how-can-i-use-ssa">How can I use SSA?</h2> <h3 id="users">Users</h3> <p>Use <code>kubectl apply --server-side</code>! Soon we (SIG API Machinery) hope to make this the default and remove the “client side” apply completely!</p> <h3 id="controller-authors">Controller authors</h3> <p>There’s two main categories here, but for both of them, <strong>you should probably <em>force conflicts</em> when using SSA</strong>. This is because your controller probably doesn’t know what to do when some other entity in the system has a different desire than your controller about a particular field. (See the <a href="#ci-cd-systems">CI/CD section</a>, though!)</p> <h4 id="get-modify-put-patch-controllers">Controllers that use either a GET-modify-PUT sequence or a PATCH</h4> <p>This kind of controller GETs an object (possibly from a <a href="https://kubernetes.io/docs/reference/using-api/api-concepts/#efficient-detection-of-changes"><strong>watch</strong></a>), modifies it, and then PUTs it back to write its changes. Sometimes it constructs a custom PATCH, but the semantics are the same. Most existing controllers (especially those in-tree) work like this.</p> <p>If your controller is perfect, great! You don’t need to change it. But if you do want to change it, you can take advantage of the new client library’s <em>extract</em> workflow– that is, <strong>get</strong> the existing object, extract your existing desires, make modifications, and re-<strong>apply</strong>. For many controllers that were computing the smallest API changes possible, this will be a minor update to the existing implementation.</p> <p>This workflow avoids the failure mode of accidentally trying to own every field in the object, which is what happens if you just GET the object, make changes, and then <strong>apply</strong>. (Note that the server will notice you did this and reject your change!)</p> <h4 id="reconstructive-controllers">Reconstructive controllers</h4> <p>This kind of controller wasn't really possible prior to SSA. The idea here is to (whenever something changes etc) reconstruct from scratch the fields of the object as the controller wishes them to be, and then <strong>apply</strong> the change to the server, letting it figure out the result. I now recommend that new controllers start out this way–it's less fiddly to say what you want an object to look like than it is to say how you want it to change.</p> <p>The client library supports this method of operation by default.</p> <p>The only downside is that you may end up sending unneeded <strong>apply</strong> requests to the API server, even if actually the object already matches your controller’s desires. This doesn't matter if it happens once in a while, but for extremely high-throughput controllers, it might cause a performance problem for the cluster–specifically, the API server. No-op writes are not written to storage (etcd) or broadcast to any watchers, so it’s not really that big of a deal. If you’re worried about this anyway, today you could use the method explained in the previous section, or you could still do it this way for now, and wait for an additional client-side mechanism to suppress zero-change applies.</p> <p>To get around this downside, why not GET the object and only send your <strong>apply</strong> if the object needs it? Surprisingly, it doesn't help much – a no-op <strong>apply</strong> is not very much more work for the API server than an extra GET; and an <strong>apply</strong> that changes things is cheaper than that same <strong>apply</strong> with a preceding GET. Worse, since it is a distributed system, something could change between your GET and <strong>apply</strong>, invalidating your computation. Instead, you can use this optimization on an object retrieved from a cache–then it legitimately will reduce load on the system (at the cost of a delay when a change is needed and the cache is a bit behind).</p> <h4 id="ci-cd-systems">CI/CD systems</h4> <p>Continuous integration (CI) and/or continuous deployment (CD) systems are a special kind of controller which is doing something like reading manifests from source control (such as a Git repo) and automatically pushing them into the cluster. Perhaps the CI / CD process first generates manifests from a template, then runs some tests, and then deploys a change. Typically, users are the entities pushing changes into source control, although that’s not necessarily always the case.</p> <p>Some systems like this continuously reconcile with the cluster, others may only operate when a change is pushed to the source control system. The following considerations are important for both, but more so for the continuously reconciling kind.</p> <p>CI/CD systems are literally controllers, but for the purpose of <strong>apply</strong>, they are more like users, and unlike other controllers, they need to pay attention to conflicts. Reasoning:</p> <ul> <li>Abstractly, CI/CD systems can change anything, which means they could conflict with <strong>any</strong> controller out there. The recommendation that controllers force conflicts is assuming that controllers change a limited number of things and you can be reasonably sure that they won’t fight with other controllers about those things; that’s clearly not the case for CI/CD controllers.</li> <li>Concrete example: imagine the CI/CD system wants <code>.spec.replicas</code> for some Deployment to be 3, because that is the value that is checked into source code; however there is also a HorizontalPodAutoscaler (HPA) that targets the same deployment. The HPA computes a target scale and decides that there should be 10 replicas. Which should win? I just said that most controllers–including the HPA–should ignore conflicts. The HPA has no idea if it has been enabled incorrectly, and the HPA has no convenient way of informing users of errors.</li> <li>The other common cause of a CI/CD system getting a conflict is probably when it is trying to overwrite a hot-fix (hand-rolled patch) placed there by a system admin / SRE / dev-on-call. You almost certainly don’t want to override that automatically.</li> <li>Of course, sometimes SRE makes an accidental change, or a dev makes an unauthorized change – those you do want to notice and overwrite; however, the CI/CD system can’t tell the difference between these last two cases.</li> </ul> <p>Hopefully this convinces you that CI/CD systems need error paths–a way to back-propagate these conflict errors to humans; in fact, they should have this already, certainly continuous integration systems need some way to report that tests are failing. But maybe I can also say something about how <em>humans</em> can deal with errors:</p> <ul> <li> <p>Reject the hotfix: the (human) administrator of the CI/CD system observes the error, and manually force-applies the manifest in question. Then the CI/CD system will be able to apply the manifest successfully and become a co-owner.</p> <p>Optional: then the administrator applies a blank manifest (just the object type / namespace / name) to relinquish any fields they became a manager for. if this step is omitted, there's some chance the administrator will end up owning fields and causing an unwanted future conflict.</p> <p><strong>Note</strong>: why an administrator? I'm assuming that developers which ordinarily push to the CI/CD system and / or its source control system may not have permissions to push directly to the cluster.</p> </li> <li> <p>Accept the hotfix: the author of the change in question sees the conflict, and edits their change to accept the value running in production.</p> </li> <li> <p>Accept then reject: as in the accept option, but after that manifest is applied, and the CI/CD queue owns everything again (so no conflicts), re-apply the original manifest.</p> </li> <li> <p>I can also imagine the CI/CD system permitting you to mark a manifest as “force conflicts” somehow– if there’s demand for this we could consider making a more standardized way to do this. A rigorous version of this which lets you declare exactly which conflicts you intend to force would require support from the API server; in lieu of that, you can make a second manifest with only that subset of fields.</p> </li> <li> <p>Future work: we could imagine an especially advanced CI/CD system that could parse <code>metadata.managedFields</code> data to see who or what they are conflicting with, over what fields, and decide whether or not to ignore the conflict. In fact, this information is also presented in any conflict errors, though perhaps not in an easily machine-parseable format. We (SIG API Machinery) mostly didn't expect that people would want to take this approach — so we would love to know if in fact people want/need the features implied by this approach, such as the ability, when <strong>apply</strong>ing to request to override certain conflicts but not others.</p> <p>If this sounds like an approach you'd want to take for your own controller, come talk to SIG API Machinery!</p> </li> </ul> <p>Happy <strong>apply</strong>ing!</p></description></item><item><title>Blog: Current State: 2019 Third Party Security Audit of Kubernetes</title><link>https://kubernetes.io/blog/2022/10/05/current-state-2019-third-party-audit/</link><pubDate>Wed, 05 Oct 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/10/05/current-state-2019-third-party-audit/</guid><description> <p><strong>Authors</strong> (in alphabetical order): Cailyn Edwards (Shopify), Pushkar Joglekar (VMware), Rey Lejano (SUSE) and Rory McCune (DataDog)</p> <p>We expect the brand new Third Party Security Audit of Kubernetes will be published later this month (Oct 2022).</p> <p>In preparation for that, let's look at the state of findings that were made public as part of the last <a href="https://github.com/kubernetes/sig-security/tree/main/sig-security-external-audit/security-audit-2019">third party security audit of 2019</a> that was based on <a href="https://github.com/kubernetes/kubernetes/tree/release-1.13">Kubernetes v1.13.4</a>.</p> <h2 id="motivation">Motivation</h2> <p><a href="https://github.com/cji">Craig Ingram</a> has graciously attempted over the years to keep track of the status of the findings reported in the last audit in this issue: <a href="https://github.com/kubernetes/kubernetes/issues/81146">kubernetes/kubernetes#81146</a>. This blog post will attempt to dive deeper into this, address any gaps in tracking and become a point in time summary of the state of the findings reported from 2019.</p> <p>This article should also help readers gain confidence through transparent communication, of work done by the community to address these findings and bubble up any findings that need help from community contributors.</p> <h2 id="current-state">Current State</h2> <p>The status of each issue / finding here is represented in a best effort manner. Authors do not claim to be 100% accurate on the status and welcome any corrections or feedback if the current state is not reflected accurately by commenting directly on the relevant issue.</p> <table> <thead> <tr> <th><strong>#</strong></th> <th><strong>Title</strong></th> <th><strong>Issue</strong></th> <th><strong>Status</strong></th> </tr> </thead> <tbody> <tr> <td>1</td> <td>hostPath PersistentVolumes enable PodSecurityPolicy bypass</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81110">#81110</a></td> <td>closed, addressed by <a href="https://github.com/kubernetes/website/pull/15756">kubernetes/website#15756</a> and <a href="https://github.com/kubernetes/kubernetes/pull/109798">kubernetes/kubernetes#109798</a></td> </tr> <tr> <td>2</td> <td>Kubernetes does not facilitate certificate revocation</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81111">#81111</a></td> <td>duplicate of <a href="https://github.com/kubernetes/kubernetes/issues/18982">#18982</a> and <strong>needs a KEP</strong></td> </tr> <tr> <td>3</td> <td>HTTPS connections are not authenticated</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81112">#81112</a></td> <td>Largely left as an end user exercise in setting up the right configuration</td> </tr> <tr> <td>4</td> <td><abbr title="Time-of-check to time-of-use bug">TOCTOU</abbr> when moving PID to manager's cgroup via kubelet</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81113">#81113</a></td> <td>Requires Node access for successful exploitation. Fix needed</td> </tr> <tr> <td>5</td> <td>Improperly patched directory traversal in <code>kubectl cp</code></td> <td><a href="https://github.com/kubernetes/kubernetes/pull/76788">#76788</a></td> <td>closed, assigned <a href="https://github.com/advisories/GHSA-v8c4-hw4j-x4pr">CVE-2019-11249</a>, fixed in <a href="https://github.com/kubernetes/kubernetes/pull/80436">#80436</a></td> </tr> <tr> <td>6</td> <td>Bearer tokens are revealed in logs</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81114">#81114</a></td> <td>closed, assigned <a href="https://github.com/advisories/GHSA-jmrx-5g74-6v2f">CVE-2019-11250</a>, fixed in <a href="https://github.com/kubernetes/kubernetes/pull/81330">#81330</a></td> </tr> <tr> <td>7</td> <td>Seccomp is disabled by default</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81115">#81115</a></td> <td>closed, addressed by <a href="https://github.com/kubernetes/kubernetes/pull/101943">#101943</a></td> </tr> <tr> <td>8</td> <td>Pervasive world-accessible file permissions</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81116">#81116</a></td> <td><a href="https://github.com/kubernetes/kubernetes/pull/112384">#112384</a> ( in progress)</td> </tr> <tr> <td>9</td> <td>Environment variables expose sensitive data</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81117">#81117</a></td> <td>closed, addressed by <a href="https://github.com/kubernetes/kubernetes/pull/84992">#84992</a> and <a href="https://github.com/kubernetes/kubernetes/pull/84677">#84677</a></td> </tr> <tr> <td>10</td> <td>Use of InsecureIgnoreHostKey in SSH connections</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81118">#81118</a></td> <td>This feature was removed in v1.22: <a href="https://github.com/kubernetes/kubernetes/pull/102297">#102297</a></td> </tr> <tr> <td>11</td> <td>Use of InsecureSkipVerify and other TLS weaknesses</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81119">#81119</a></td> <td><strong>Needs a KEP</strong></td> </tr> <tr> <td>12</td> <td><code>kubeadm</code> performs potentially-dangerous reset operations</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81120">#81120</a></td> <td>closed, fixed by <a href="https://github.com/kubernetes/kubernetes/pull/81495">#81495</a>, <a href="https://github.com/kubernetes/kubernetes/pull/81494">#81494</a>, and <a href="https://github.com/kubernetes/website/pull/15881">kubernetes/website#15881</a></td> </tr> <tr> <td>13</td> <td>Overflows when using strconv.Atoi and downcasting the result</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81121">#81121</a></td> <td>closed, fixed by <a href="https://github.com/kubernetes/kubernetes/pull/89120">#89120</a></td> </tr> <tr> <td>14</td> <td>kubelet can cause an Out of Memory error with a malicious manifest</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81122">#81122</a></td> <td>closed, fixed by <a href="https://github.com/kubernetes/kubernetes/pull/76518">#76518</a></td> </tr> <tr> <td>15</td> <td><code>kubectl</code> can cause an Out Of Memory error with a malicious Pod specification</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81123">#81123</a></td> <td>Fix needed</td> </tr> <tr> <td>16</td> <td>Improper fetching of PIDs allows incorrect cgroup movement</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81124">#81124</a></td> <td>Fix needed</td> </tr> <tr> <td>17</td> <td>Directory traversal of host logs running kube-apiserver and kubelet</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81125">#81125</a></td> <td>closed, fixed by <a href="https://github.com/kubernetes/kubernetes/pull/87273">#87273</a></td> </tr> <tr> <td>18</td> <td>Non-constant time password comparison</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81126">#81126</a></td> <td>closed, fixed by <a href="https://github.com/kubernetes/kubernetes/pull/81152">#81152</a></td> </tr> <tr> <td>19</td> <td>Encryption recommendations not in accordance with best practices</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81127">#81127</a></td> <td>Work in Progress</td> </tr> <tr> <td>20</td> <td>Adding credentials to containers by default is unsafe</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81128">#81128</a></td> <td>Closed, fixed by <a href="https://github.com/kubernetes/kubernetes/pull/89193">#89193</a></td> </tr> <tr> <td>21</td> <td>kubelet liveness probes can be used to enumerate host network</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81129">#81129</a></td> <td><strong>Needs a KEP</strong></td> </tr> <tr> <td>22</td> <td>iSCSI volume storage cleartext secrets in logs</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81130">#81130</a></td> <td>closed, fixed by <a href="https://github.com/kubernetes/kubernetes/pull/81215">#81215</a></td> </tr> <tr> <td>23</td> <td>Hard coded credential paths</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81131">#81131</a></td> <td>closed, awaiting more evidence</td> </tr> <tr> <td>24</td> <td>Log rotation is not atomic</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81132">#81132</a></td> <td>Fix needed</td> </tr> <tr> <td>25</td> <td>Arbitrary file paths without bounding</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81133">#81133</a></td> <td>Fix needed.</td> </tr> <tr> <td>26</td> <td>Unsafe JSON construction</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81134">#81134</a></td> <td>Partially fixed</td> </tr> <tr> <td>27</td> <td>kubelet crash due to improperly handled errors</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81135">#81135</a></td> <td>Closed. Fixed by <a href="https://github.com/kubernetes/kubernetes/issues/81135">#81135</a></td> </tr> <tr> <td>28</td> <td>Legacy tokens do not expire</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81136">#81136</a></td> <td>closed, fixed as part of <a href="https://github.com/kubernetes/kubernetes/issues/70679">#70679</a></td> </tr> <tr> <td>29</td> <td>CoreDNS leaks internal cluster information across namespaces</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81137">#81137</a></td> <td>Closed, resolved with CoreDNS v1.6.2. <a href="https://github.com/kubernetes/kubernetes/issues/81137">#81137</a> (comment)</td> </tr> <tr> <td>30</td> <td>Services use questionable default functions</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81138">#81138</a></td> <td>Fix needed</td> </tr> <tr> <td>31</td> <td>Incorrect docker daemon process name in container manager</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81139">#81139</a></td> <td>closed, fixed by <a href="https://github.com/kubernetes/kubernetes/pull/81083">#81083</a></td> </tr> <tr> <td>32</td> <td>Use standard formats everywhere</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81140">#81140</a></td> <td><strong>Needs a KEP</strong></td> </tr> <tr> <td>33</td> <td>Superficial health check provides false sense of safety</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81141">#81141</a></td> <td>closed, fixed by <a href="https://github.com/kubernetes/kubernetes/pull/81319">#81319</a></td> </tr> <tr> <td>34</td> <td>Hardcoded use of insecure gRPC transport</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81142">#81142</a></td> <td><strong>Needs a KEP</strong></td> </tr> <tr> <td>35</td> <td>Incorrect handling of <code>Retry-After</code></td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81143">#81143</a></td> <td>closed, fixed by <a href="https://github.com/kubernetes/kubernetes/pull/91048">#91048</a></td> </tr> <tr> <td>36</td> <td>Incorrect isKernelPid check</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81144">#81144</a></td> <td>closed, fixed by <a href="https://github.com/kubernetes/kubernetes/pull/81086">#81086</a></td> </tr> <tr> <td>37</td> <td>Kubelet supports insecure TLS ciphersuites</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81145">#81145</a></td> <td>closed but fix needed for <a href="https://github.com/kubernetes/kubernetes/issues/91444">#91444</a> (see <a href="https://github.com/kubernetes/kubernetes/issues/81145#issuecomment-630291221">this comment</a>)</td> </tr> </tbody> </table> <h3 id="inspired-outcomes">Inspired outcomes</h3> <p>Apart from fixes to the specific issues, the 2019 third party security audit also motivated security focussed enhancements in the next few releases of Kubernetes. One such example is <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-security/1933-secret-logging-static-analysis">Kubernetes Enhancement Proposal (KEP) 1933 Defend Against Logging Secrets via Static Analysis</a> to prevent exposing secrets to logs with <a href="@PurelyApplied">Patrick Rhomberg</a> driving the implementation. As a result of this KEP, <a href="https://github.com/google/go-flow-levee"><code>go-flow-levee</code></a>, a taint propagation analysis tool configured to detect logging of secrets, is executed in a <a href="https://github.com/kubernetes/kubernetes/blob/master/hack/verify-govet-levee.sh">script</a> as a Prow presubmit job. This KEP was introduced in v1.20.0 as an alpha feature, then graduated to beta in v1.21.0, and graduated to stable in v1.23.0. As stable, the analysis runs as a blocking presubmit test. This KEP also helped resolve the following issues from the 2019 third party security audit:</p> <ul> <li><a href="https://github.com/kubernetes/kubernetes/issues/81114">#81114 Bearer tokens are revealed in logs</a></li> <li><a href="https://github.com/kubernetes/kubernetes/issues/81117">#81117 Environment variables expose sensitive data</a></li> <li><a href="https://github.com/kubernetes/kubernetes/issues/81130">#81130 iSCSI volume storage cleartext secrets in logs</a></li> </ul> <h2 id="remaining-work">Remaining Work</h2> <p>Many of the 37 findings identified were fixed by work from our community members over the last 3 years. However, we still have some work left to do. Here's a breakdown of remaining work with rough estimates on time commitment, complexity and benefits to the ecosystem on fixing these pending issues.</p> <div class="alert alert-info note callout" role="alert"> <strong>Note:</strong> Anything requiring a KEP (Kubernetes Enhancement Proposal) is considered <em>high</em> time commitment and <em>high</em> complexity. Benefits to Ecosystem are roughly equivalent to risk of keeping the finding unfixed which is determined by Severity Level + Likelihood of a successful vulnerability exploit. These estimates and values in the table below are the authors' personal opinion. An individual or end users' threat model may rate the benefits to fix a particular issue higher or lower. </div> <table> <thead> <tr> <th>Title</th> <th>Issue</th> <th>Time Commitment</th> <th>Complexity</th> <th>Benefit to Ecosystem</th> </tr> </thead> <tbody> <tr> <td>Kubernetes does not facilitate certificate revocation</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81111">#81111</a></td> <td>High</td> <td>High</td> <td>Medium</td> </tr> <tr> <td>Use of InsecureSkipVerify and other TLS weaknesses</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81119">#81119</a></td> <td>High</td> <td>High</td> <td>Medium</td> </tr> <tr> <td><code>kubectl</code> can cause a local Out Of Memory error with a malicious Pod specification</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81123">#81123</a></td> <td>Medium</td> <td>Medium</td> <td>Medium</td> </tr> <tr> <td>Improper fetching of PIDs allows incorrect cgroup movement</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81124">#81124</a></td> <td>Medium</td> <td>Medium</td> <td>Medium</td> </tr> <tr> <td>kubelet liveness probes can be used to enumerate host network</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81129">#81129</a></td> <td>High</td> <td>High</td> <td>Medium</td> </tr> <tr> <td>API Server supports insecure TLS ciphersuites</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81145">#81145</a></td> <td>Medium</td> <td>Medium</td> <td>Low</td> </tr> <tr> <td><abbr title="Time-of-check to time-of-use bug">TOCTOU</abbr> when moving PID to manager's cgroup via kubelet</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81113">#81113</a></td> <td>Medium</td> <td>Medium</td> <td>Low</td> </tr> <tr> <td>Log rotation is not atomic</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81132">#81132</a></td> <td>Medium</td> <td>Medium</td> <td>Low</td> </tr> <tr> <td>Arbitrary file paths without bounding</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81133">#81133</a></td> <td>Medium</td> <td>Medium</td> <td>Low</td> </tr> <tr> <td>Services use questionable default functions</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81138">#81138</a></td> <td>Medium</td> <td>Medium</td> <td>Low</td> </tr> <tr> <td>Use standard formats everywhere</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81140">#81140</a></td> <td>High</td> <td>High</td> <td>Very Low</td> </tr> <tr> <td>Hardcoded use of insecure gRPC transport</td> <td><a href="https://github.com/kubernetes/kubernetes/issues/81142">#81142</a></td> <td>High</td> <td>High</td> <td>Very Low</td> </tr> </tbody> </table> <p>To get started on fixing any of these findings that need help, please consider getting involved in <a href="https://github.com/kubernetes/community/tree/master/sig-security#contact">Kubernetes SIG Security</a> by joining our bi-weekly meetings or hanging out with us on our Slack Channel.</p></description></item><item><title>Blog: Introducing Kueue</title><link>https://kubernetes.io/blog/2022/10/04/introducing-kueue/</link><pubDate>Tue, 04 Oct 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/10/04/introducing-kueue/</guid><description> <p><strong>Authors:</strong> Abdullah Gharaibeh (Google), Aldo Culquicondor (Google)</p> <p>Whether on-premises or in the cloud, clusters face real constraints for resource usage, quota, and cost management reasons. Regardless of the autoscalling capabilities, clusters have finite capacity. As a result, users want an easy way to fairly and efficiently share resources.</p> <p>In this article, we introduce <a href="https://github.com/kubernetes-sigs/kueue/tree/main/docs#readme">Kueue</a>, an open source job queueing controller designed to manage batch jobs as a single unit. Kueue leaves pod-level orchestration to existing stable components of Kubernetes. Kueue natively supports the Kubernetes <a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/">Job</a> API and offers hooks for integrating other custom-built APIs for batch jobs.</p> <h2 id="why-kueue">Why Kueue?</h2> <p>Job queueing is a key feature to run batch workloads at scale in both on-premises and cloud environments. The main goal of job queueing is to manage access to a limited pool of resources shared by multiple tenants. Job queueing decides which jobs should wait, which can start immediately, and what resources they can use.</p> <p>Some of the most desired job queueing requirements include:</p> <ul> <li>Quota and budgeting to control who can use what and up to what limit. This is not only needed in clusters with static resources like on-premises, but it is also needed in cloud environments to control spend or usage of scarce resources.</li> <li>Fair sharing of resources between tenants. To maximize the usage of available resources, any unused quota assigned to inactive tenants should be allowed to be shared fairly between active tenants.</li> <li>Flexible placement of jobs across different resource types based on availability. This is important in cloud environments which have heterogeneous resources such as different architectures (GPU or CPU models) and different provisioning modes (spot vs on-demand).</li> <li>Support for autoscaled environments where resources can be provisioned on demand.</li> </ul> <p>Plain Kubernetes doesn't address the above requirements. In normal circumstances, once a Job is created, the job-controller instantly creates the pods and kube-scheduler continuously attempts to assign the pods to nodes. At scale, this situation can work the control plane to death. There is also currently no good way to control at the job level which jobs should get which resources first, and no way to express order or fair sharing. The current ResourceQuota model is not a good fit for these needs because quotas are enforced on resource creation, and there is no queueing of requests. The intent of ResourceQuotas is to provide a builtin reliability mechanism with policies needed by admins to protect clusters from failing over.</p> <p>In the Kubernetes ecosystem, there are several solutions for job scheduling. However, we found that these alternatives have one or more of the following problems:</p> <ul> <li>They replace existing stable components of Kubernetes, like kube-scheduler or the job-controller. This is problematic not only from an operational point of view, but also the duplication in the job APIs causes fragmentation of the ecosystem and reduces portability.</li> <li>They don't integrate with autoscaling, or</li> <li>They lack support for resource flexibility.</li> </ul> <h2 id="overview">How Kueue works</h2> <p>With Kueue we decided to take a different approach to job queueing on Kubernetes that is anchored around the following aspects:</p> <ul> <li>Not duplicating existing functionalities already offered by established Kubernetes components for pod scheduling, autoscaling and job lifecycle management.</li> <li>Adding key features that are missing to existing components. For example, we invested in the Job API to cover more use cases like <a href="https://kubernetes.io/blog/2021/04/19/introducing-indexed-jobs">IndexedJob</a> and <a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/#job-tracking-with-finalizers">fixed long standing issues related to pod tracking</a>. While this path takes longer to land features, we believe it is the more sustainable long term solution.</li> <li>Ensuring compatibility with cloud environments where compute resources are elastic and heterogeneous.</li> </ul> <p>For this approach to be feasible, Kueue needs knobs to influence the behavior of those established components so it can effectively manage when and where to start a job. We added those knobs to the Job API in the form of two features:</p> <ul> <li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/#suspending-a-job">Suspend field</a>, which allows Kueue to signal to the job-controller when to start or stop a Job.</li> <li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/#mutable-scheduling-directives">Mutable scheduling directives</a>, which allows Kueue to update a Job's <code>.spec.template.spec.nodeSelector</code> before starting the Job. This way, Kueue can control Pod placement while still delegating to kube-scheduler the actual pod-to-node scheduling.</li> </ul> <p>Note that any custom job API can be managed by Kueue if that API offers the above two capabilities.</p> <h3 id="resource-model">Resource model</h3> <p>Kueue defines new APIs to address the requirements mentioned at the beginning of this post. The three main APIs are:</p> <ul> <li>ResourceFlavor: a cluster-scoped API to define resource flavor available for consumption, like a GPU model. At its core, a ResourceFlavor is a set of labels that mirrors the labels on the nodes that offer those resources.</li> <li>ClusterQueue: a cluster-scoped API to define resource pools by setting quotas for one or more ResourceFlavor.</li> <li>LocalQueue: a namespaced API for grouping and managing single tenant jobs. In its simplest form, a LocalQueue is a pointer to the ClusterQueue that the tenant (modeled as a namespace) can use to start their jobs.</li> </ul> <p>For more details, take a look at the <a href="https://sigs.k8s.io/kueue/docs/concepts">API concepts documentation</a>. While the three APIs may look overwhelming, most of Kueue’s operations are centered around ClusterQueue; the ResourceFlavor and LocalQueue APIs are mainly organizational wrappers.</p> <h3 id="example-use-case">Example use case</h3> <p>Imagine the following setup for running batch workloads on a Kubernetes cluster on the cloud:</p> <ul> <li>You have <a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler">cluster-autoscaler</a> installed in the cluster to automatically adjust the size of your cluster.</li> <li>There are two types of autoscaled node groups that differ on their provisioning policies: spot and on-demand. The nodes of each group are differentiated by the label <code>instance-type=spot</code> or <code>instance-type=ondemand</code>. Moreover, since not all Jobs can tolerate running on spot nodes, the nodes are tainted with <code>spot=true:NoSchedule</code>.</li> <li>To strike a balance between cost and resource availability, imagine you want Jobs to use up to 1000 cores of on-demand nodes, then use up to 2000 cores of spot nodes.</li> </ul> <p>As an admin for the batch system, you define two ResourceFlavors that represent the two types of nodes:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kueue.x-k8s.io/v1alpha2<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ResourceFlavor<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>ondemand<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">instance-type</span>:<span style="color:#bbb"> </span>ondemand <span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kueue.x-k8s.io/v1alpha2<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ResourceFlavor<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>spot<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">instance-type</span>:<span style="color:#bbb"> </span>spot<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">taints</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span>- <span style="color:#008000;font-weight:bold">effect</span>:<span style="color:#bbb"> </span>NoSchedule<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>spot<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb"> </span></span></span></code></pre></div><p>Then you define the quotas by creating a ClusterQueue as follows:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kueue.x-k8s.io/v1alpha2<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ClusterQueue<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>research-pool<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">namespaceSelector</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;cpu&#34;</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">flavors</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>ondemand<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">quota</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">min</span>:<span style="color:#bbb"> </span><span style="color:#666">1000</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>spot<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">quota</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">min</span>:<span style="color:#bbb"> </span><span style="color:#666">2000</span><span style="color:#bbb"> </span></span></span></code></pre></div><p>Note that the order of flavors in the ClusterQueue resources matters: Kueue will attempt to fit jobs in the available quotas according to the order unless the job has an explicit affinity to specific flavors.</p> <p>For each namespace, you define a LocalQueue that points to the ClusterQueue above:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>kueue.x-k8s.io/v1alpha2<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>LocalQueue<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>training<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>team-ml<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">clusterQueue</span>:<span style="color:#bbb"> </span>research-pool<span style="color:#bbb"> </span></span></span></code></pre></div><p>Admins create the above setup once. Batch users are able to find the queues they are allowed to submit to by listing the LocalQueues in their namespace(s). The command is similar to the following: <code>kubectl get -n my-namespace localqueues</code></p> <p>To submit work, create a Job and set the <code>kueue.x-k8s.io/queue-name</code> annotation as follows:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>batch/v1<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Job<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">generateName</span>:<span style="color:#bbb"> </span>sample-job-<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">kueue.x-k8s.io/queue-name</span>:<span style="color:#bbb"> </span>training<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">parallelism</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">completions</span>:<span style="color:#bbb"> </span><span style="color:#666">3</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">tolerations</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>spot<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;Exists&#34;</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">effect</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;NoSchedule&#34;</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>example-batch-workload<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>registry.example/batch/calculate-pi:3.14<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">args</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;30s&#34;</span>]<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">cpu</span>:<span style="color:#bbb"> </span><span style="color:#666">1</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb"> </span></span></span></code></pre></div><p>Kueue intervenes to suspend the Job as soon as it is created. Once the Job is at the head of the ClusterQueue, Kueue evaluates if it can start by checking if the resources requested by the job fit the available quota.</p> <p>In the above example, the Job tolerates spot resources. If there are previously admitted Jobs consuming all existing on-demand quota but not all of spot’s, Kueue admits the Job using the spot quota. Kueue does this by issuing a single update to the Job object that:</p> <ul> <li>Changes the <code>.spec.suspend</code> flag to false</li> <li>Adds the term <code>instance-type: spot</code> to the job's <code>.spec.template.spec.nodeSelector</code> so that when the pods are created by the job controller, those pods can only schedule onto spot nodes.</li> </ul> <p>Finally, if there are available empty nodes with matching node selector terms, then kube-scheduler will directly schedule the pods. If not, then kube-scheduler will initially mark the pods as unschedulable, which will trigger the cluster-autoscaler to provision new nodes.</p> <h2 id="future-work-and-getting-involved">Future work and getting involved</h2> <p>The example above offers a glimpse of some of Kueue's features including support for quota, resource flexibility, and integration with cluster autoscaler. Kueue also supports fair-sharing, job priorities, and different queueing strategies. Take a look at the <a href="https://github.com/kubernetes-sigs/kueue/tree/main/docs">Kueue documentation</a> to learn more about those features and how to use Kueue.</p> <p>We have a number of features that we plan to add to Kueue, such as hierarchical quota, budgets, and support for dynamically sized jobs. In the more immediate future, we are focused on adding support for job preemption.</p> <p>The latest <a href="https://github.com/kubernetes-sigs/kueue/releases">Kueue release</a> is available on Github; try it out if you run batch workloads on Kubernetes (requires v1.22 or newer). We are in the early stages of this project and we are seeking feedback of all levels, major or minor, so please don’t hesitate to reach out. We’re also open to additional contributors, whether it is to fix or report bugs, or help add new features or write documentation. You can get in touch with us via our <a href="http://sigs.k8s.io/kueue">repo</a>, <a href="https://groups.google.com/a/kubernetes.io/g/wg-batch">mailing list</a> or on <a href="https://kubernetes.slack.com/messages/wg-batch">Slack</a>.</p> <p>Last but not least, thanks to all <a href="https://github.com/kubernetes-sigs/kueue/graphs/contributors">our contributors</a> who made this project possible!</p></description></item><item><title>Blog: Kubernetes 1.25: alpha support for running Pods with user namespaces</title><link>https://kubernetes.io/blog/2022/10/03/userns-alpha/</link><pubDate>Mon, 03 Oct 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/10/03/userns-alpha/</guid><description> <p><strong>Authors:</strong> Rodrigo Campos (Microsoft), Giuseppe Scrivano (Red Hat)</p> <p>Kubernetes v1.25 introduces the support for user namespaces.</p> <p>This is a major improvement for running secure workloads in Kubernetes. Each pod will have access only to a limited subset of the available UIDs and GIDs on the system, thus adding a new security layer to protect from other pods running on the same system.</p> <h2 id="how-does-it-work">How does it work?</h2> <p>A process running on Linux can use up to 4294967296 different UIDs and GIDs.</p> <p>User namespaces is a Linux feature that allows mapping a set of users in the container to different users in the host, thus restricting what IDs a process can effectively use. Furthermore, the capabilities granted in a new user namespace do not apply in the host initial namespaces.</p> <h2 id="why-is-it-important">Why is it important?</h2> <p>There are mainly two reasons why user namespaces are important:</p> <ul> <li> <p>improve security since they restrict the IDs a pod can use, so each pod can run in its own separate environment with unique IDs.</p> </li> <li> <p>enable running workloads as root in a safer manner.</p> </li> </ul> <p>In a user namespace we can map the root user inside the pod to a non-zero ID outside the container, containers believe in running as root while they are a regular unprivileged ID from the host point of view.</p> <p>The process can keep capabilities that are usually restricted to privileged pods and do it in a safe way since the capabilities granted in a new user namespace do not apply in the host initial namespaces.</p> <h2 id="how-do-i-enable-user-namespaces">How do I enable user namespaces?</h2> <p>At the moment, user namespaces support is opt-in, so you must enable it for a pod setting <code>hostUsers</code> to <code>false</code> under the pod spec stanza:</p> <pre tabindex="0"><code>apiVersion: v1 kind: Pod spec: hostUsers: false containers: - name: nginx image: docker.io/nginx </code></pre><p>The feature is behind a feature gate, so make sure to enable the <code>UserNamespacesStatelessPodsSupport</code> gate before you can use the new feature.</p> <p>The runtime must also support user namespaces:</p> <ul> <li> <p>containerd: support is planned for the 1.7 release. See containerd issue <a href="https://github.com/containerd/containerd/issues/7063">#7063</a> for more details.</p> </li> <li> <p>CRI-O: v1.25 has support for user namespaces.</p> </li> </ul> <p>Support for this in <code>cri-dockerd</code> is <a href="https://github.com/Mirantis/cri-dockerd/issues/74">not planned</a> yet.</p> <h2 id="how-do-i-get-involved">How do I get involved?</h2> <p>You can reach SIG Node by several means:</p> <ul> <li>Slack: <a href="https://kubernetes.slack.com/messages/sig-node">#sig-node</a></li> <li><a href="https://groups.google.com/forum/#!forum/kubernetes-sig-node">Mailing list</a></li> <li><a href="https://github.com/kubernetes/community/labels/sig%2Fnode">Open Community Issues/PRs</a></li> </ul> <p>You can also contact us directly:</p> <ul> <li>GitHub / Slack: @rata @giuseppe</li> </ul></description></item><item><title>Blog: Enforce CRD Immutability with CEL Transition Rules</title><link>https://kubernetes.io/blog/2022/09/29/enforce-immutability-using-cel/</link><pubDate>Thu, 29 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/09/29/enforce-immutability-using-cel/</guid><description> <p><strong>Author:</strong> <a href="https://github.com/alexzielenski">Alexander Zielenski</a> (Google)</p> <p>Immutable fields can be found in a few places in the built-in Kubernetes types. For example, you can't change the <code>.metadata.name</code> of an object. Specific objects have fields where changes to existing objects are constrained; for example, the <code>.spec.selector</code> of a Deployment.</p> <p>Aside from simple immutability, there are other common design patterns such as lists which are append-only, or a map with mutable values and immutable keys.</p> <p>Until recently the best way to restrict field mutability for CustomResourceDefinitions has been to create a validating <a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks">admission webhook</a>: this means a lot of complexity for the common case of making a field immutable.</p> <p>Beta since Kubernetes 1.25, CEL Validation Rules allow CRD authors to express validation constraints on their fields using a rich expression language, <a href="https://github.com/google/cel-spec">CEL</a>. This article explores how you can use validation rules to implement a few common immutability patterns directly in the manifest for a CRD.</p> <h2 id="basics-of-validation-rules">Basics of validation rules</h2> <p>The new support for CEL validation rules in Kubernetes allows CRD authors to add complicated admission logic for their resources without writing any code!</p> <p>For example, A CEL rule to constrain a field <code>maximumSize</code> to be greater than a <code>minimumSize</code> for a CRD might look like the following:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">rule</span>:<span style="color:#bbb"> </span>|<span style="color:#b44;font-style:italic"> </span></span></span><span style="display:flex;"><span><span style="color:#b44;font-style:italic"> </span><span style="color:#bbb"> </span>self.maximumSize &gt; self.minimumSize<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">message</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#39;Maximum size must be greater than minimum size.&#39;</span><span style="color:#bbb"> </span></span></span></code></pre></div><p>The rule field contains an expression written in CEL. <code>self</code> is a special keyword in CEL which refers to the object whose type contains the rule.</p> <p>The message field is an error message which will be sent to Kubernetes clients whenever this particular rule is not satisfied.</p> <p>For more details about the capabilities and limitations of Validation Rules using CEL, please refer to <a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules">validation rules</a>. The <a href="https://github.com/google/cel-spec">CEL specification</a> is also a good reference for information specifically related to the language.</p> <h2 id="immutability-patterns-with-cel-validation-rules">Immutability patterns with CEL validation rules</h2> <p>This section implements several common use cases for immutability in Kubernetes CustomResourceDefinitions, using validation rules expressed as <a href="https://book.kubebuilder.io/reference/markers/crd.html">kubebuilder marker comments</a>. Resultant OpenAPI generated by the kubebuilder marker comments will also be included so that if you are writing your CRD manifests by hand you can still follow along.</p> <h2 id="project-setup">Project setup</h2> <p>To use CEL rules with kubebuilder comments, you first need to set up a Golang project structure with the CRD defined in Go.</p> <p>You may skip this step if you are not using kubebuilder or are only interested in the resultant OpenAPI extensions.</p> <p>Begin with a folder structure of a Go module set up like the following. If you have your own project already set up feel free to adapt this tutorial to your liking:</p> <figure> <div class="mermaid"> graph LR . --> generate.go . --> pkg --> apis --> stable.example.com --> v1 v1 --> doc.go v1 --> types.go . --> tools.go </div> </figure> <noscript> <div class="alert alert-secondary callout" role="alert"> <em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em> </div> </noscript> <p>This is the typical folder structure used by Kubernetes projects for defining new API resources.</p> <p><code>doc.go</code> contains package-level metadata such as the group and the version:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-go" data-lang="go"><span style="display:flex;"><span><span style="color:#080;font-style:italic">// +groupName=stable.example.com </span></span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic">// +versionName=v1 </span></span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic"></span><span style="color:#a2f;font-weight:bold">package</span> v1 </span></span></code></pre></div><p><code>types.go</code> contains all type definitions in stable.example.com/v1</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-go" data-lang="go"><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">package</span> v1 </span></span><span style="display:flex;"><span> </span></span><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">import</span> ( </span></span><span style="display:flex;"><span> metav1 <span style="color:#b44">&#34;k8s.io/apimachinery/pkg/apis/meta/v1&#34;</span> </span></span><span style="display:flex;"><span>) </span></span><span style="display:flex;"><span> </span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic">// An empty CRD as an example of defining a type using controller tools </span></span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic">// +kubebuilder:storageversion </span></span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic">// +kubebuilder:subresource:status </span></span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic"></span><span style="color:#a2f;font-weight:bold">type</span> TestCRD <span style="color:#a2f;font-weight:bold">struct</span> { </span></span><span style="display:flex;"><span> metav1.TypeMeta <span style="color:#b44">`json:&#34;,inline&#34;`</span> </span></span><span style="display:flex;"><span> metav1.ObjectMeta <span style="color:#b44">`json:&#34;metadata,omitempty&#34;`</span> </span></span><span style="display:flex;"><span> </span></span><span style="display:flex;"><span> Spec TestCRDSpec <span style="color:#b44">`json:&#34;spec,omitempty&#34;`</span> </span></span><span style="display:flex;"><span> Status TestCRDStatus <span style="color:#b44">`json:&#34;status,omitempty&#34;`</span> </span></span><span style="display:flex;"><span>} </span></span><span style="display:flex;"><span> </span></span><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">type</span> TestCRDStatus <span style="color:#a2f;font-weight:bold">struct</span> {} </span></span><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">type</span> TestCRDSpec <span style="color:#a2f;font-weight:bold">struct</span> { </span></span><span style="display:flex;"><span> <span style="color:#080;font-style:italic">// You will fill this in as you go along </span></span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic"></span>} </span></span></code></pre></div><p><code>tools.go</code> contains a dependency on <a href="https://book.kubebuilder.io/reference/generating-crd.html#generating-crds">controller-gen</a> which will be used to generate the CRD definition:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-go" data-lang="go"><span style="display:flex;"><span><span style="color:#080;font-style:italic">//go:build tools </span></span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic"></span> </span></span><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">package</span> celimmutabilitytutorial </span></span><span style="display:flex;"><span> </span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic">// Force direct dependency on code-generator so that it may be executed with go run </span></span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic"></span><span style="color:#a2f;font-weight:bold">import</span> ( </span></span><span style="display:flex;"><span> _ <span style="color:#b44">&#34;sigs.k8s.io/controller-tools/cmd/controller-gen&#34;</span> </span></span><span style="display:flex;"><span>) </span></span></code></pre></div><p>Finally, <code>generate.go</code>contains a <code>go:generate</code> directive to make use of <code>controller-gen</code>. <code>controller-gen</code> parses our <code>types.go</code> and creates generates CRD yaml files into a <code>crd</code> folder:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-go" data-lang="go"><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">package</span> celimmutabilitytutorial </span></span><span style="display:flex;"><span> </span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic">//go:generate go run sigs.k8s.io/controller-tools/cmd/controller-gen crd paths=./pkg/apis/... output:dir=./crds </span></span></span></code></pre></div><p>You may now want to add dependencies for our definitions and test the code generation:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#a2f">cd</span> cel-immutability-tutorial </span></span><span style="display:flex;"><span>go mod init &lt;your-org&gt;/&lt;your-module-name&gt; </span></span><span style="display:flex;"><span>go mod tidy </span></span><span style="display:flex;"><span>go generate ./... </span></span></code></pre></div><p>After running these commands you now have completed the basic project structure. Your folder tree should look like the following:</p> <figure> <div class="mermaid"> graph LR . --> crds --> stable.example.com_testcrds.yaml . --> generate.go . --> go.mod . --> go.sum . --> pkg --> apis --> stable.example.com --> v1 v1 --> doc.go v1 --> types.go . --> tools.go </div> </figure> <noscript> <div class="alert alert-secondary callout" role="alert"> <em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em> </div> </noscript> <p>The manifest for the example CRD is now available in <code>crds/stable.example.com_testcrds.yaml</code>.</p> <h2 id="immutablility-after-first-modification">Immutablility after first modification</h2> <p>A common immutability design pattern is to make the field immutable once it has been first set. This example will throw a validation error if the field after changes after being first initialized.</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-go" data-lang="go"><span style="display:flex;"><span><span style="color:#080;font-style:italic">// +kubebuilder:validation:XValidation:rule=&#34;!has(oldSelf.value) || has(self.value)&#34;, message=&#34;Value is required once set&#34; </span></span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic"></span><span style="color:#a2f;font-weight:bold">type</span> ImmutableSinceFirstWrite <span style="color:#a2f;font-weight:bold">struct</span> { </span></span><span style="display:flex;"><span> metav1.TypeMeta <span style="color:#b44">`json:&#34;,inline&#34;`</span> </span></span><span style="display:flex;"><span> metav1.ObjectMeta <span style="color:#b44">`json:&#34;metadata,omitempty&#34;`</span> </span></span><span style="display:flex;"><span> </span></span><span style="display:flex;"><span> <span style="color:#080;font-style:italic">// +kubebuilder:validation:Optional </span></span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic"></span> <span style="color:#080;font-style:italic">// +kubebuilder:validation:XValidation:rule=&#34;self == oldSelf&#34;,message=&#34;Value is immutable&#34; </span></span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic"></span> <span style="color:#080;font-style:italic">// +kubebuilder:validation:MaxLength=512 </span></span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic"></span> Value <span style="color:#0b0;font-weight:bold">string</span> <span style="color:#b44">`json:&#34;value&#34;`</span> </span></span><span style="display:flex;"><span>} </span></span></code></pre></div><p>The <code>+kubebuilder</code> directives in the comments inform controller-gen how to annotate the generated OpenAPI. The <code>XValidation</code> rule causes the rule to appear among the <code>x-kubernetes-validations</code> OpenAPI extension. Kubernetes then respects the OpenAPI spec to enforce our constraints.</p> <p>To enforce a field's immutability after its first write, you need to apply the following constraints:</p> <ol> <li>Field must be allowed to be initially unset <code>+kubebuilder:validation:Optional</code></li> <li>Once set, field must not be allowed to be removed: <code>!has(oldSelf.value) | has(self.value)</code> (type-scoped rule)</li> <li>Once set, field must not be allowed to change value <code>self == oldSelf</code> (field-scoped rule)</li> </ol> <p>Also note the additional directive <code>+kubebuilder:validation:MaxLength</code>. CEL requires that all strings have attached max length so that it may estimate the computation cost of the rule. Rules that are too expensive will be rejected. For more information on CEL cost budgeting, check out the other tutorial.</p> <h3 id="example-usage">Example usage</h3> <p>Generating and installing the CRD should succeed:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#080;font-style:italic"># Ensure the CRD yaml is generated by controller-gen</span> </span></span><span style="display:flex;"><span>go generate ./... </span></span><span style="display:flex;"><span>kubectl apply -f crds/stable.example.com_immutablesincefirstwrites.yaml </span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span><span style="color:#888">customresourcedefinition.apiextensions.k8s.io/immutablesincefirstwrites.stable.example.com created </span></span></span></code></pre></div><p>Creating initial empty object with no <code>value</code> is permitted since <code>value</code> is <code>optional</code>:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>kubectl apply -f - <span style="color:#b44">&lt;&lt;EOF </span></span></span><span style="display:flex;"><span><span style="color:#b44">--- </span></span></span><span style="display:flex;"><span><span style="color:#b44">apiVersion: stable.example.com/v1 </span></span></span><span style="display:flex;"><span><span style="color:#b44">kind: ImmutableSinceFirstWrite </span></span></span><span style="display:flex;"><span><span style="color:#b44">metadata: </span></span></span><span style="display:flex;"><span><span style="color:#b44"> name: test1 </span></span></span><span style="display:flex;"><span><span style="color:#b44">EOF</span> </span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span><span style="color:#888">immutablesincefirstwrite.stable.example.com/test1 created </span></span></span></code></pre></div><p>The initial modification of <code>value</code> succeeds:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>kubectl apply -f - <span style="color:#b44">&lt;&lt;EOF </span></span></span><span style="display:flex;"><span><span style="color:#b44">--- </span></span></span><span style="display:flex;"><span><span style="color:#b44">apiVersion: stable.example.com/v1 </span></span></span><span style="display:flex;"><span><span style="color:#b44">kind: ImmutableSinceFirstWrite </span></span></span><span style="display:flex;"><span><span style="color:#b44">metadata: </span></span></span><span style="display:flex;"><span><span style="color:#b44"> name: test1 </span></span></span><span style="display:flex;"><span><span style="color:#b44">value: Hello, world! </span></span></span><span style="display:flex;"><span><span style="color:#b44">EOF</span> </span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span><span style="color:#888">immutablesincefirstwrite.stable.example.com/test1 configured </span></span></span></code></pre></div><p>An attempt to change <code>value</code> is blocked by the field-level validation rule. Note the error message shown to the user comes from the validation rule.</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>kubectl apply -f - <span style="color:#b44">&lt;&lt;EOF </span></span></span><span style="display:flex;"><span><span style="color:#b44">--- </span></span></span><span style="display:flex;"><span><span style="color:#b44">apiVersion: stable.example.com/v1 </span></span></span><span style="display:flex;"><span><span style="color:#b44">kind: ImmutableSinceFirstWrite </span></span></span><span style="display:flex;"><span><span style="color:#b44">metadata: </span></span></span><span style="display:flex;"><span><span style="color:#b44"> name: test1 </span></span></span><span style="display:flex;"><span><span style="color:#b44">value: Hello, new world! </span></span></span><span style="display:flex;"><span><span style="color:#b44">EOF</span> </span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span><span style="color:#888">The ImmutableSinceFirstWrite &#34;test1&#34; is invalid: value: Invalid value: &#34;string&#34;: Value is immutable </span></span></span></code></pre></div><p>An attempt to remove the <code>value</code> field altogether is blocked by the other validation rule on the type. The error message also comes from the rule.</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>kubectl apply -f - <span style="color:#b44">&lt;&lt;EOF </span></span></span><span style="display:flex;"><span><span style="color:#b44">--- </span></span></span><span style="display:flex;"><span><span style="color:#b44">apiVersion: stable.example.com/v1 </span></span></span><span style="display:flex;"><span><span style="color:#b44">kind: ImmutableSinceFirstWrite </span></span></span><span style="display:flex;"><span><span style="color:#b44">metadata: </span></span></span><span style="display:flex;"><span><span style="color:#b44"> name: test1 </span></span></span><span style="display:flex;"><span><span style="color:#b44">EOF</span> </span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span><span style="color:#888">The ImmutableSinceFirstWrite &#34;test1&#34; is invalid: &lt;nil&gt;: Invalid value: &#34;object&#34;: Value is required once set </span></span></span></code></pre></div><h3 id="generated-schema">Generated schema</h3> <p>Note that in the generated schema there are two separate rule locations. One is directly attached to the property <code>immutable_since_first_write</code>. The other rule is associated with the crd type itself.</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">openAPIV3Schema</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">maxLength</span>:<span style="color:#bbb"> </span><span style="color:#666">512</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">x-kubernetes-validations</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- <span style="color:#008000;font-weight:bold">message</span>:<span style="color:#bbb"> </span>Value is immutable<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">rule</span>:<span style="color:#bbb"> </span>self == oldSelf<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">x-kubernetes-validations</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- <span style="color:#008000;font-weight:bold">message</span>:<span style="color:#bbb"> </span>Value is required once set<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">rule</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#39;!has(oldSelf.value) || has(self.value)&#39;</span><span style="color:#bbb"> </span></span></span></code></pre></div><h2 id="immutability-upon-object-creation">Immutability upon object creation</h2> <p>A field which is immutable upon creation time is implemented similarly to the earlier example. The difference is that that field is marked required, and the type-scoped rule is no longer necessary.</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-go" data-lang="go"><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">type</span> ImmutableSinceCreation <span style="color:#a2f;font-weight:bold">struct</span> { </span></span><span style="display:flex;"><span> metav1.TypeMeta <span style="color:#b44">`json:&#34;,inline&#34;`</span> </span></span><span style="display:flex;"><span> metav1.ObjectMeta <span style="color:#b44">`json:&#34;metadata,omitempty&#34;`</span> </span></span><span style="display:flex;"><span> </span></span><span style="display:flex;"><span> <span style="color:#080;font-style:italic">// +kubebuilder:validation:Required </span></span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic"></span> <span style="color:#080;font-style:italic">// +kubebuilder:validation:XValidation:rule=&#34;self == oldSelf&#34;,message=&#34;Value is immutable&#34; </span></span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic"></span> <span style="color:#080;font-style:italic">// +kubebuilder:validation:MaxLength=512 </span></span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic"></span> Value <span style="color:#0b0;font-weight:bold">string</span> <span style="color:#b44">`json:&#34;value&#34;`</span> </span></span><span style="display:flex;"><span>} </span></span></code></pre></div><p>This field will be required when the object is created, and after that point will not be allowed to be modified. Our CEL Validation Rule <code>self == oldSelf</code></p> <h3 id="usage-example">Usage example</h3> <p>Generating and installing the CRD should succeed:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#080;font-style:italic"># Ensure the CRD yaml is generated by controller-gen</span> </span></span><span style="display:flex;"><span>go generate ./... </span></span><span style="display:flex;"><span>kubectl apply -f crds/stable.example.com_immutablesincecreations.yaml </span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span><span style="color:#888">customresourcedefinition.apiextensions.k8s.io/immutablesincecreations.stable.example.com created </span></span></span></code></pre></div><p>Applying an object without the required field should fail:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>kubectl apply -f - <span style="color:#b44">&lt;&lt;EOF </span></span></span><span style="display:flex;"><span><span style="color:#b44">apiVersion: stable.example.com/v1 </span></span></span><span style="display:flex;"><span><span style="color:#b44">kind: ImmutableSinceCreation </span></span></span><span style="display:flex;"><span><span style="color:#b44">metadata: </span></span></span><span style="display:flex;"><span><span style="color:#b44"> name: test1 </span></span></span><span style="display:flex;"><span><span style="color:#b44">EOF</span> </span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span><span style="color:#888">The ImmutableSinceCreation &#34;test1&#34; is invalid: </span></span></span><span style="display:flex;"><span><span style="color:#888">* value: Required value </span></span></span><span style="display:flex;"><span><span style="color:#888">* &lt;nil&gt;: Invalid value: &#34;null&#34;: some validation rules were not checked because the object was invalid; correct the existing errors to complete validation </span></span></span></code></pre></div><p>Now that the field has been added, the operation is permitted:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>kubectl apply -f - <span style="color:#b44">&lt;&lt;EOF </span></span></span><span style="display:flex;"><span><span style="color:#b44">apiVersion: stable.example.com/v1 </span></span></span><span style="display:flex;"><span><span style="color:#b44">kind: ImmutableSinceCreation </span></span></span><span style="display:flex;"><span><span style="color:#b44">metadata: </span></span></span><span style="display:flex;"><span><span style="color:#b44"> name: test1 </span></span></span><span style="display:flex;"><span><span style="color:#b44">value: Hello, world! </span></span></span><span style="display:flex;"><span><span style="color:#b44">EOF</span> </span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span><span style="color:#888">immutablesincecreation.stable.example.com/test1 created </span></span></span></code></pre></div><p>If you attempt to change the <code>value</code>, the operation is blocked due to the validation rules in the CRD. Note that the error message is as it was defined in the validation rule.</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>kubectl apply -f - <span style="color:#b44">&lt;&lt;EOF </span></span></span><span style="display:flex;"><span><span style="color:#b44">apiVersion: stable.example.com/v1 </span></span></span><span style="display:flex;"><span><span style="color:#b44">kind: ImmutableSinceCreation </span></span></span><span style="display:flex;"><span><span style="color:#b44">metadata: </span></span></span><span style="display:flex;"><span><span style="color:#b44"> name: test1 </span></span></span><span style="display:flex;"><span><span style="color:#b44">value: Hello, new world! </span></span></span><span style="display:flex;"><span><span style="color:#b44">EOF</span> </span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span><span style="color:#888">The ImmutableSinceCreation &#34;test1&#34; is invalid: value: Invalid value: &#34;string&#34;: Value is immutable </span></span></span></code></pre></div><p>Also if you attempted to remove <code>value</code> altogether after adding it, you will see an error as expected:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>kubectl apply -f - <span style="color:#b44">&lt;&lt;EOF </span></span></span><span style="display:flex;"><span><span style="color:#b44">apiVersion: stable.example.com/v1 </span></span></span><span style="display:flex;"><span><span style="color:#b44">kind: ImmutableSinceCreation </span></span></span><span style="display:flex;"><span><span style="color:#b44">metadata: </span></span></span><span style="display:flex;"><span><span style="color:#b44"> name: test1 </span></span></span><span style="display:flex;"><span><span style="color:#b44">EOF</span> </span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span><span style="color:#888">The ImmutableSinceCreation &#34;test1&#34; is invalid: </span></span></span><span style="display:flex;"><span><span style="color:#888">* value: Required value </span></span></span><span style="display:flex;"><span><span style="color:#888">* &lt;nil&gt;: Invalid value: &#34;null&#34;: some validation rules were not checked because the object was invalid; correct the existing errors to complete validation </span></span></span></code></pre></div><h3 id="generated-schema-1">Generated schema</h3> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">openAPIV3Schema</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">maxLength</span>:<span style="color:#bbb"> </span><span style="color:#666">512</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">x-kubernetes-validations</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- <span style="color:#008000;font-weight:bold">message</span>:<span style="color:#bbb"> </span>Value is immutable<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">rule</span>:<span style="color:#bbb"> </span>self == oldSelf<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">required</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- value<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb"> </span></span></span></code></pre></div><h2 id="append-only-list-of-containers">Append-only list of containers</h2> <p>In the case of ephemeral containers on Pods, Kubernetes enforces that the elements in the list are immutable, and can’t be removed. The following example shows how you could use CEL to achieve the same behavior.</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-go" data-lang="go"><span style="display:flex;"><span><span style="color:#080;font-style:italic">// +kubebuilder:validation:XValidation:rule=&#34;!has(oldSelf.value) || has(self.value)&#34;, message=&#34;Value is required once set&#34; </span></span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic"></span><span style="color:#a2f;font-weight:bold">type</span> AppendOnlyList <span style="color:#a2f;font-weight:bold">struct</span> { </span></span><span style="display:flex;"><span> metav1.TypeMeta <span style="color:#b44">`json:&#34;,inline&#34;`</span> </span></span><span style="display:flex;"><span> metav1.ObjectMeta <span style="color:#b44">`json:&#34;metadata,omitempty&#34;`</span> </span></span><span style="display:flex;"><span> </span></span><span style="display:flex;"><span> <span style="color:#080;font-style:italic">// +kubebuilder:validation:Optional </span></span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic"></span> <span style="color:#080;font-style:italic">// +kubebuilder:validation:MaxItems=100 </span></span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic"></span> <span style="color:#080;font-style:italic">// +kubebuilder:validation:XValidation:rule=&#34;oldSelf.all(x, x in self)&#34;,message=&#34;Values may only be added&#34; </span></span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic"></span> Values []v1.EphemeralContainer <span style="color:#b44">`json:&#34;value&#34;`</span> </span></span><span style="display:flex;"><span>} </span></span></code></pre></div><ol> <li>Once set, field must not be deleted: <code>!has(oldSelf.value) || has(self.value)</code> (type-scoped)</li> <li>Once a value is added it is not removed: <code>oldSelf.all(x, x in self)</code> (field-scoped)</li> <li>Value may be initially unset: <code>+kubebuilder:validation:Optional</code></li> </ol> <p>Note that for cost-budgeting purposes, <code>MaxItems</code> is also required to be specified.</p> <h3 id="example-usage-1">Example usage</h3> <p>Generating and installing the CRD should succeed:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#080;font-style:italic"># Ensure the CRD yaml is generated by controller-gen</span> </span></span><span style="display:flex;"><span>go generate ./... </span></span><span style="display:flex;"><span>kubectl apply -f crds/stable.example.com_appendonlylists.yaml </span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span><span style="color:#888">customresourcedefinition.apiextensions.k8s.io/appendonlylists.stable.example.com created </span></span></span></code></pre></div><p>Creating an inital list with one element inside should succeed without problem:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>kubectl apply -f - <span style="color:#b44">&lt;&lt;EOF </span></span></span><span style="display:flex;"><span><span style="color:#b44">--- </span></span></span><span style="display:flex;"><span><span style="color:#b44">apiVersion: stable.example.com/v1 </span></span></span><span style="display:flex;"><span><span style="color:#b44">kind: AppendOnlyList </span></span></span><span style="display:flex;"><span><span style="color:#b44">metadata: </span></span></span><span style="display:flex;"><span><span style="color:#b44"> name: testlist </span></span></span><span style="display:flex;"><span><span style="color:#b44">value: </span></span></span><span style="display:flex;"><span><span style="color:#b44"> - name: container1 </span></span></span><span style="display:flex;"><span><span style="color:#b44"> image: nginx/nginx </span></span></span><span style="display:flex;"><span><span style="color:#b44">EOF</span> </span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span><span style="color:#888">appendonlylist.stable.example.com/testlist created </span></span></span></code></pre></div><p>Adding an element to the list should also proceed without issue as expected:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>kubectl apply -f - <span style="color:#b44">&lt;&lt;EOF </span></span></span><span style="display:flex;"><span><span style="color:#b44">--- </span></span></span><span style="display:flex;"><span><span style="color:#b44">apiVersion: stable.example.com/v1 </span></span></span><span style="display:flex;"><span><span style="color:#b44">kind: AppendOnlyList </span></span></span><span style="display:flex;"><span><span style="color:#b44">metadata: </span></span></span><span style="display:flex;"><span><span style="color:#b44"> name: testlist </span></span></span><span style="display:flex;"><span><span style="color:#b44">value: </span></span></span><span style="display:flex;"><span><span style="color:#b44"> - name: container1 </span></span></span><span style="display:flex;"><span><span style="color:#b44"> image: nginx/nginx </span></span></span><span style="display:flex;"><span><span style="color:#b44"> - name: container2 </span></span></span><span style="display:flex;"><span><span style="color:#b44"> image: mongodb/mongodb </span></span></span><span style="display:flex;"><span><span style="color:#b44">EOF</span> </span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span><span style="color:#888">appendonlylist.stable.example.com/testlist configured </span></span></span></code></pre></div><p>But if you now attempt to remove an element, the error from the validation rule is triggered:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>kubectl apply -f - <span style="color:#b44">&lt;&lt;EOF </span></span></span><span style="display:flex;"><span><span style="color:#b44">--- </span></span></span><span style="display:flex;"><span><span style="color:#b44">apiVersion: stable.example.com/v1 </span></span></span><span style="display:flex;"><span><span style="color:#b44">kind: AppendOnlyList </span></span></span><span style="display:flex;"><span><span style="color:#b44">metadata: </span></span></span><span style="display:flex;"><span><span style="color:#b44"> name: testlist </span></span></span><span style="display:flex;"><span><span style="color:#b44">value: </span></span></span><span style="display:flex;"><span><span style="color:#b44"> - name: container1 </span></span></span><span style="display:flex;"><span><span style="color:#b44"> image: nginx/nginx </span></span></span><span style="display:flex;"><span><span style="color:#b44">EOF</span> </span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span><span style="color:#888">The AppendOnlyList &#34;testlist&#34; is invalid: value: Invalid value: &#34;array&#34;: Values may only be added </span></span></span></code></pre></div><p>Additionally, to attempt to remove the field once it has been set is also disallowed by the type-scoped validation rule.</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>kubectl apply -f - <span style="color:#b44">&lt;&lt;EOF </span></span></span><span style="display:flex;"><span><span style="color:#b44">--- </span></span></span><span style="display:flex;"><span><span style="color:#b44">apiVersion: stable.example.com/v1 </span></span></span><span style="display:flex;"><span><span style="color:#b44">kind: AppendOnlyList </span></span></span><span style="display:flex;"><span><span style="color:#b44">metadata: </span></span></span><span style="display:flex;"><span><span style="color:#b44"> name: testlist </span></span></span><span style="display:flex;"><span><span style="color:#b44">EOF</span> </span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span><span style="color:#888">The AppendOnlyList &#34;testlist&#34; is invalid: &lt;nil&gt;: Invalid value: &#34;object&#34;: Value is required once set </span></span></span></code></pre></div><h3 id="generated-schema-2">Generated schema</h3> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">openAPIV3Schema</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">value</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">items</span>:<span style="color:#bbb"> </span>...<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">maxItems</span>:<span style="color:#bbb"> </span><span style="color:#666">100</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>array<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">x-kubernetes-validations</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- <span style="color:#008000;font-weight:bold">message</span>:<span style="color:#bbb"> </span>Values may only be added<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">rule</span>:<span style="color:#bbb"> </span>oldSelf.all(x, x in self)<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">x-kubernetes-validations</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- <span style="color:#008000;font-weight:bold">message</span>:<span style="color:#bbb"> </span>Value is required once set<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">rule</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#39;!has(oldSelf.value) || has(self.value)&#39;</span><span style="color:#bbb"> </span></span></span></code></pre></div><h2 id="map-with-append-only-keys-immutable-values">Map with append-only keys, immutable values</h2> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-go" data-lang="go"><span style="display:flex;"><span><span style="color:#080;font-style:italic">// A map which does not allow keys to be removed or their values changed once set. New keys may be added, however. </span></span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic">// +kubebuilder:validation:XValidation:rule=&#34;!has(oldSelf.values) || has(self.values)&#34;, message=&#34;Value is required once set&#34; </span></span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic"></span><span style="color:#a2f;font-weight:bold">type</span> MapAppendOnlyKeys <span style="color:#a2f;font-weight:bold">struct</span> { </span></span><span style="display:flex;"><span> metav1.TypeMeta <span style="color:#b44">`json:&#34;,inline&#34;`</span> </span></span><span style="display:flex;"><span> metav1.ObjectMeta <span style="color:#b44">`json:&#34;metadata,omitempty&#34;`</span> </span></span><span style="display:flex;"><span> </span></span><span style="display:flex;"><span> <span style="color:#080;font-style:italic">// +kubebuilder:validation:Optional </span></span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic"></span> <span style="color:#080;font-style:italic">// +kubebuilder:validation:MaxProperties=10 </span></span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic"></span> <span style="color:#080;font-style:italic">// +kubebuilder:validation:XValidation:rule=&#34;oldSelf.all(key, key in self &amp;&amp; self[key] == oldSelf[key])&#34;,message=&#34;Keys may not be removed and their values must stay the same&#34; </span></span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic"></span> Values <span style="color:#a2f;font-weight:bold">map</span>[<span style="color:#0b0;font-weight:bold">string</span>]<span style="color:#0b0;font-weight:bold">string</span> <span style="color:#b44">`json:&#34;values,omitempty&#34;`</span> </span></span><span style="display:flex;"><span>} </span></span></code></pre></div><ol> <li>Once set, field must not be deleted: <code>!has(oldSelf.values) || has(self.values)</code> (type-scoped)</li> <li>Once a key is added it is not removed nor is its value modified: <code>oldSelf.all(key, key in self &amp;&amp; self[key] == oldSelf[key])</code> (field-scoped)</li> <li>Value may be initially unset: <code>+kubebuilder:validation:Optional</code></li> </ol> <h3 id="example-usage-2">Example usage</h3> <p>Generating and installing the CRD should succeed:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#080;font-style:italic"># Ensure the CRD yaml is generated by controller-gen</span> </span></span><span style="display:flex;"><span>go generate ./... </span></span><span style="display:flex;"><span>kubectl apply -f crds/stable.example.com_mapappendonlykeys.yaml </span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span><span style="color:#888">customresourcedefinition.apiextensions.k8s.io/mapappendonlykeys.stable.example.com created </span></span></span></code></pre></div><p>Creating an initial object with one key within <code>values</code> should be permitted:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>kubectl apply -f - <span style="color:#b44">&lt;&lt;EOF </span></span></span><span style="display:flex;"><span><span style="color:#b44">--- </span></span></span><span style="display:flex;"><span><span style="color:#b44">apiVersion: stable.example.com/v1 </span></span></span><span style="display:flex;"><span><span style="color:#b44">kind: MapAppendOnlyKeys </span></span></span><span style="display:flex;"><span><span style="color:#b44">metadata: </span></span></span><span style="display:flex;"><span><span style="color:#b44"> name: testmap </span></span></span><span style="display:flex;"><span><span style="color:#b44">values: </span></span></span><span style="display:flex;"><span><span style="color:#b44"> key1: value1 </span></span></span><span style="display:flex;"><span><span style="color:#b44">EOF</span> </span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span><span style="color:#888">mapappendonlykeys.stable.example.com/testmap created </span></span></span></code></pre></div><p>Adding new keys to the map should also be permitted:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>kubectl apply -f - <span style="color:#b44">&lt;&lt;EOF </span></span></span><span style="display:flex;"><span><span style="color:#b44">--- </span></span></span><span style="display:flex;"><span><span style="color:#b44">apiVersion: stable.example.com/v1 </span></span></span><span style="display:flex;"><span><span style="color:#b44">kind: MapAppendOnlyKeys </span></span></span><span style="display:flex;"><span><span style="color:#b44">metadata: </span></span></span><span style="display:flex;"><span><span style="color:#b44"> name: testmap </span></span></span><span style="display:flex;"><span><span style="color:#b44">values: </span></span></span><span style="display:flex;"><span><span style="color:#b44"> key1: value1 </span></span></span><span style="display:flex;"><span><span style="color:#b44"> key2: value2 </span></span></span><span style="display:flex;"><span><span style="color:#b44">EOF</span> </span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span><span style="color:#888">mapappendonlykeys.stable.example.com/testmap configured </span></span></span></code></pre></div><p>But if a key is removed, the error messagr from the validation rule should be returned:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>kubectl apply -f - <span style="color:#b44">&lt;&lt;EOF </span></span></span><span style="display:flex;"><span><span style="color:#b44">--- </span></span></span><span style="display:flex;"><span><span style="color:#b44">apiVersion: stable.example.com/v1 </span></span></span><span style="display:flex;"><span><span style="color:#b44">kind: MapAppendOnlyKeys </span></span></span><span style="display:flex;"><span><span style="color:#b44">metadata: </span></span></span><span style="display:flex;"><span><span style="color:#b44"> name: testmap </span></span></span><span style="display:flex;"><span><span style="color:#b44">values: </span></span></span><span style="display:flex;"><span><span style="color:#b44"> key1: value1 </span></span></span><span style="display:flex;"><span><span style="color:#b44">EOF</span> </span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span><span style="color:#888">The MapAppendOnlyKeys &#34;testmap&#34; is invalid: values: Invalid value: &#34;object&#34;: Keys may not be removed and their values must stay the same </span></span></span></code></pre></div><p>If the entire field is removed, the other validation rule is triggered and the operation is prevented. Note that the error message for the validation rule is shown to the user.</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>kubectl apply -f - <span style="color:#b44">&lt;&lt;EOF </span></span></span><span style="display:flex;"><span><span style="color:#b44">--- </span></span></span><span style="display:flex;"><span><span style="color:#b44">apiVersion: stable.example.com/v1 </span></span></span><span style="display:flex;"><span><span style="color:#b44">kind: MapAppendOnlyKeys </span></span></span><span style="display:flex;"><span><span style="color:#b44">metadata: </span></span></span><span style="display:flex;"><span><span style="color:#b44"> name: testmap </span></span></span><span style="display:flex;"><span><span style="color:#b44">EOF</span> </span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span><span style="color:#888">The MapAppendOnlyKeys &#34;testmap&#34; is invalid: &lt;nil&gt;: Invalid value: &#34;object&#34;: Value is required once set </span></span></span></code></pre></div><h3 id="generated-schema-3">Generated schema</h3> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">openAPIV3Schema</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">description</span>:<span style="color:#bbb"> </span>A map which does not allow keys to be removed or their values<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>changed once set. New keys may be added, however.<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">additionalProperties</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>string<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">maxProperties</span>:<span style="color:#bbb"> </span><span style="color:#666">10</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">x-kubernetes-validations</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- <span style="color:#008000;font-weight:bold">message</span>:<span style="color:#bbb"> </span>Keys may not be removed and their values must stay the same<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">rule</span>:<span style="color:#bbb"> </span>oldSelf.all(key, key in self &amp;&amp; self[key] == oldSelf[key])<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">x-kubernetes-validations</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- <span style="color:#008000;font-weight:bold">message</span>:<span style="color:#bbb"> </span>Value is required once set<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">rule</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#39;!has(oldSelf.values) || has(self.values)&#39;</span><span style="color:#bbb"> </span></span></span></code></pre></div><h1 id="going-further">Going further</h1> <p>The above examples showed how CEL rules can be added to kubebuilder types. The same rules can be added directly to OpenAPI if writing a manifest for a CRD by hand.</p> <p>For native types, the same behavior can be achieved using kube-openapi’s marker <a href="https://github.com/kubernetes/kube-openapi/blob/923526ac052c59656d41710b45bbcb03748aa9d6/pkg/generators/extension.go#L69"><code>+validations</code></a>.</p> <p>Usage of CEL within Kubernetes Validation Rules is so much more powerful than what has been shown in this article. For more information please check out <a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules">validation rules</a> in the Kubernetes documentation and <a href="https://kubernetes.io/blog/2022/09/23/crd-validation-rules-beta/">CRD Validation Rules Beta</a> blog post.</p></description></item><item><title>Blog: Kubernetes 1.25: Kubernetes In-Tree to CSI Volume Migration Status Update</title><link>https://kubernetes.io/blog/2022/09/26/storage-in-tree-to-csi-migration-status-update-1.25/</link><pubDate>Mon, 26 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/09/26/storage-in-tree-to-csi-migration-status-update-1.25/</guid><description> <p><strong>Author:</strong> Jiawei Wang (Google)</p> <p>The Kubernetes in-tree storage plugin to <a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">Container Storage Interface (CSI)</a> migration infrastructure has already been <a href="https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/">beta</a> since v1.17. CSI migration was introduced as alpha in Kubernetes v1.14. Since then, SIG Storage and other Kubernetes special interest groups are working to ensure feature stability and compatibility in preparation for CSI Migration feature to go GA.</p> <p>SIG Storage is excited to announce that the core CSI Migration feature is <strong>generally available</strong> in Kubernetes v1.25 release!</p> <p>SIG Storage wrote a blog post in v1.23 for <a href="https://kubernetes.io/blog/2021/12/10/storage-in-tree-to-csi-migration-status-update/">CSI Migration status update</a> which discussed the CSI migration status for each storage driver. It has been a while and this article is intended to give a latest status update on each storage driver for their CSI Migration status in Kubernetes v1.25.</p> <h2 id="quick-recap-what-is-csi-migration-and-why-migrate">Quick recap: What is CSI Migration, and why migrate?</h2> <p>The Container Storage Interface (CSI) was designed to help Kubernetes replace its existing, in-tree storage driver mechanisms - especially vendor specific plugins. Kubernetes support for the <a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#README">Container Storage Interface</a> has been <a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">generally available</a> since Kubernetes v1.13. Support for using CSI drivers was introduced to make it easier to add and maintain new integrations between Kubernetes and storage backend technologies. Using CSI drivers allows for better maintainability (driver authors can define their own release cycle and support lifecycle) and reduce the opportunity for vulnerabilities (with less in-tree code, the risks of a mistake are reduced, and cluster operators can select only the storage drivers that their cluster requires).</p> <p>As more CSI Drivers were created and became production ready, SIG Storage wanted all Kubernetes users to benefit from the CSI model. However, we could not break API compatibility with the existing storage API types due to k8s architecture conventions. The solution we came up with was CSI migration: a feature that translates in-tree APIs to equivalent CSI APIs and delegates operations to a replacement CSI driver.</p> <p>The CSI migration effort enables the replacement of existing in-tree storage plugins such as <code>kubernetes.io/gce-pd</code> or <code>kubernetes.io/aws-ebs</code> with a corresponding <a href="https://kubernetes-csi.github.io/docs/introduction.html">CSI driver</a> from the storage backend. If CSI Migration is working properly, Kubernetes end users shouldn’t notice a difference. Existing <code>StorageClass</code>, <code>PersistentVolume</code> and <code>PersistentVolumeClaim</code> objects should continue to work. When a Kubernetes cluster administrator updates a cluster to enable CSI migration, existing workloads that utilize PVCs which are backed by in-tree storage plugins will continue to function as they always have. However, behind the scenes, Kubernetes hands control of all storage management operations (previously targeting in-tree drivers) to CSI drivers.</p> <p>For example, suppose you are a <code>kubernetes.io/gce-pd</code> user; after CSI migration, you can still use <code>kubernetes.io/gce-pd</code> to provision new volumes, mount existing GCE-PD volumes or delete existing volumes. All existing APIs and Interface will still function correctly. However, the underlying function calls are all going through the <a href="https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver">GCE PD CSI driver</a> instead of the in-tree Kubernetes function.</p> <p>This enables a smooth transition for end users. Additionally as storage plugin developers, we can reduce the burden of maintaining the in-tree storage plugins and eventually remove them from the core Kubernetes binary.</p> <h2 id="timeline-and-status">What is the timeline / status?</h2> <p>The current and targeted releases for each individual driver is shown in the table below:</p> <table> <thead> <tr> <th>Driver</th> <th>Alpha</th> <th>Beta (in-tree deprecated)</th> <th>Beta (on-by-default)</th> <th>GA</th> <th>Target &quot;in-tree plugin&quot; removal</th> </tr> </thead> <tbody> <tr> <td>AWS EBS</td> <td>1.14</td> <td>1.17</td> <td>1.23</td> <td>1.25</td> <td>1.27 (Target)</td> </tr> <tr> <td>Azure Disk</td> <td>1.15</td> <td>1.19</td> <td>1.23</td> <td>1.24</td> <td>1.26 (Target)</td> </tr> <tr> <td>Azure File</td> <td>1.15</td> <td>1.21</td> <td>1.24</td> <td>1.26 (Target)</td> <td>1.28 (Target)</td> </tr> <tr> <td>Ceph FS</td> <td>1.26 (Target)</td> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td>Ceph RBD</td> <td>1.23</td> <td>1.26 (Target)</td> <td>1.27 (Target)</td> <td>1.28 (Target)</td> <td>1.30 (Target)</td> </tr> <tr> <td>GCE PD</td> <td>1.14</td> <td>1.17</td> <td>1.23</td> <td>1.25</td> <td>1.27 (Target)</td> </tr> <tr> <td>OpenStack Cinder</td> <td>1.14</td> <td>1.18</td> <td>1.21</td> <td>1.24</td> <td>1.26 (Target)</td> </tr> <tr> <td>Portworx</td> <td>1.23</td> <td>1.25</td> <td>1.26 (Target)</td> <td>1.27 (Target)</td> <td>1.29 (Target)</td> </tr> <tr> <td>vSphere</td> <td>1.18</td> <td>1.19</td> <td>1.25</td> <td>1.26 (Target)</td> <td>1.28 (Target)</td> </tr> </tbody> </table> <p>The following storage drivers will not have CSI migration support. The <code>scaleio</code>, <code>flocker</code>, <code>quobyte</code> and <code>storageos</code> drivers were removed; the others are deprecated and will be removed from core Kubernetes in the coming releases.</p> <table> <thead> <tr> <th>Driver</th> <th>Deprecated</th> <th>Code Removal</th> </tr> </thead> <tbody> <tr> <td>Flocker</td> <td>1.22</td> <td>1.25</td> </tr> <tr> <td>GlusterFS</td> <td>1.25</td> <td>1.26 (Target)</td> </tr> <tr> <td>Quobyte</td> <td>1.22</td> <td>1.25</td> </tr> <tr> <td>ScaleIO</td> <td>1.16</td> <td>1.22</td> </tr> <tr> <td>StorageOS</td> <td>1.22</td> <td>1.25</td> </tr> </tbody> </table> <h2 id="what-does-it-mean-for-the-core-csi-migration-feature-to-go-ga">What does it mean for the core CSI Migration feature to go GA?</h2> <p>Core CSI Migration goes to GA means that the general framework, core library and API for CSI migration is stable for Kubernetes v1.25 and will be part of future Kubernetes releases as well.</p> <ul> <li>If you are a Kubernetes distribution maintainer, this means if you disabled <code>CSIMigration</code> feature gate previously, you are no longer allowed to do so because the feature gate has been locked.</li> <li>If you are a Kubernetes storage driver developer, this means you can expect no backwards incompatibility changes in the CSI migration library.</li> <li>If you are a Kubernetes maintainer, expect nothing changes from your day to day development flows.</li> <li>If you are a Kubernetes user, expect nothing to change from your day-to-day usage flows. If you encounter any storage related issues, contact the people who operate your cluster (if that's you, contact the provider of your Kubernetes distribution, or get help from the <a href="https://kubernetes.io/community/#discuss">community</a>).</li> </ul> <h2 id="what-does-it-mean-for-the-storage-driver-csi-migration-to-go-ga">What does it mean for the storage driver CSI migration to go GA?</h2> <p>Storage Driver CSI Migration goes to GA means that the specific storage driver supports CSI Migration. Expect feature parity between the in-tree plugin with the CSI driver.</p> <ul> <li>If you are a Kubernetes distribution maintainer, make sure you install the corresponding CSI driver on the distribution. And make sure you are not disabling the specific <code>CSIMigration{provider}</code> flag, as they are locked.</li> <li>If you are a Kubernetes storage driver maintainer, make sure the CSI driver can ensure feature parity if it supports CSI migration.</li> <li>If you are a Kubernetes maintainer/developer, expect nothing to change from your day-to-day development flows.</li> <li>If you are a Kubernetes user, the CSI Migration feature should be completely transparent to you, the only requirement is to install the corresponding CSI driver.</li> </ul> <h2 id="what-s-next">What's next?</h2> <p>We are expecting cloud provider in-tree storage plugins code removal to start to happen as part of the v1.26 and v1.27 releases of Kubernetes. More and more drivers that support CSI migration will go GA in the upcoming releases.</p> <h2 id="how-do-i-get-involved">How do I get involved?</h2> <p>The Kubernetes Slack channel <a href="https://kubernetes.slack.com/messages/csi-migration">#csi-migration</a> along with any of the standard <a href="https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact">SIG Storage communication channels</a> are great ways to reach out to the SIG Storage and migration working group teams.</p> <p>This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together. We offer a huge thank you to the contributors who stepped up these last quarters to help move the project forward:</p> <ul> <li>Xing Yang (xing-yang)</li> <li>Hemant Kumar (gnufied)</li> </ul> <p>Special thanks to the following people for the insightful reviews, thorough consideration and valuable contribution to the CSI migration feature:</p> <ul> <li>Andy Zhang (andyzhangz)</li> <li>Divyen Patel (divyenpatel)</li> <li>Deep Debroy (ddebroy)</li> <li>Humble Devassy Chirammal (humblec)</li> <li>Ismail Alidzhikov (ialidzhikov)</li> <li>Jordan Liggitt (liggitt)</li> <li>Matthew Cary (mattcary)</li> <li>Matthew Wong (wongma7)</li> <li>Neha Arora (nearora-msft)</li> <li>Oksana Naumov (trierra)</li> <li>Saad Ali (saad-ali)</li> <li>Michelle Au (msau42)</li> </ul> <p>Those interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the <a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes Storage Special Interest Group (SIG)</a>. We’re rapidly growing and always welcome new contributors.</p></description></item><item><title>Blog: Kubernetes 1.25: CustomResourceDefinition Validation Rules Graduate to Beta</title><link>https://kubernetes.io/blog/2022/09/23/crd-validation-rules-beta/</link><pubDate>Fri, 23 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/09/23/crd-validation-rules-beta/</guid><description> <p><strong>Authors:</strong> Joe Betz (Google), Cici Huang (Google), Kermit Alexander (Google)</p> <p>In Kubernetes 1.25, <a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules">Validation rules for CustomResourceDefinitions</a> (CRDs) have graduated to Beta!</p> <p>Validation rules make it possible to declare how custom resources are validated using the <a href="https://github.com/google/cel-spec">Common Expression Language</a> (CEL). For example:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>apiextensions.k8s.io/v1<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>CustomResourceDefinition<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>...<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">openAPIV3Schema</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>object<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">x-kubernetes-validations</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- <span style="color:#008000;font-weight:bold">rule</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;self.minReplicas &lt;= self.replicas &amp;&amp; self.replicas &lt;= self.maxReplicas&#34;</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">message</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;replicas should be in the range minReplicas..maxReplicas.&#34;</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">properties</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">replicas</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>integer<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>...<span style="color:#bbb"> </span></span></span></code></pre></div><p>Validation rules support a wide range of use cases. To get a sense of some of the capabilities, let's look at a few examples:</p> <table> <thead> <tr> <th>Validation Rule</th> <th>Purpose</th> </tr> </thead> <tbody> <tr> <td><code>self.minReplicas &lt;= self.replicas</code></td> <td>Validate an integer field is less than or equal to another integer field</td> </tr> <tr> <td><code>'Available' in self.stateCounts</code></td> <td>Validate an entry with the 'Available' key exists in a map</td> </tr> <tr> <td><code>self.set1.all(e, !(e in self.set2))</code></td> <td>Validate that the elements of two sets are disjoint</td> </tr> <tr> <td><code>self == oldSelf</code></td> <td>Validate that a required field is immutable once it is set</td> </tr> <tr> <td><code>self.created + self.ttl &lt; self.expired</code></td> <td>Validate that 'expired' date is after a 'create' date plus a 'ttl' duration</td> </tr> </tbody> </table> <p>Validation rules are expressive and flexible. See the <a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules">Validation Rules documentation</a> to learn more about what validation rules are capable of.</p> <h2 id="why-cel">Why CEL?</h2> <p>CEL was chosen as the language for validation rules for a couple reasons:</p> <ul> <li>CEL expressions can easily be inlined into CRD schemas. They are sufficiently expressive to replace the vast majority of CRD validation checks currently implemented in admission webhooks. This results in CRDs that are self-contained and are easier to understand.</li> <li>CEL expressions are compiled and type checked against a CRD's schema &quot;ahead-of-time&quot; (when CRDs are created and updated) allowing them to be evaluated efficiently and safely &quot;runtime&quot; (when custom resources are validated). Even regex string literals in CEL are validated and pre-compiled when CRDs are created or updated.</li> </ul> <h2 id="why-not-use-validation-webhooks">Why not use validation webhooks?</h2> <p>Benefits of using validation rules when compared with validation webhooks:</p> <ul> <li>CRD authors benefit from a simpler workflow since validation rules eliminate the need to develop and maintain a webhook.</li> <li>Cluster administrators benefit by no longer having to install, upgrade and operate webhooks for the purposes of CRD validation.</li> <li>Cluster operability improves because CRD validation no longer requires a remote call to a webhook endpoint, eliminating a potential point of failure in the request-serving-path of the Kubernetes API server. This allows clusters to retain high availability while scaling to larger amounts of installed CRD extensions, since expected control plane availability would otherwise decrease with each additional webhook installed.</li> </ul> <h2 id="getting-started-with-validation-rules">Getting started with validation rules</h2> <h3 id="writing-validation-rules-in-openapiv3-schemas">Writing validation rules in OpenAPIv3 schemas</h3> <p>You can define validation rules for any level of a CRD's OpenAPIv3 schema. Validation rules are automatically scoped to their location in the schema where they are declared.</p> <p>Good practices for CRD validation rules:</p> <ul> <li>Scope validation rules as close as possible to the fields(s) they validate.</li> <li>Use multiple rules when validating independent constraints.</li> <li>Do not use validation rules for validations already</li> <li>Use OpenAPIv3 <a href="https://swagger.io/specification/#properties">value validations</a> (<code>maxLength</code>, <code>maxItems</code>, <code>maxProperties</code>, <code>required</code>, <code>enum</code>, <code>minimum</code>, <code>maximum</code>, ..) and <a href="https://swagger.io/docs/specification/data-models/data-types/#format">string formats</a> where available.</li> <li>Use <code>x-kubernetes-int-or-string</code>, <code>x-kubernetes-embedded-type</code> and <code>x-kubernetes-list-type=(set|map)</code> were appropriate.</li> </ul> <p>Examples of good practice:</p> <table> <thead> <tr> <th>Validation</th> <th>Best Practice</th> <th>Example(s)</th> </tr> </thead> <tbody> <tr> <td>Validate an integer is between 0 and 100.</td> <td>Use OpenAPIv3 value validations.</td> <td><pre>type: integer<br>minimum: 0<br>maximum: 100</pre></td> </tr> <tr> <td>Constraint the max size limits on maps (objects with additionalProperties), arrays and string.</td> <td>Use OpenAPIv3 value validations. Recommended for all maps, arrays and strings. This best practice is essential for rule cost estimation (explained below).</td> <td><pre>type:<br>maxItems: 100</pre></td> </tr> <tr> <td>Require a date-time be more recent than a particular timestamp.</td> <td>Use OpenAPIv3 string formats to declare that the field is a date-time. Use validation rules to compare it to a particular timestamp.</td> <td><pre>type: string<br>format: date-time<br>x-kubernetes-validations:<br> - rule: &quot;self &gt;= timestamp('2000-01-01T00:00:00.000Z')&quot;</pre></td> </tr> <tr> <td>Require two sets to be disjoint.</td> <td>Use x-kubernetes-list-type to validate that the arrays are sets. <br>Use validation rules to validate the sets are disjoint.</td> <td><pre>type: object<br>properties:<br> set1:<br> type: array<br> x-kubernetes-list-type: set<br> set2: ...<br> x-kubernetes-validations:<br> - rule: &quot;!self.set1.all(e, !(e in self.set2))&quot;</pre></td> </tr> </tbody> </table> <h2 id="crd-transition-rules">CRD transition rules</h2> <p><a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#transition-rules">Transition Rules</a> make it possible to compare the new state against the old state of a resource in validation rules. You use transition rules to make sure that the cluster's API server does not accept invalid state transitions. A transition rule is a validation rule that references 'oldSelf'. The API server only evaluates transition rules when both an old value and new value exist.</p> <p>Transition rule examples:</p> <table> <thead> <tr> <th>Transition Rule</th> <th>Purpose</th> </tr> </thead> <tbody> <tr> <td><code>self == oldSelf</code></td> <td>For a required field, make that field immutable once it is set. For an optional field, only allow transitioning from unset to set, or from set to unset.</td> </tr> <tr> <td>(on parent of field) <code>has(self.field) == has(oldSelf.field)</code><br>on field: <code>self == oldSelf</code></td> <td>Make a field immutable: validate that a field, even if optional, never changes after the resource is created (for a required field, the previous rule is simpler).</td> </tr> <tr> <td><code>self.all(x, x in oldSelf)</code></td> <td>Only allow adding items to a field that represents a set (prevent removals).</td> </tr> <tr> <td><code>self &gt;= oldSelf</code></td> <td>Validate that a number is monotonically increasing.</td> </tr> </tbody> </table> <h2 id="using-the-functions-libraries">Using the Functions Libraries</h2> <p>Validation rules have access to a couple different function libraries:</p> <ul> <li>CEL standard functions, defined in the <a href="https://github.com/google/cel-spec/blob/v0.7.0/doc/langdef.md#list-of-standard-definitions">list of standard definitions</a></li> <li>CEL standard <a href="https://github.com/google/cel-spec/blob/v0.7.0/doc/langdef.md#macros">macros</a></li> <li>CEL <a href="https://pkg.go.dev/github.com/google/cel-go/ext#Strings">extended string function library</a></li> <li>Kubernetes <a href="https://pkg.go.dev/k8s.io/apiextensions-apiserver/pkg/apiserver/schema/cel/library#pkg-functions">CEL extension library</a> which includes supplemental functions for <a href="https://pkg.go.dev/k8s.io/apiextensions-apiserver/pkg/apiserver/schema/cel/library#pkg-functions">lists</a>, <a href="https://pkg.go.dev/k8s.io/apiextensions-apiserver/pkg/apiserver/schema/cel/library#Regex">regex</a>, and <a href="https://pkg.go.dev/k8s.io/apiextensions-apiserver/pkg/apiserver/schema/cel/library#Regex">URLs</a>.</li> </ul> <p>Examples of function libraries in use:</p> <table> <thead> <tr> <th>Validation Rule</th> <th>Purpose</th> </tr> </thead> <tbody> <tr> <td><code>!(self.getDayOfWeek() in [0, 6])</code></td> <td>Validate that a date is not a Sunday or Saturday.</td> </tr> <tr> <td><code>isUrl(self) &amp;&amp; url(self).getHostname() in [a.example.com', 'b.example.com']</code></td> <td>Validate that a URL has an allowed hostname.</td> </tr> <tr> <td><code>self.map(x, x.weight).sum() == 1</code></td> <td>Validate that the weights of a list of objects sum to 1.</td> </tr> <tr> <td><code>int(self.find('^[0-9]*')) &lt; 100</code></td> <td>Validate that a string starts with a number less than 100.</td> </tr> <tr> <td><code>self.isSorted()</code></td> <td>Validates that a list is sorted.</td> </tr> </tbody> </table> <h2 id="resource-use-and-limits">Resource use and limits</h2> <p>To prevent CEL evaluation from consuming excessive compute resources, validation rules impose some limits. These limits are based on CEL <em>cost units</em>, a platform and machine independent measure of execution cost. As a result, the limits are the same regardless of where they are enforced.</p> <h3 id="estimated-cost-limit">Estimated cost limit</h3> <p>CEL is, by design, non-Turing-complete in such a way that the halting problem isn’t a concern. CEL takes advantage of this design choice to include an &quot;estimated cost&quot; subsystem that can statically compute the worst case run time cost of any CEL expression. Validation rules are <a href="o/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#resource-use-by-validation-functions">integrated with the estimated cost system</a> and disallow CEL expressions from being included in CRDs if they have a sufficiently poor (high) estimated cost. The estimated cost limit is set quite high and typically requires an O(n^2) or worse operation, across something of unbounded size, to be exceeded. Fortunately the fix is usually quite simple: because the cost system is aware of size limits declared in the CRD's schema, CRD authors can add size limits to the CRD's schema (<code>maxItems</code> for arrays, <code>maxProperties</code> for maps, <code>maxLength</code> for strings) to reduce the estimated cost.</p> <p>Good practice:</p> <p>Set <code>maxItems</code>, <code>maxProperties</code> and <code>maxLength</code> on all array, map (<code>object</code> with <code>additionalProperties</code>) and string types in CRD schemas! This results in lower and more accurate estimated costs and generally makes a CRD safer to use.</p> <h3 id="runtime-cost-limits-for-crd-validation-rules">Runtime cost limits for CRD validation rules</h3> <p>In addition to the estimated cost limit, CEL keeps track of actual cost while evaluating a CEL expression and will halt execution of the expression if a limit is exceeded.</p> <p>With the estimated cost limit already in place, the runtime cost limit is rarely encountered. But it is possible. For example, it might be encountered for a large resource composed entirely of a single large list and a validation rule that is either evaluated on each element in the list, or traverses the entire list.</p> <p>CRD authors can ensure the runtime cost limit will not be exceeded in much the same way the estimated cost limit is avoided: by setting <code>maxItems</code>, <code>maxProperties</code> and <code>maxLength</code> on array, map and string types.</p> <h2 id="future-work">Future work</h2> <p>We look forward to working with the community on the adoption of CRD Validation Rules, and hope to see this feature promoted to general availability in an upcoming Kubernetes release!</p> <p>There is a growing community of Kubernetes contributors thinking about how to make it possible to write extensible admission controllers using CEL as a substitute for admission webhooks for policy enforcement use cases. Anyone interested should reach out to us on the usual <a href="https://github.com/kubernetes/community/tree/master/sig-api-machinery">SIG API Machinery</a> channels or via slack at <a href="https://kubernetes.slack.com/archives/C02TTBG6LF4">#sig-api-machinery-cel-dev</a>.</p> <h2 id="acknowledgements">Acknowledgements</h2> <p>Special thanks to Cici Huang, Ben Luddy, Jordan Liggitt, David Eads, Daniel Smith, Dr. Stefan Schimanski, Leila Jalali and everyone who contributed to Validation Rules!</p></description></item><item><title>Blog: Kubernetes 1.25: Use Secrets for Node-Driven Expansion of CSI Volumes</title><link>https://kubernetes.io/blog/2022/09/21/kubernetes-1-25-use-secrets-while-expanding-csi-volumes-on-node-alpha/</link><pubDate>Wed, 21 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/09/21/kubernetes-1-25-use-secrets-while-expanding-csi-volumes-on-node-alpha/</guid><description> <p><strong>Author:</strong> Humble Chirammal (Red Hat), Louis Koo (deeproute.ai)</p> <p>Kubernetes v1.25, released earlier this month, introduced a new feature that lets your cluster expand storage volumes, even when access to those volumes requires a secret (for example: a credential for accessing a SAN fabric) to perform node expand operation. This new behavior is in alpha and you must enable a feature gate (<code>CSINodeExpandSecret</code>) to make use of it. You must also be using <a href="https://kubernetes-csi.github.io/docs/">CSI</a> storage; this change isn't relevant to storage drivers that are built in to Kubernetes.</p> <p>To turn on this new, alpha feature, you enable the <code>CSINodeExpandSecret</code> feature gate for the kube-apiserver and kubelet, which turns on a mechanism to send <code>secretRef</code> configuration as part of NodeExpansion by the CSI drivers thus make use of the same to perform node side expansion operation with the underlying storage system.</p> <h2 id="what-is-this-all-about">What is this all about?</h2> <p>Before Kubernetes v1.24, you were able to define a cluster-level StorageClass that made use of <a href="https://kubernetes-csi.github.io/docs/secrets-and-credentials-storage-class.html">StorageClass Secrets</a>, but you didn't have any mechanism to specify the credentials that would be used for operations that take place when the storage was mounted onto a node and when the volume has to be expanded at node side.</p> <p>The Kubernetes CSI already implemented a similar mechanism specific kinds of volume resizes; namely, resizes of PersistentVolumes where the resizes take place independently from any node referred as Controller Expansion. In that case, you associate a PersistentVolume with a Secret that contains credentials for volume resize actions, so that controller expansion can take place. CSI also supports a <code>nodeExpandVolume</code> operation which CSI drivers can make use independent of Controller Expansion or along with Controller Expansion on which, where the resize is driven from a node in your cluster where the volume is attached. Please read <a href="https://kubernetes.io/blog/2022/05/05/volume-expansion-ga/">Kubernetes 1.24: Volume Expansion Now A Stable Feature</a></p> <ul> <li> <p>At times, the CSI driver needs to check the actual size of the backend block storage (or image) before proceeding with a node-level filesystem expand operation. This avoids false positive returns from the backend storage cluster during filesystem expands.</p> </li> <li> <p>When a PersistentVolume represents encrypted block storage (for example using LUKS) you need to provide a passphrase in order to expand the device, and also to make it possible to grow the filesystem on that device.</p> </li> <li> <p>For various validations at time of node expansion, the CSI driver has to be connected to the backend storage cluster. If the <code>nodeExpandVolume</code> request includes a <code>secretRef</code> then the CSI driver can make use of the same and connect to the storage cluster to perform the cluster operations.</p> </li> </ul> <h2 id="how-does-it-work">How does it work?</h2> <p>To enable this functionality from this version of Kubernetes, SIG Storage have introduced a new feature gate called <code>CSINodeExpandSecret</code>. Once the feature gate is enabled in the cluster, NodeExpandVolume requests can include a <code>secretRef</code> field. The NodeExpandVolume request is part of CSI; for example, in a request which has been sent from the Kubernetes control plane to the CSI driver.</p> <p>As a cluster operator, you admin can specify these secrets as an opaque parameter in a StorageClass, the same way that you can already specify other CSI secret data. The StorageClass needs to have some CSI-specific parameters set. Here's an example of those parameters:</p> <pre tabindex="0"><code>csi.storage.k8s.io/node-expand-secret-name: test-secret csi.storage.k8s.io/node-expand-secret-namespace: default </code></pre><p>If feature gates are enabled and storage class carries the above secret configuration, the CSI provisioner receives the credentials from the Secret as part of the NodeExpansion request.</p> <p>CSI volumes that require secrets for online expansion will have NodeExpandSecretRef field set. If not set, the NodeExpandVolume CSI RPC call will be made without a secret.</p> <h2 id="trying-it-out">Trying it out</h2> <ol> <li> <p>Enable the <code>CSINodeExpandSecret</code> feature gate (please refer to <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">Feature Gates</a>).</p> </li> <li> <p>Create a Secret, and then a StorageClass that uses that Secret.</p> </li> </ol> <p>Here's an example manifest for a Secret that holds credentials:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Secret<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-secret<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">data</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">stringData</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">username</span>:<span style="color:#bbb"> </span>admin<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">password</span>:<span style="color:#bbb"> </span>t0p-Secret<span style="color:#bbb"> </span></span></span></code></pre></div><p>Here's an example manifest for a StorageClass that refers to those credentials:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>storage.k8s.io/v1<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>StorageClass<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>csi-blockstorage-sc<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">parameters</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">csi.storage.k8s.io/node-expand-secret-name</span>:<span style="color:#bbb"> </span>test-secret <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># the name of the Secret</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">csi.storage.k8s.io/node-expand-secret-namespace</span>:<span style="color:#bbb"> </span>default <span style="color:#bbb"> </span><span style="color:#080;font-style:italic"># the namespace that the Secret is in</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">provisioner</span>:<span style="color:#bbb"> </span>blockstorage.cloudprovider.example<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">reclaimPolicy</span>:<span style="color:#bbb"> </span>Delete<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">volumeBindingMode</span>:<span style="color:#bbb"> </span>Immediate<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">allowVolumeExpansion</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb"> </span></span></span></code></pre></div><h2 id="example-output">Example output</h2> <p>If the PersistentVolumeClaim (PVC) was created successfully, you can see that configuration within the <code>spec.csi</code> field of the PersistentVolume (look for <code>spec.csi.nodeExpandSecretRef</code>). Check that it worked by running <code>kubectl get persistentvolume &lt;pv_name&gt; -o yaml</code>. You should see something like.</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PersistentVolume<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">pv.kubernetes.io/provisioned-by</span>:<span style="color:#bbb"> </span>blockstorage.cloudprovider.example<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">creationTimestamp</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2022-08-26T15:14:07Z&#34;</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">finalizers</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- kubernetes.io/pv-protection<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>pvc-95eb531a-d675-49f6-940b-9bc3fde83eb0<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">resourceVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;420263&#34;</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span>6fa824d7-8a06-4e0c-b722-d3f897dcbd65<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- ReadWriteOnce<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">capacity</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>6Gi<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">claimRef</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>csi-pvc<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">resourceVersion</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;419862&#34;</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">uid</span>:<span style="color:#bbb"> </span>95eb531a-d675-49f6-940b-9bc3fde83eb0<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">csi</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">driver</span>:<span style="color:#bbb"> </span>blockstorage.cloudprovider.example<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">nodeExpandSecretRef</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-secret<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">volumeAttributes</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">storage.kubernetes.io/csiProvisionerIdentity</span>:<span style="color:#bbb"> </span><span style="color:#666">1648042783218-8081</span>-blockstorage.cloudprovider.example<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">volumeHandle</span>:<span style="color:#bbb"> </span>e21c7809-aabb-11ec-917a-2e2e254eb4cf<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">nodeAffinity</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">required</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">nodeSelectorTerms</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- <span style="color:#008000;font-weight:bold">matchExpressions</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- <span style="color:#008000;font-weight:bold">key</span>:<span style="color:#bbb"> </span>topology.hostpath.csi/node<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">operator</span>:<span style="color:#bbb"> </span>In<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">values</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- racknode01<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">persistentVolumeReclaimPolicy</span>:<span style="color:#bbb"> </span>Delete<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">storageClassName</span>:<span style="color:#bbb"> </span>csi-blockstorage-sc<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">volumeMode</span>:<span style="color:#bbb"> </span>Filesystem<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">status</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">phase</span>:<span style="color:#bbb"> </span>Bound<span style="color:#bbb"> </span></span></span></code></pre></div><p>If you then trigger online storage expansion, the kubelet passes the appropriate credentials to the CSI driver, by loading that Secret and passing the data to the storage driver.</p> <p>Here's an example debug log:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span><span style="color:#888">I0330 03:29:51.966241 1 server.go:101] GRPC call: /csi.v1.Node/NodeExpandVolume </span></span></span><span style="display:flex;"><span><span style="color:#888">I0330 03:29:51.966261 1 server.go:105] GRPC request: {&#34;capacity_range&#34;:{&#34;required_bytes&#34;:7516192768},&#34;secrets&#34;:&#34;***stripped***&#34;,&#34;staging_target_path&#34;:&#34;/var/lib/kubelet/plugins/kubernetes.io/csi/blockstorage.cloudprovider.example/f7c62e6e08ce21e9b2a95c841df315ed4c25a15e91d8fcaf20e1c2305e5300ab/globalmount&#34;,&#34;volume_capability&#34;:{&#34;AccessType&#34;:{&#34;Mount&#34;:{}},&#34;access_mode&#34;:{&#34;mode&#34;:7}},&#34;volume_id&#34;:&#34;e21c7809-aabb-11ec-917a-2e2e254eb4cf&#34;,&#34;volume_path&#34;:&#34;/var/lib/kubelet/pods/bcb1b2c4-5793-425c-acf1-47163a81b4d7/volumes/kubernetes.io~csi/pvc-95eb531a-d675-49f6-940b-9bc3fde83eb0/mount&#34;} </span></span></span><span style="display:flex;"><span><span style="color:#888">I0330 03:29:51.966360 1 nodeserver.go:459] req:volume_id:&#34;e21c7809-aabb-11ec-917a-2e2e254eb4cf&#34; volume_path:&#34;/var/lib/kubelet/pods/bcb1b2c4-5793-425c-acf1-47163a81b4d7/volumes/kubernetes.io~csi/pvc-95eb531a-d675-49f6-940b-9bc3fde83eb0/mount&#34; capacity_range:&lt;required_bytes:7516192768 &gt; staging_target_path:&#34;/var/lib/kubelet/plugins/kubernetes.io/csi/blockstorage.cloudprovider.example/f7c62e6e08ce21e9b2a95c841df315ed4c25a15e91d8fcaf20e1c2305e5300ab/globalmount&#34; volume_capability:&lt;mount:&lt;&gt; access_mode:&lt;mode:SINGLE_NODE_MULTI_WRITER &gt; &gt; secrets:&lt;key:&#34;XXXXXX&#34; value:&#34;XXXXX&#34; &gt; secrets:&lt;key:&#34;XXXXX&#34; value:&#34;XXXXXX&#34; &gt; </span></span></span></code></pre></div><h2 id="the-future">The future</h2> <p>As this feature is still in alpha, Kubernetes Storage SIG expect to update or get feedback from CSI driver authors with more tests and implementation. The community plans to eventually promote the feature to Beta in upcoming releases.</p> <h2 id="get-involved-or-learn-more">Get involved or learn more?</h2> <p>The enhancement proposal includes lots of detail about the history and technical implementation of this feature.</p> <p>To learn more about StorageClass based dynamic provisioning in Kubernetes, please refer to <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/">Storage Classes</a> and <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">Persistent Volumes</a>.</p> <p>Please get involved by joining the Kubernetes <a href="https://github.com/kubernetes/community/blob/master/sig-storage/README.md">Storage SIG</a> (Special Interest Group) to help us enhance this feature. There are a lot of good ideas already and we'd be thrilled to have more!</p></description></item><item><title>Blog: Kubernetes 1.25: Local Storage Capacity Isolation Reaches GA</title><link>https://kubernetes.io/blog/2022/09/19/local-storage-capacity-isolation-ga/</link><pubDate>Mon, 19 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/09/19/local-storage-capacity-isolation-ga/</guid><description> <p><strong>Author:</strong> Jing Xu (Google)</p> <p>Local ephemeral storage capacity isolation was introduced as a alpha feature in Kubernetes 1.7 and it went beta in 1.9. With Kubernetes 1.25 we are excited to announce general availability(GA) of this feature.</p> <p>Pods use ephemeral local storage for scratch space, caching, and logs. The lifetime of local ephemeral storage does not extend beyond the life of the individual pod. It is exposed to pods using the container’s writable layer, logs directory, and <code>EmptyDir</code> volumes. Before this feature was introduced, there were issues related to the lack of local storage accounting and isolation, such as Pods not knowing how much local storage is available and being unable to request guaranteed local storage. Local storage is a best-effort resource and pods can be evicted due to other pods filling the local storage.</p> <p>The <a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#local-ephemeral-storage">local storage capacity isolation feature</a> allows users to manage local ephemeral storage in the same way as managing CPU and memory. It provides support for capacity isolation of shared storage between pods, such that a pod can be hard limited in its consumption of shared resources by evicting Pods if its consumption of shared storage exceeds that limit. It also allows setting ephemeral storage requests for resource reservation. The limits and requests for shared <code>ephemeral-storage</code> are similar to those for memory and CPU consumption.</p> <h3 id="how-to-use-local-storage-capacity-isolation">How to use local storage capacity isolation</h3> <p>A typical configuration for local ephemeral storage is to place all different kinds of ephemeral local data (emptyDir volumes, writeable layers, container images, logs) into one filesystem. Typically, both /var/lib/kubelet and /var/log are on the system's root filesystem. If users configure the local storage in different ways, kubelet might not be able to correctly measure disk usage and use this feature.</p> <h4 id="setting-requests-and-limits-for-local-ephemeral-storage">Setting requests and limits for local ephemeral storage</h4> <p>You can specify <code>ephemeral-storage</code> for managing local ephemeral storage. Each container of a Pod can specify either or both of the following:</p> <ul> <li><code>spec.containers[].resources.limits.ephemeral-storage</code></li> <li><code>spec.containers[].resources.requests.ephemeral-storage</code></li> </ul> <p>In the following example, the Pod has two containers. The first container has a request of 8GiB of local ephemeral storage and a limit of 12GiB. The second container requests 2GiB of local storage, but no limit setting. Therefore, the Pod requests a total of 10GiB (8GiB+2GiB) of local ephemeral storage and enforces a limit of 12GiB of local ephemeral storage. It also sets emptyDir sizeLimit to 5GiB. With this setting in pod spec, it will affect how the scheduler makes a decision on scheduling pods and also how kubelet evict pods.</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>frontend<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>app<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>images.my-company.example/app:v4<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">ephemeral-storage</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;8Gi&#34;</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">ephemeral-storage</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;12Gi&#34;</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>ephemeral<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/tmp&#34;</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>log-aggregator<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>images.my-company.example/log-aggregator:v6<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">ephemeral-storage</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;2Gi&#34;</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>ephemeral<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/tmp&#34;</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>ephemeral<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">emptyDir</span>:<span style="color:#bbb"> </span>{}<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">sizeLimit</span>:<span style="color:#bbb"> </span>5Gi<span style="color:#bbb"> </span></span></span></code></pre></div><p>First of all, the scheduler ensures that the sum of the resource requests of the scheduled containers is less than the capacity of the node. In this case, the pod can be assigned to a node only if its available ephemeral storage (allocatable resource) has more than 10GiB.</p> <p>Secondly, at container level, since one of the container sets resource limit, kubelet eviction manager will measure the disk usage of this container and evict the pod if the storage usage of the first container exceeds its limit (12GiB). At pod level, kubelet works out an overall Pod storage limit by adding up the limits of all the containers in that Pod. In this case, the total storage usage at pod level is the sum of the disk usage from all containers plus the Pod's <code>emptyDir</code> volumes. If this total usage exceeds the overall Pod storage limit (12GiB), then the kubelet also marks the Pod for eviction.</p> <p>Last, in this example, emptyDir volume sets its sizeLimit to 5Gi. It means that if this pod's emptyDir used up more local storage than 5GiB, the pod will be evicted from the node.</p> <h4 id="setting-resource-quota-and-limitrange-for-local-ephemeral-storage">Setting resource quota and limitRange for local ephemeral storage</h4> <p>This feature adds two more resource quotas for storage. The request and limit set constraints on the total requests/limits of all containers’ in a namespace.</p> <ul> <li><code>requests.ephemeral-storage</code></li> <li><code>limits.ephemeral-storage</code></li> </ul> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>ResourceQuota<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>storage-resources<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">hard</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">requests.ephemeral-storage</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;10Gi&#34;</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">limits.ephemeral-storage</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;20Gi&#34;</span><span style="color:#bbb"> </span></span></span></code></pre></div><p>Similar to CPU and memory, admin could use LimitRange to set default container’s local storage request/limit, and/or minimum/maximum resource constraints for a namespace.</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>LimitRange<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>storage-limit-range<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">limits</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- <span style="color:#008000;font-weight:bold">default</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">ephemeral-storage</span>:<span style="color:#bbb"> </span>10Gi<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">defaultRequest</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">ephemeral-storage</span>:<span style="color:#bbb"> </span>5Gi<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>Container<span style="color:#bbb"> </span></span></span></code></pre></div><p>Also, ephemeral-storage may be specified to reserve for kubelet or system. example, <code>--system-reserved=[cpu=100m][,][memory=100Mi][,][ephemeral-storage=10Gi][,][pid=1000] --kube-reserved=[cpu=100m][,][memory=100Mi][,][ephemeral-storage=5Gi][,][pid=1000]</code>. If your cluster node root disk capacity is 100Gi, after setting system-reserved and kube-reserved value, the available allocatable ephemeral storage would become 85Gi. The schedule will use this information to assign pods based on request and allocatable resources from each node. The eviction manager will also use allocatable resource to determine pod eviction. See more details from <a href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/">Reserve Compute Resources for System Daemons</a>.</p> <h3 id="how-do-i-get-involved">How do I get involved?</h3> <p>This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together.</p> <p>We offer a huge thank you to all the contributors in <a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes Storage SIG</a> and CSI community who helped review the design and implementation of the project, including but not limited to the following:</p><ul><li>Benjamin Elder (<a href=https://github.com/BenTheElder>BenTheElder</a>)</li><li>Michelle Au (<a href=https://github.com/msau42>msau42</a>)</li><li>Tim Hockin (<a href=https://github.com/thockin>thockin</a>)</li><li>Jordan Liggitt (<a href=https://github.com/liggitt>liggitt</a>)</li><li>Xing Yang (<a href=https://github.com/xing-yang>xing-yang</a>)</li></p></description></item><item><title>Blog: Kubernetes 1.25: Two Features for Apps Rollouts Graduate to Stable</title><link>https://kubernetes.io/blog/2022/09/15/app-rollout-features-reach-stable/</link><pubDate>Thu, 15 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/09/15/app-rollout-features-reach-stable/</guid><description> <p><strong>Authors:</strong> Ravi Gudimetla (Apple), Filip Křepinský (Red Hat), Maciej Szulik (Red Hat)</p> <p>This blog describes the two features namely <code>minReadySeconds</code> for StatefulSets and <code>maxSurge</code> for DaemonSets that SIG Apps is happy to graduate to stable in Kubernetes 1.25.</p> <p>Specifying <code>minReadySeconds</code> slows down a rollout of a StatefulSet, when using a <code>RollingUpdate</code> value in <code>.spec.updateStrategy</code> field, by waiting for each pod for a desired time. This time can be used for initializing the pod (e.g. warming up the cache) or as a delay before acknowledging the pod.</p> <p><code>maxSurge</code> allows a DaemonSet workload to run multiple instances of the same pod on a node during a rollout when using a <code>RollingUpdate</code> value in <code>.spec.updateStrategy</code> field. This helps to minimize the downtime of the DaemonSet for consumers.</p> <p>These features were already available in a Deployment and other workloads. This graduation helps to align this functionality across the workloads.</p> <h2 id="what-problems-do-these-features-solve">What problems do these features solve?</h2> <h3 id="solved-problem-statefulset-minreadyseconds">minReadySeconds for StatefulSets</h3> <p><code>minReadySeconds</code> ensures that the StatefulSet workload is <code>Ready</code> for the given number of seconds before reporting the pod as <code>Available</code>. The notion of being <code>Ready</code> and <code>Available</code> is quite important for workloads. For example, some workloads, like Prometheus with multiple instances of Alertmanager, should be considered <code>Available</code> only when the Alertmanager's state transfer is complete. <code>minReadySeconds</code> also helps when using loadbalancers with cloud providers. Since the pod should be <code>Ready</code> for the given number of seconds, it provides buffer time to prevent killing pods in rotation before new pods show up.</p> <h3 id="how-use-daemonset-maxsurge">maxSurge for DaemonSets</h3> <p>Kubernetes system-level components like CNI, CSI are typically run as DaemonSets. These components can have impact on the availability of the workloads if those DaemonSets go down momentarily during the upgrades. The feature allows DaemonSet pods to temporarily increase their number, thereby ensuring zero-downtime for the DaemonSets.</p> <p>Please note that the usage of <code>hostPort</code> in conjunction with <code>maxSurge</code> in DaemonSets is not allowed as DaemonSet pods are tied to a single node and two active pods cannot share the same port on the same node.</p> <h2 id="how-does-it-work">How does it work?</h2> <h3 id="how-does-statefulset-minreadyseconds-work">minReadySeconds for StatefulSets</h3> <p>The StatefulSet controller watches for the StatefulSet pods and counts how long a particular pod has been in the <code>Running</code> state, if this value is greater than or equal to the time specified in <code>.spec.minReadySeconds</code> field of the StatefulSet, the StatefulSet controller updates the <code>AvailableReplicas</code> field in the StatefulSet's status.</p> <h3 id="how-does-daemonset-maxsurge-work">maxSurge for DaemonSets</h3> <p>The DaemonSet controller creates the additional pods (above the desired number resulting from DaemonSet spec) based on the value given in <code>.spec.strategy.rollingUpdate.maxSurge</code>. The additional pods would run on the same node where the old DaemonSet pod is running till the old pod gets killed.</p> <ul> <li>The default value is 0.</li> <li>The value cannot be <code>0</code> when <code>MaxUnavailable</code> is 0.</li> <li>The value can be specified either as an absolute number of pods, or a percentage (rounded up) of desired pods.</li> </ul> <h2 id="how-do-i-use-it">How do I use it?</h2> <h3 id="how-use-statefulset-minreadyseconds">minReadySeconds for StatefulSets</h3> <p>Specify a value for <code>minReadySeconds</code> for any StatefulSet and check if pods are available or not by inspecting <code>AvailableReplicas</code> field using:</p> <p><code>kubectl get statefulset/&lt;name_of_the_statefulset&gt; -o yaml</code></p> <p>Please note that the default value of <code>minReadySeconds</code> is 0.</p> <h3 id="how-use-daemonset-maxsurge">maxSurge for DaemonSets</h3> <p>Specify a value for <code>.spec.updateStrategy.rollingUpdate.maxSurge</code> and set <code>.spec.updateStrategy.rollingUpdate.maxUnavailable</code> to <code>0</code>.</p> <p>Then observe a faster rollout and higher number of pods running at the same time in the next rollout.</p> <pre tabindex="0"><code>kubectl rollout restart daemonset &lt;name_of_the_daemonset&gt; kubectl get pods -w </code></pre><h2 id="how-can-i-learn-more">How can I learn more?</h2> <h3 id="learn-more-statefulset-minreadyseconds">minReadySeconds for StatefulSets</h3> <ul> <li>Documentation: <a href="https://k8s.io/docs/concepts/workloads/controllers/statefulset/#minimum-ready-seconds">https://k8s.io/docs/concepts/workloads/controllers/statefulset/#minimum-ready-seconds</a></li> <li>KEP: <a href="https://github.com/kubernetes/enhancements/issues/2599">https://github.com/kubernetes/enhancements/issues/2599</a></li> <li>API Changes: <a href="https://github.com/kubernetes/kubernetes/pull/100842">https://github.com/kubernetes/kubernetes/pull/100842</a></li> </ul> <h3 id="learn-more-daemonset-maxsurge">maxSurge for DaemonSets</h3> <ul> <li>Documentation: <a href="https://k8s.io/docs/tasks/manage-daemon/update-daemon-set/">https://k8s.io/docs/tasks/manage-daemon/update-daemon-set/</a></li> <li>KEP: <a href="https://github.com/kubernetes/enhancements/issues/1591">https://github.com/kubernetes/enhancements/issues/1591</a></li> <li>API Changes: <a href="https://github.com/kubernetes/kubernetes/pull/96375">https://github.com/kubernetes/kubernetes/pull/96375</a></li> </ul> <h2 id="how-do-i-get-involved">How do I get involved?</h2> <p>Please reach out to us on <a href="https://kubernetes.slack.com/archives/C18NZM5K9">#sig-apps</a> channel on Slack, or through the SIG Apps mailing list <a href="https://groups.google.com/g/kubernetes-sig-apps">kubernetes-sig-apps@googlegroups.com</a>.</p></description></item><item><title>Blog: Kubernetes 1.25: PodHasNetwork Condition for Pods</title><link>https://kubernetes.io/blog/2022/09/14/pod-has-network-condition/</link><pubDate>Wed, 14 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/09/14/pod-has-network-condition/</guid><description> <p><strong>Author:</strong> Deep Debroy (Apple)</p> <p>Kubernetes 1.25 introduces Alpha support for a new kubelet-managed pod condition in the status field of a pod: <code>PodHasNetwork</code>. The kubelet, for a worker node, will use the <code>PodHasNetwork</code> condition to accurately surface the initialization state of a pod from the perspective of pod sandbox creation and network configuration by a container runtime (typically in coordination with CNI plugins). The kubelet starts to pull container images and start individual containers (including init containers) after the status of the <code>PodHasNetwork</code> condition is set to <code>&quot;True&quot;</code>. Metrics collection services that report latency of pod initialization from a cluster infrastructural perspective (i.e. agnostic of per container characteristics like image size or payload) can utilize the <code>PodHasNetwork</code> condition to accurately generate Service Level Indicators (SLIs). Certain operators or controllers that manage underlying pods may utilize the <code>PodHasNetwork</code> condition to optimize the set of actions performed when pods repeatedly fail to come up.</p> <h3 id="how-is-this-different-from-the-existing-initialized-condition-reported-for-pods">How is this different from the existing Initialized condition reported for pods?</h3> <p>The kubelet sets the status of the existing <code>Initialized</code> condition reported in the status field of a pod depending on the presence of init containers in a pod.</p> <p>If a pod specifies init containers, the status of the <code>Initialized</code> condition in the pod status will not be set to <code>&quot;True&quot;</code> until all init containers for the pod have succeeded. However, init containers, configured by users, may have errors (payload crashing, invalid image, etc) and the number of init containers configured in a pod may vary across different workloads. Therefore, cluster-wide, infrastructural SLIs around pod initialization cannot depend on the <code>Initialized</code> condition of pods.</p> <p>If a pod does not specify init containers, the status of the <code>Initialized</code> condition in the pod status is set to <code>&quot;True&quot;</code> very early in the lifecycle of the pod. This occurs before the kubelet initiates any pod runtime sandbox creation and network configuration steps. As a result, a pod without init containers will report the status of the <code>Initialized</code> condition as <code>&quot;True&quot;</code> even if the container runtime is not able to successfully initialize the pod sandbox environment.</p> <p>Relative to either situation above, the <code>PodHasNetwork</code> condition surfaces more accurate data around when the pod runtime sandbox was initialized with networking configured so that the kubelet can proceed to launch user-configured containers (including init containers) in the pod.</p> <h3 id="special-cases">Special Cases</h3> <p>If a pod specifies <code>hostNetwork</code> as <code>&quot;True&quot;</code>, the <code>PodHasNetwork</code> condition is set to <code>&quot;True&quot;</code> based on successful creation of the pod sandbox while the network configuration state of the pod sandbox is ignored. This is because the CRI implementation typically skips any pod sandbox network configuration when <code>hostNetwork</code> is set to <code>&quot;True&quot;</code> for a pod.</p> <p>A node agent may dynamically re-configure network interface(s) for a pod by watching changes in pod annotations that specify additional networking configuration (e.g. <code>k8s.v1.cni.cncf.io/networks</code>). Dynamic updates of pod networking configuration after the pod sandbox is initialized by Kubelet (in coordination with a container runtime) are not reflected by the <code>PodHasNetwork</code> condition.</p> <h3 id="try-out-the-podhasnetwork-condition-for-pods">Try out the PodHasNetwork condition for pods</h3> <p>In order to have the kubelet report the <code>PodHasNetwork</code> condition in the status field of a pod, please enable the <code>PodHasNetworkCondition</code> feature gate on the kubelet.</p> <p>For a pod whose runtime sandbox has been successfully created and has networking configured, the kubelet will report the <code>PodHasNetwork</code> condition with status set to <code>&quot;True&quot;</code>:</p> <pre tabindex="0"><code>$ kubectl describe pod nginx1 Name: nginx1 Namespace: default ... Conditions: Type Status PodHasNetwork True Initialized True Ready True ContainersReady True PodScheduled True </code></pre><p>For a pod whose runtime sandbox has not been created yet (and networking not configured either), the kubelet will report the <code>PodHasNetwork</code> condition with status set to <code>&quot;False&quot;</code>:</p> <pre tabindex="0"><code>$ kubectl describe pod nginx2 Name: nginx2 Namespace: default ... Conditions: Type Status PodHasNetwork False Initialized True Ready False ContainersReady False PodScheduled True </code></pre><h3 id="what-s-next">What’s next?</h3> <p>Depending on feedback and adoption, the Kubernetes team plans to push the reporting of the <code>PodHasNetwork</code> condition to Beta in 1.26 or 1.27.</p> <h3 id="how-can-i-learn-more">How can I learn more?</h3> <p>Please check out the <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/">documentation</a> for the <code>PodHasNetwork</code> condition to learn more about it and how it fits in relation to other pod conditions.</p> <h3 id="how-to-get-involved">How to get involved?</h3> <p>This feature is driven by the SIG Node community. Please join us to connect with the community and share your ideas and feedback around the above feature and beyond. We look forward to hearing from you!</p> <h3 id="acknowledgements">Acknowledgements</h3> <p>We want to thank the following people for their insightful and helpful reviews of the KEP and PRs around this feature: Derek Carr (@derekwaynecarr), Mrunal Patel (@mrunalp), Dawn Chen (@dchen1107), Qiutong Song (@qiutongs), Ruiwen Zhao (@ruiwen-zhao), Tim Bannister (@sftim), Danielle Lancashire (@endocrimes) and Agam Dua (@agamdua).</p></description></item><item><title>Blog: Announcing the Auto-refreshing Official Kubernetes CVE Feed</title><link>https://kubernetes.io/blog/2022/09/12/k8s-cve-feed-alpha/</link><pubDate>Mon, 12 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/09/12/k8s-cve-feed-alpha/</guid><description> <p><strong>Author</strong>: Pushkar Joglekar (VMware)</p> <p>A long-standing request from the Kubernetes community has been to have a programmatic way for end users to keep track of Kubernetes security issues (also called &quot;CVEs&quot;, after the database that tracks public security issues across different products and vendors). Accompanying the release of Kubernetes v1.25, we are excited to announce availability of such a <a href="https://kubernetes.io/docs/reference/issues-security/official-cve-feed/">feed</a> as an <code>alpha</code> feature. This blog will cover the background and scope of this new service.</p> <h2 id="motivation">Motivation</h2> <p>With the growing number of eyes on Kubernetes, the number of CVEs related to Kubernetes have increased. Although most CVEs that directly, indirectly, or transitively impact Kubernetes are regularly fixed, there is no single place for the end users of Kubernetes to programmatically subscribe or pull the data of fixed CVEs. Current options are either broken or incomplete.</p> <h2 id="scope">Scope</h2> <h3 id="what-this-does">What This Does</h3> <p>Create a periodically auto-refreshing, human and machine-readable list of official Kubernetes CVEs</p> <h3 id="what-this-doesn-t-do">What This Doesn't Do</h3> <ul> <li>Triage and vulnerability disclosure will continue to be done by SRC (Security Response Committee).</li> <li>Listing CVEs that are identified in build time dependencies and container images are out of scope.</li> <li>Only official CVEs announced by the Kubernetes SRC will be published in the feed.</li> </ul> <h3 id="who-it-s-for">Who It's For</h3> <ul> <li><strong>End Users</strong>: Persons or teams who <em>use</em> Kubernetes to deploy applications they own</li> <li><strong>Platform Providers</strong>: Persons or teams who <em>manage</em> Kubernetes clusters</li> <li><strong>Maintainers</strong>: Persons or teams who <em>create</em> and <em>support</em> Kubernetes releases through their work in Kubernetes Community - via various Special Interest Groups and Committees.</li> </ul> <h2 id="implementation-details">Implementation Details</h2> <p>A supporting <a href="https://kubernetes.dev/blog/2022/09/12/k8s-cve-feed-alpha/">contributor blog</a> was published that describes in depth on how this CVE feed was implemented to ensure the feed was reasonably protected against tampering and was automatically updated after a new CVE was announced.</p> <h2 id="what-s-next">What's Next?</h2> <p>In order to graduate this feature, SIG Security is gathering feedback from end users who are using this alpha feed.</p> <p>So in order to improve the feed in future Kubernetes Releases, if you have any feedback, please let us know by adding a comment to this <a href="https://github.com/kubernetes/sig-security/issues/1">tracking issue</a> or let us know on <a href="https://kubernetes.slack.com/archives/C01CUSVMHPY">#sig-security-tooling</a> Kubernetes Slack channel. (Join <a href="https://slack.k8s.io">Kubernetes Slack here</a>)</p> <p><em>A special shout out and massive thanks to Neha Lohia <a href="https://github.com/nehalohia27">(@nehalohia27)</a> and Tim Bannister <a href="https://github.com/sftim">(@sftim)</a> for their stellar collaboration for many months from &quot;ideation to implementation&quot; of this feature.</em></p></description></item><item><title>Blog: Kubernetes 1.25: KMS V2 Improvements</title><link>https://kubernetes.io/blog/2022/09/09/kms-v2-improvements/</link><pubDate>Fri, 09 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/09/09/kms-v2-improvements/</guid><description> <p><strong>Authors:</strong> Anish Ramasekar, Rita Zhang, Mo Khan, and Xander Grzywinski (Microsoft)</p> <p>With Kubernetes v1.25, SIG Auth is introducing a new <code>v2alpha1</code> version of the Key Management Service (KMS) API. There are a lot of improvements in the works, and we're excited to be able to start down the path of a new and improved KMS!</p> <h2 id="what-is-kms">What is KMS?</h2> <p>One of the first things to consider when securing a Kubernetes cluster is encrypting persisted API data at rest. KMS provides an interface for a provider to utilize a key stored in an external key service to perform this encryption.</p> <p>Encryption at rest using KMS v1 has been a feature of Kubernetes since version v1.10, and is currently in beta as of version v1.12.</p> <h2 id="what-s-new-in-v2alpha1">What’s new in <code>v2alpha1</code>?</h2> <p>While the original v1 implementation has been successful in helping Kubernetes users encrypt etcd data, it did fall short in a few key ways:</p> <ol> <li><strong>Performance:</strong> When starting a cluster, all resources are serially fetched and decrypted to fill the <code>kube-apiserver</code> cache. When using a KMS plugin, this can cause slow startup times due to the large number of requests made to the remote vault. In addition, there is the potential to hit API rate limits on external key services depending on how many encrypted resources exist in the cluster.</li> <li><strong>Key Rotation:</strong> With KMS v1, rotation of a key-encrypting key is a manual and error-prone process. It can be difficult to determine what encryption keys are in-use on a cluster.</li> <li><strong>Health Check &amp; Status:</strong> Before the KMS v2 API, the <code>kube-apiserver</code> was forced to make encrypt and decrypt calls as a proxy to determine if the KMS plugin is healthy. With cloud services these operations usually cost actual money with cloud service. Whatever the cost, those operations on their own do not provide a holistic view of the service's health.</li> <li><strong>Observability:</strong> Without some kind of trace ID, it's has been difficult to correlate events found in the various logs across <code>kube-apiserver</code>, KMS, and KMS plugins.</li> </ol> <p>The KMS v2 enhancement attempts to address all of these shortcomings, though not all planned features are implemented in the initial alpha release. Here are the improvements that arrived in Kubernetes v1.25:</p> <ol> <li>Support for KMS plugins that use a key hierarchy to reduce network requests made to the remote vault. To learn more, check out the <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-auth/3299-kms-v2-improvements#key-hierachy">design details for how a KMS plugin can leverage key hierarchy</a>.</li> <li>Extra metadata is now tracked to allow a KMS plugin to communicate what key it is currently using with the <code>kube-apiserver</code>, allowing for rotation without API server restart. Data stored in etcd follows a more standard proto format to allow external tools to observe its state. To learn more, check out the <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-auth/3299-kms-v2-improvements#metadata">details for metadata</a>.</li> <li>A dedicated status API is used to communicate the health of the KMS plugin with the API server. To learn more, check out the <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-auth/3299-kms-v2-improvements#status-api">details for status API</a>.</li> <li>To improve observability, a new <code>UID</code> field is included in <code>EncryptRequest</code> and <code>DecryptRequest</code> of the v2 API. The UID is generated for each envelope operation. To learn more, check out the <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-auth/3299-kms-v2-improvements#Observability">details for observability</a>.</li> </ol> <h3 id="sequence-diagram">Sequence Diagram</h3> <h4 id="encrypt-request">Encrypt Request</h4> <!-- source - https://mermaid.ink/img/pako:eNrNVD1v2zAQ_SsEC0GLkxgt2kEIvEQeCo8tOgkoTuTJIiyRypFMIwj67yUlxx-w0CLoUg0a7t29e3eP5MCFkcgzniSD0splbEhdjS2mGUtLsJiu2Bz4AaSgbNAGZGBpR6oF6p9MYyjmfvj08YvAzzH9CH3HV3eGq6qaqK6C6_U6HccxSQpt8dmjFpgr2BO0hWbh64CcEqoD7Rg6IW-jB18idMoivSAtwK3tGr9XeoHv1SFpaELKDF5R3W02p9qMBWHUd45RFGndnA-NY94qvWcH7FmtkIBE3c_gRPhGsEyWb3fsl3I1a4yAhu22u-XSC6Hn4lPNTEHYGofXHBd1iwJQ_q3zRY0AUeM7Ki93mQV5zpO-WKPtTHCcPZb0sGFDwYMnNVI8HAXPWMEfz53CmjYFX8Ul_1RyAs_Tsq_5BM5EBQetjQOnAnskCsxB1X1UQxod2ntlHibpdwc83LQ6DRU4x3GeDJugM5D-2eokYcITYThXJdbwogy9w8z8H23M_xcbbg04rVHL5VsWr3XGrDOEt8JAy6Ux45-veIvUgpLh8RpipODTOzUrl1iBb8IYhR5Dqu8kONxKFfrwrIJg6oqDd-ZbrwXPHHl8Szo-QMes8Tffb72O)](https://mermaid.live/edit#pako:eNrNVD1v2zAQ_SsEC0GLkxgt2kEIvEQeCo8tOgkoTuTJIiyRypFMIwj67yUlxx-w0CLoUg0a7t29e3eP5MCFkcgzniSD0splbEhdjS2mGUtLsJiu2Bz4AaSgbNAGZGBpR6oF6p9MYyjmfvj08YvAzzH9CH3HV3eGq6qaqK6C6_U6HccxSQpt8dmjFpgr2BO0hWbh64CcEqoD7Rg6IW-jB18idMoivSAtwK3tGr9XeoHv1SFpaELKDF5R3W02p9qMBWHUd45RFGndnA-NY94qvWcH7FmtkIBE3c_gRPhGsEyWb3fsl3I1a4yAhu22u-XSC6Hn4lPNTEHYGofXHBd1iwJQ_q3zRY0AUeM7Ki93mQV5zpO-WKPtTHCcPZb0sGFDwYMnNVI8HAXPWMEfz53CmjYFX8Ul_1RyAs_Tsq_5BM5EBQetjQOnAnskCsxB1X1UQxod2ntlHibpdwc83LQ6DRU4x3GeDJugM5D-2eokYcITYThXJdbwogy9w8z8H23M_xcbbg04rVHL5VsWr3XGrDOEt8JAy6Ux45-veIvUgpLh8RpipODTOzUrl1iBb8IYhR5Dqu8kONxKFfrwrIJg6oqDd-ZbrwXPHHl8Szo-QMes8Tffb72O --> <figure class="diagram-large"> <img src="https://kubernetes.io/images/blog/2022-09-09-kubernetes-1.25-kms-v2-improvements/kubernetes-1.25-encryption.svg" alt="Sequence diagram for KMSv2 Encrypt"/> </figure> <h4 id="decrypt-request">Decrypt Request</h4> <!-- source - https://mermaid.ink/img/pako:eNrNVD1v2zAQ_SsEC0GLkxgt2kEIvEQeCo8tOgkoTuTJIiyRypFMIwj67yUlxx-w0CLoUg0a7t29e3eP5MCFkcgzniSD0splbEhdjS2mGUtLsJiu2Bz4AaSgbNAGZGBpR6oF6p9MYyjmfvj08YvAzzH9CH3HV3eGq6qaqK6C6_U6HccxSQpt8dmjFpgr2BO0hWbh64CcEqoD7Rg6IW-jB18idMoivSAtwK3tGr9XeoHv1SFpaELKDF5R3W02p9qMBWHUd45RFGndnA-NY94qvWcH7FmtkIBE3c_gRPhGsEyWb3fsl3I1a4yAhu22u-XSC6Hn4lPNTEHYGofXHBd1iwJQ_q3zRY0AUeM7Ki93mQV5zpO-WKPtTHCcPZb0sGFDwYMnNVI8HAXPWMEfz53CmjYFX8Ul_1RyAs_Tsq_5BM5EBQetjQOnAnskCsxB1X1UQxod2ntlHibpdwc83LQ6DRU4x3GeDJugM5D-2eokYcITYThXJdbwogy9w8z8H23M_xcbbg04rVHL5VsWr3XGrDOEt8JAy6Ux45-veIvUgpLh8RpipODTOzUrl1iBb8IYhR5Dqu8kONxKFfrwrIJg6oqDd-ZbrwXPHHl8Szo-QMes8Tffb72O)](https://mermaid.live/edit#pako:eNrNVD1v2zAQ_SsEC0GLkxgt2kEIvEQeCo8tOgkoTuTJIiyRypFMIwj67yUlxx-w0CLoUg0a7t29e3eP5MCFkcgzniSD0splbEhdjS2mGUtLsJiu2Bz4AaSgbNAGZGBpR6oF6p9MYyjmfvj08YvAzzH9CH3HV3eGq6qaqK6C6_U6HccxSQpt8dmjFpgr2BO0hWbh64CcEqoD7Rg6IW-jB18idMoivSAtwK3tGr9XeoHv1SFpaELKDF5R3W02p9qMBWHUd45RFGndnA-NY94qvWcH7FmtkIBE3c_gRPhGsEyWb3fsl3I1a4yAhu22u-XSC6Hn4lPNTEHYGofXHBd1iwJQ_q3zRY0AUeM7Ki93mQV5zpO-WKPtTHCcPZb0sGFDwYMnNVI8HAXPWMEfz53CmjYFX8Ul_1RyAs_Tsq_5BM5EBQetjQOnAnskCsxB1X1UQxod2ntlHibpdwc83LQ6DRU4x3GeDJugM5D-2eokYcITYThXJdbwogy9w8z8H23M_xcbbg04rVHL5VsWr3XGrDOEt8JAy6Ux45-veIvUgpLh8RpipODTOzUrl1iBb8IYhR5Dqu8kONxKFfrwrIJg6oqDd-ZbrwXPHHl8Szo-QMes8Tffb72O ](https://mermaid.ink/img/pako:eNrVVU2P0zAQ_SsjoyggdXcrEHuIVr3QHlBvgDhFQtN40lhN7GA7C1GU_47jdOuUhi4HkKCn1DPzPkbPcscyxYklLIo6IYVNoIttQRXFCcQ7NBQvYDz4jFrgriTjKh3EtRYV6vadKpUeel-8eX2f0duh_Vj6RN9tKOd57qHODpfLZdz3fRSl0tDXhmRGa4F7jVUqwf1q1FZkokZp4dDsCGthSD-SnilXpi6bvZCXJUdJWmLpWsZiFIHIoVQZlrDdbEFIQCmVRSuUNAtwfiU0Rsg9FII06qxox0ksHZzMdFtb4lME8xPI2A7nqm9Wq5PMBDh5HNCDc2PhYafvVtClzMkuSA-bSlkCKXsIjOvNdpWyBRyo_SK4L46fsFfWOtVovHVQOWzGqQ9kaieI_NzIkbKpUsfhSJ2w20GslmTJ3Ap1593dHOhwoeLk22H2_ZPVK9uRkGFWUOj0q3laxfxanFX4JmwRcMI4lYZmmZyr32CbBCLwBRDPqqlSls5pPXWYndU9lfPH_F4Z91avk5Pk4c8ZzDScibNsGy0nuRyDE4JZlyjkJJeBdSaXYYHwfv2Xw_fLPLh7eYzEzN38b27n9I49m-P1ZYLhpcGKYEcFPgqlBxlWcWxfTTLyfKzX00z9gzE6hUFytmAV6QoFdy9bNxynzD9iIyOnHJvS0aeyd61NzdHShgurNEtydGFaMGys-tjKjCVWN_TUdHydjl39D0CLbdk)](https://mermaid.live/edit#pako:eNrVVU2P0zAQ_SsjoyggdXcrEHuIVr3QHlBvgDhFQtN40lhN7GA7C1GU_47jdOuUhi4HkKCn1DPzPkbPcscyxYklLIo6IYVNoIttQRXFCcQ7NBQvYDz4jFrgriTjKh3EtRYV6vadKpUeel-8eX2f0duh_Vj6RN9tKOd57qHODpfLZdz3fRSl0tDXhmRGa4F7jVUqwf1q1FZkokZp4dDsCGthSD-SnilXpi6bvZCXJUdJWmLpWsZiFIHIoVQZlrDdbEFIQCmVRSuUNAtwfiU0Rsg9FII06qxox0ksHZzMdFtb4lME8xPI2A7nqm9Wq5PMBDh5HNCDc2PhYafvVtClzMkuSA-bSlkCKXsIjOvNdpWyBRyo_SK4L46fsFfWOtVovHVQOWzGqQ9kaieI_NzIkbKpUsfhSJ2w20GslmTJ3Ap1593dHOhwoeLk22H2_ZPVK9uRkGFWUOj0q3laxfxanFX4JmwRcMI4lYZmmZyr32CbBCLwBRDPqqlSls5pPXWYndU9lfPH_F4Z91avk5Pk4c8ZzDScibNsGy0nuRyDE4JZlyjkJJeBdSaXYYHwfv2Xw_fLPLh7eYzEzN38b27n9I49m-P1ZYLhpcGKYEcFPgqlBxlWcWxfTTLyfKzX00z9gzE6hUFytmAV6QoFdy9bNxynzD9iIyOnHJvS0aeyd61NzdHShgurNEtydGFaMGys-tjKjCVWN_TUdHydjl39D0CLbdk --> <figure class="diagram-large"> <img src="https://kubernetes.io/images/blog/2022-09-09-kubernetes-1.25-kms-v2-improvements/kubernetes-1.25-decryption.svg" alt="Sequence diagram for KMSv2 Decrypt"/> </figure> <h2 id="what-s-next">What’s next?</h2> <p>For Kubernetes v1.26, we expect to ship another alpha version. As of right now, the alpha API will be ready to be used by KMS plugin authors. We hope to include a reference plugin implementation with the next release, and you'll be able to try out the feature at that time.</p> <p>You can learn more about KMS v2 by reading <a href="https://kubernetes.io/docs/tasks/administer-cluster/kms-provider/">Using a KMS provider for data encryption</a>. You can also follow along on the <a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/3299-kms-v2-improvements/#readme">KEP</a> to track progress across the coming Kubernetes releases.</p> <h2 id="how-to-get-involved">How to get involved</h2> <p>If you are interested in getting involved in the development of this feature or would like to share feedback, please reach out on the <a href="https://kubernetes.slack.com/archives/C03035EH4VB">#sig-auth-kms-dev</a> channel on Kubernetes Slack.</p> <p>You are also welcome to join the bi-weekly <a href="https://github.com/kubernetes/community/blob/master/sig-auth/README.md#meetings">SIG Auth meetings</a>, held every-other Wednesday.</p> <h2 id="acknowledgements">Acknowledgements</h2> <p>This feature has been an effort driven by contributors from several different companies. We would like to extend a huge thank you to everyone that contributed their time and effort to help make this possible.</p></description></item><item><title>Blog: Kubernetes’s IPTables Chains Are Not API</title><link>https://kubernetes.io/blog/2022/09/07/iptables-chains-not-api/</link><pubDate>Wed, 07 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/09/07/iptables-chains-not-api/</guid><description> <p><strong>Author:</strong> Dan Winship (Red Hat)</p> <p>Some Kubernetes components (such as kubelet and kube-proxy) create iptables chains and rules as part of their operation. These chains were never intended to be part of any Kubernetes API/ABI guarantees, but some external components nonetheless make use of some of them (in particular, using <code>KUBE-MARK-MASQ</code> to mark packets as needing to be masqueraded).</p> <p>As a part of the v1.25 release, SIG Network made this declaration explicit: that (with one exception), the iptables chains that Kubernetes creates are intended only for Kubernetes’s own internal use, and third-party components should not assume that Kubernetes will create any specific iptables chains, or that those chains will contain any specific rules if they do exist.</p> <p>Then, in future releases, as part of <a href="https://github.com/kubernetes/enhancements/issues/3178">KEP-3178</a>, we will begin phasing out certain chains that Kubernetes itself no longer needs. Components outside of Kubernetes itself that make use of <code>KUBE-MARK-MASQ</code>, <code>KUBE-MARK-DROP</code>, or other Kubernetes-generated iptables chains should start migrating away from them now.</p> <h2 id="background">Background</h2> <p>In addition to various service-specific iptables chains, kube-proxy creates certain general-purpose iptables chains that it uses as part of service proxying. In the past, kubelet also used iptables for a few features (such as setting up <code>hostPort</code> mapping for pods) and so it also redundantly created some of the same chains.</p> <p>However, with <a href="https://kubernetes.io/blog/2022/02/17/dockershim-faq/">the removal of dockershim</a> in Kubernetes in 1.24, kubelet now no longer ever uses any iptables rules for its own purposes; the things that it used to use iptables for are now always the responsibility of the container runtime or the network plugin, and there is no reason for kubelet to be creating any iptables rules.</p> <p>Meanwhile, although <code>iptables</code> is still the default kube-proxy backend on Linux, it is unlikely to remain the default forever, since the associated command-line tools and kernel APIs are essentially deprecated, and no longer receiving improvements. (RHEL 9 <a href="https://access.redhat.com/solutions/6739041">logs a warning</a> if you use the iptables API, even via <code>iptables-nft</code>.)</p> <p>Although as of Kubernetes 1.25 iptables kube-proxy remains popular, and kubelet continues to create the iptables rules that it historically created (despite no longer <em>using</em> them), third party software cannot assume that core Kubernetes components will keep creating these rules in the future.</p> <h2 id="upcoming-changes">Upcoming changes</h2> <p>Starting a few releases from now, kubelet will no longer create the following iptables chains in the <code>nat</code> table:</p> <ul> <li><code>KUBE-MARK-DROP</code></li> <li><code>KUBE-MARK-MASQ</code></li> <li><code>KUBE-POSTROUTING</code></li> </ul> <p>Additionally, the <code>KUBE-FIREWALL</code> chain in the <code>filter</code> table will no longer have the functionality currently associated with <code>KUBE-MARK-DROP</code> (and it may eventually go away entirely).</p> <p>This change will be phased in via the <code>IPTablesOwnershipCleanup</code> feature gate. That feature gate is available and can be manually enabled for testing in Kubernetes 1.25. The current plan is that it will become enabled-by-default in Kubernetes 1.27, though this may be delayed to a later release. (It will not happen sooner than Kubernetes 1.27.)</p> <h2 id="what-to-do-if-you-use-kubernetes-s-iptables-chains">What to do if you use Kubernetes’s iptables chains</h2> <p>(Although the discussion below focuses on short-term fixes that are still based on iptables, you should probably also start thinking about eventually migrating to nftables or another API).</p> <h3 id="use-case-kube-mark-masq">If you use <code>KUBE-MARK-MASQ</code>...</h3> <p>If you are making use of the <code>KUBE-MARK-MASQ</code> chain to cause packets to be masqueraded, you have two options: (1) rewrite your rules to use <code>-j MASQUERADE</code> directly, (2) create your own alternative “mark for masquerade” chain.</p> <p>The reason kube-proxy uses <code>KUBE-MARK-MASQ</code> is because there are lots of cases where it needs to call both <code>-j DNAT</code> and <code>-j MASQUERADE</code> on a packet, but it’s not possible to do both of those at the same time in iptables; <code>DNAT</code> must be called from the <code>PREROUTING</code> (or <code>OUTPUT</code>) chain (because it potentially changes where the packet will be routed to) while <code>MASQUERADE</code> must be called from <code>POSTROUTING</code> (because the masqueraded source IP that it picks depends on what the final routing decision was).</p> <p>In theory, kube-proxy could have one set of rules to match packets in <code>PREROUTING</code>/<code>OUTPUT</code> and call <code>-j DNAT</code>, and then have a second set of rules to match the same packets in <code>POSTROUTING</code> and call <code>-j MASQUERADE</code>. But instead, for efficiency, it only matches them once, during <code>PREROUTING</code>/<code>OUTPUT</code>, at which point it calls <code>-j DNAT</code> and then calls <code>-j KUBE-MARK-MASQ</code> to set a bit on the kernel packet mark as a reminder to itself. Then later, during <code>POSTROUTING</code>, it has a single rule that matches all previously-marked packets, and calls <code>-j MASQUERADE</code> on them.</p> <p>If you have <em>a lot</em> of rules where you need to apply both DNAT and masquerading to the same packets like kube-proxy does, then you may want a similar arrangement. But in many cases, components that use <code>KUBE-MARK-MASQ</code> are only doing it because they copied kube-proxy’s behavior without understanding why kube-proxy was doing it that way. Many of these components could easily be rewritten to just use separate DNAT and masquerade rules. (In cases where no DNAT is occurring then there is even less point to using <code>KUBE-MARK-MASQ</code>; just move your rules from <code>PREROUTING</code> to <code>POSTROUTING</code> and call <code>-j MASQUERADE</code> directly.)</p> <h3 id="use-case-kube-mark-drop">If you use <code>KUBE-MARK-DROP</code>...</h3> <p>The rationale for <code>KUBE-MARK-DROP</code> is similar to the rationale for <code>KUBE-MARK-MASQ</code>: kube-proxy wanted to make packet-dropping decisions alongside other decisions in the <code>nat</code> <code>KUBE-SERVICES</code> chain, but you can only call <code>-j DROP</code> from the <code>filter</code> table. So instead, it uses <code>KUBE-MARK-DROP</code> to mark packets to be dropped later on.</p> <p>In general, the approach for removing a dependency on <code>KUBE-MARK-DROP</code> is the same as for removing a dependency on <code>KUBE-MARK-MASQ</code>. In kube-proxy’s case, it is actually quite easy to replace the usage of <code>KUBE-MARK-DROP</code> in the <code>nat</code> table with direct calls to <code>DROP</code> in the <code>filter</code> table, because there are no complicated interactions between DNAT rules and drop rules, and so the drop rules can simply be moved from <code>nat</code> to <code>filter</code>.</p> <p>In more complicated cases, it might be necessary to “re-match” the same packets in both <code>nat</code> and <code>filter</code>.</p> <h3 id="use-case-iptables-mode">If you use Kubelet’s iptables rules to figure out <code>iptables-legacy</code> vs <code>iptables-nft</code>...</h3> <p>Components that manipulate host-network-namespace iptables rules from inside a container need some way to figure out whether the host is using the old <code>iptables-legacy</code> binaries or the newer <code>iptables-nft</code> binaries (which talk to a different kernel API underneath).</p> <p>The <a href="https://github.com/kubernetes-sigs/iptables-wrappers/"><code>iptables-wrappers</code></a> module provides a way for such components to autodetect the system iptables mode, but in the past it did this by assuming that Kubelet will have created “a bunch” of iptables rules before any containers start, and so it can guess which mode the iptables binaries in the host filesystem are using by seeing which mode has more rules defined.</p> <p>In future releases, Kubelet will no longer create many iptables rules, so heuristics based on counting the number of rules present may fail.</p> <p>However, as of 1.24, Kubelet always creates a chain named <code>KUBE-IPTABLES-HINT</code> in the <code>mangle</code> table of whichever iptables subsystem it is using. Components can now look for this specific chain to know which iptables subsystem Kubelet (and thus, presumably, the rest of the system) is using.</p> <p>(Additionally, since Kubernetes 1.17, kubelet has created a chain called <code>KUBE-KUBELET-CANARY</code> in the <code>mangle</code> table. While this chain may go away in the future, it will of course still be there in older releases, so in any recent version of Kubernetes, at least one of <code>KUBE-IPTABLES-HINT</code> or <code>KUBE-KUBELET-CANARY</code> will be present.)</p> <p>The <code>iptables-wrappers</code> package has <a href="https://github.com/kubernetes-sigs/iptables-wrappers/pull/3">already been updated</a> with this new heuristic, so if you were previously using that, you can rebuild your container images with an updated version of that.</p> <h2 id="further-reading">Further reading</h2> <p>The project to clean up iptables chain ownership and deprecate the old chains is tracked by <a href="https://github.com/kubernetes/enhancements/issues/3178">KEP-3178</a>.</p></description></item><item><title>Blog: Introducing COSI: Object Storage Management using Kubernetes APIs</title><link>https://kubernetes.io/blog/2022/09/02/cosi-kubernetes-object-storage-management/</link><pubDate>Fri, 02 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/09/02/cosi-kubernetes-object-storage-management/</guid><description> <p><strong>Authors:</strong> Sidhartha Mani (<a href="https://min.io">Minio, Inc</a>)</p> <p>This article introduces the Container Object Storage Interface (COSI), a standard for provisioning and consuming object storage in Kubernetes. It is an alpha feature in Kubernetes v1.25.</p> <p>File and block storage are treated as first class citizens in the Kubernetes ecosystem via <a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">Container Storage Interface</a> (CSI). Workloads using CSI volumes enjoy the benefits of portability across vendors and across Kubernetes clusters without the need to change application manifests. An equivalent standard does not exist for Object storage.</p> <p>Object storage has been rising in popularity in recent years as an alternative form of storage to filesystems and block devices. Object storage paradigm promotes disaggregation of compute and storage. This is done by making data available over the network, rather than locally. Disaggregated architectures allow compute workloads to be stateless, which consequently makes them easier to manage, scale and automate.</p> <h2 id="cosi">COSI</h2> <p>COSI aims to standardize consumption of object storage to provide the following benefits:</p> <ul> <li>Kubernetes Native - Use the Kubernetes API to provision, configure and manage buckets</li> <li>Self Service - A clear delineation between administration and operations (DevOps) to enable self-service capability for DevOps personnel</li> <li>Portability - Vendor neutrality enabled through portability across Kubernetes Clusters and across Object Storage vendors</li> </ul> <p><em>Portability across vendors is only possible when both vendors support a common datapath-API. Eg. it is possible to port from AWS S3 to Ceph, or AWS S3 to MinIO and back as they all use S3 API. In contrast, it is not possible to port from AWS S3 and Google Cloud’s GCS or vice versa.</em></p> <h2 id="architecture">Architecture</h2> <p>COSI is made up of three components:</p> <ul> <li>COSI Controller Manager</li> <li>COSI Sidecar</li> <li>COSI Driver</li> </ul> <p>The COSI Controller Manager acts as the main controller that processes changes to COSI API objects. It is responsible for fielding requests for bucket creation, updates, deletion and access management. One instance of the controller manager is required per kubernetes cluster. Only one is needed even if multiple object storage providers are used in the cluster.</p> <p>The COSI Sidecar acts as a translator between COSI API requests and vendor-specific COSI Drivers. This component uses a standardized gRPC protocol that vendor drivers are expected to satisfy.</p> <p>The COSI Driver is the vendor specific component that receives requests from the sidecar and calls the appropriate vendor APIs to create buckets, manage their lifecycle and manage access to them.</p> <h2 id="api">API</h2> <p>The COSI API is centered around buckets, since bucket is the unit abstraction for object storage. COSI defines three Kubernetes APIs aimed at managing them</p> <ul> <li>Bucket</li> <li>BucketClass</li> <li>BucketClaim</li> </ul> <p>In addition, two more APIs for managing access to buckets are also defined:</p> <ul> <li>BucketAccess</li> <li>BucketAccessClass</li> </ul> <p>In a nutshell, Bucket and BucketClaim can be considered to be similar to PersistentVolume and PersistentVolumeClaim respectively. The BucketClass’ counterpart in the file/block device world is StorageClass.</p> <p>Since Object Storage is always authenticated, and over the network, access credentials are required to access buckets. The two APIs, namely, BucketAccess and BucketAccessClass are used to denote access credentials and policies for authentication. More info about these APIs can be found in the official COSI proposal - <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1979-object-storage-support">https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1979-object-storage-support</a></p> <h2 id="self-service">Self-Service</h2> <p>Other than providing kubernetes-API driven bucket management, COSI also aims to empower DevOps personnel to provision and manage buckets on their own, without admin intervention. This, further enabling dev teams to realize faster turn-around times and faster time-to-market.</p> <p>COSI achieves this by dividing bucket provisioning steps among two different stakeholders, namely the administrator (admin), and the cluster operator. The administrator will be responsible for setting broad policies and limits on how buckets are provisioned, and how access is obtained for them. The cluster operator will be free to create and utilize buckets within the limits set by the admin.</p> <p>For example, a cluster operator could use an admin policy could be used to restrict maximum provisioned capacity to 100GB, and developers would be allowed to create buckets and store data upto that limit. Similarly for access credentials, admins would be able to restrict who can access which buckets, and developers would be able to access all the buckets available to them.</p> <h2 id="portability">Portability</h2> <p>The third goal of COSI is to achieve vendor neutrality for bucket management. COSI enables two kinds of portability:</p> <ul> <li>Cross Cluster</li> <li>Cross Provider</li> </ul> <p>Cross Cluster portability is allowing buckets provisioned in one cluster to be available in another cluster. This is only valid when the object storage backend itself is accessible from both clusters.</p> <p>Cross-provider portability is about allowing organizations or teams to move from one object storage provider to another seamlessly, and without requiring changes to application definitions (PodTemplates, StatefulSets, Deployment and so on). This is only possible if the source and destination providers use the same data.</p> <p><em>COSI does not handle data migration as it is outside of its scope. In case porting between providers requires data to be migrated as well, then other measures need to be taken to ensure data availability.</em></p> <h2 id="what-s-next">What’s next</h2> <p>The amazing sig-storage-cosi community has worked hard to bring the COSI standard to alpha status. We are looking forward to onboarding a lot of vendors to write COSI drivers and become COSI compatible!</p> <p>We want to add more authentication mechanisms for COSI buckets, we are designing advanced bucket sharing primitives, multi-cluster bucket management and much more. Lots of great ideas and opportunities ahead!</p> <p>Stay tuned for what comes next, and if you have any questions, comments or suggestions</p> <ul> <li>Chat with us on the Kubernetes <a href="https://kubernetes.slack.com/archives/C017EGC1C6N">Slack:#sig-storage-cosi</a></li> <li>Join our <a href="https://zoom.us/j/614261834?pwd=Sk1USmtjR2t0MUdjTGVZeVVEV1BPQT09">Zoom meeting</a>, every Thursday at 10:00 Pacific Time</li> <li>Participate in the <a href="https://github.com/kubernetes/enhancements/pull/2813">bucket API proposal PR</a> to add your ideas, suggestions and more.</li> </ul></description></item><item><title>Blog: Kubernetes 1.25: cgroup v2 graduates to GA</title><link>https://kubernetes.io/blog/2022/08/31/cgroupv2-ga-1-25/</link><pubDate>Wed, 31 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/08/31/cgroupv2-ga-1-25/</guid><description> <p><strong>Authors:</strong>: David Porter (Google), Mrunal Patel (Red Hat)</p> <p>Kubernetes 1.25 brings cgroup v2 to GA (general availability), letting the <a href="https://kubernetes.io/docs/concepts/overview/components/#kubelet">kubelet</a> use the latest container resource management capabilities.</p> <h2 id="what-are-cgroups">What are cgroups?</h2> <p>Effective <a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/">resource management</a> is a critical aspect of Kubernetes. This involves managing the finite resources in your nodes, such as CPU, memory, and storage.</p> <p><em>cgroups</em> are a Linux kernel capability that establish resource management functionality like limiting CPU usage or setting memory limits for running processes.</p> <p>When you use the resource management capabilities in Kubernetes, such as configuring <a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits">requests and limits for Pods and containers</a>, Kubernetes uses cgroups to enforce your resource requests and limits.</p> <p>The Linux kernel offers two versions of cgroups: cgroup v1 and cgroup v2.</p> <h2 id="what-is-cgroup-v2">What is cgroup v2?</h2> <p>cgroup v2 is the latest version of the Linux cgroup API. cgroup v2 provides a unified control system with enhanced resource management capabilities.</p> <p>cgroup v2 has been in development in the Linux Kernel since 2016 and in recent years has matured across the container ecosystem. With Kubernetes 1.25, cgroup v2 support has graduated to general availability.</p> <p>Many recent releases of Linux distributions have switched over to cgroup v2 by default so it's important that Kubernetes continues to work well on these new updated distros.</p> <p>cgroup v2 offers several improvements over cgroup v1, such as the following:</p> <ul> <li>Single unified hierarchy design in API</li> <li>Safer sub-tree delegation to containers</li> <li>Newer features like <a href="https://www.kernel.org/doc/html/latest/accounting/psi.html">Pressure Stall Information</a></li> <li>Enhanced resource allocation management and isolation across multiple resources <ul> <li>Unified accounting for different types of memory allocations (network and kernel memory, etc)</li> <li>Accounting for non-immediate resource changes such as page cache write backs</li> </ul> </li> </ul> <p>Some Kubernetes features exclusively use cgroup v2 for enhanced resource management and isolation. For example, the <a href="https://kubernetes.io/blog/2021/11/26/qos-memory-resources/">MemoryQoS feature</a> improves memory utilization and relies on cgroup v2 functionality to enable it. New resource management features in the kubelet will also take advantage of the new cgroup v2 features moving forward.</p> <h2 id="how-do-you-use-cgroup-v2">How do you use cgroup v2?</h2> <p>Many Linux distributions are switching to cgroup v2 by default; you might start using it the next time you update the Linux version of your control plane and nodes!</p> <p>Using a Linux distribution that uses cgroup v2 by default is the recommended method. Some of the popular Linux distributions that use cgroup v2 include the following:</p> <ul> <li>Container Optimized OS (since M97)</li> <li>Ubuntu (since 21.10)</li> <li>Debian GNU/Linux (since Debian 11 Bullseye)</li> <li>Fedora (since 31)</li> <li>Arch Linux (since April 2021)</li> <li>RHEL and RHEL-like distributions (since 9)</li> </ul> <p>To check if your distribution uses cgroup v2 by default, refer to <a href="https://kubernetes.io/docs/concepts/architecture/cgroups/#check-cgroup-version">Check your cgroup version</a> or consult your distribution's documentation.</p> <p>If you're using a managed Kubernetes offering, consult your provider to determine how they're adopting cgroup v2, and whether you need to take action.</p> <p>To use cgroup v2 with Kubernetes, you must meet the following requirements:</p> <ul> <li>Your Linux distribution enables cgroup v2 on kernel version 5.8 or later</li> <li>Your container runtime supports cgroup v2. For example: <ul> <li><a href="https://containerd.io/">containerd</a> v1.4 or later</li> <li><a href="https://cri-o.io/">cri-o</a> v1.20 or later</li> </ul> </li> <li>The kubelet and the container runtime are configured to use the <a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes#systemd-cgroup-driver">systemd cgroup driver</a></li> </ul> <p>The kubelet and container runtime use a <a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes#cgroup-drivers">cgroup driver</a> to set cgroup paramaters. When using cgroup v2, it's strongly recommended that both the kubelet and your container runtime use the <a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes#systemd-cgroup-driver">systemd cgroup driver</a>, so that there's a single cgroup manager on the system. To configure the kubelet and the container runtime to use the driver, refer to the <a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes#systemd-cgroup-driver">systemd cgroup driver documentation</a>.</p> <h2 id="migrate-to-cgroup-v2">Migrate to cgroup v2</h2> <p>When you run Kubernetes with a Linux distribution that enables cgroup v2, the kubelet should automatically adapt without any additional configuration required, as long as you meet the requirements.</p> <p>In most cases, you won't see a difference in the user experience when you switch to using cgroup v2 unless your users access the cgroup file system directly.</p> <p>If you have applications that access the cgroup file system directly, either on the node or from inside a container, you must update the applications to use the cgroup v2 API instead of the cgroup v1 API.</p> <p>Scenarios in which you might need to update to cgroup v2 include the following:</p> <ul> <li>If you run third-party monitoring and security agents that depend on the cgroup file system, update the agents to versions that support cgroup v2.</li> <li>If you run <a href="https://github.com/google/cadvisor">cAdvisor</a> as a stand-alone DaemonSet for monitoring pods and containers, update it to v0.43.0 or later.</li> <li>If you deploy Java applications with the JDK, prefer to use JDK 11.0.16 and later or JDK 15 and later, which <a href="https://bugs.openjdk.org/browse/JDK-8230305">fully support cgroup v2</a>.</li> </ul> <h2 id="learn-more">Learn more</h2> <ul> <li>Read the <a href="https://kubernetes.io/docs/concepts/architecture/cgroups/">Kubernetes cgroup v2 documentation</a></li> <li>Read the enhancement proposal, <a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/2254-cgroup-v2/README.md">KEP 2254</a></li> <li>Learn more about <a href="https://man7.org/linux/man-pages/man7/cgroups.7.html">cgroups</a> on Linux Manual Pages and <a href="https://docs.kernel.org/admin-guide/cgroup-v2.html">cgroup v2</a> on the Linux Kernel documentation</li> </ul> <h2 id="get-involved">Get involved</h2> <p>Your feedback is always welcome! SIG Node meets regularly and are available in the <code>#sig-node</code> channel in the Kubernetes <a href="https://slack.k8s.io/">Slack</a>, or using the SIG <a href="https://github.com/kubernetes/community/tree/master/sig-node#contact">mailing list</a>.</p> <p>cgroup v2 has had a long journey and is a great example of open source community collaboration across the industry because it required work across the stack, from the Linux Kernel to systemd to various container runtimes, and (of course) Kubernetes.</p> <h2 id="acknowledgments">Acknowledgments</h2> <p>We would like to thank <a href="https://github.com/giuseppe">Giuseppe Scrivano</a> who initiated cgroup v2 support in Kubernetes, and reviews and leadership from the SIG Node community including chairs <a href="https://github.com/dchen1107">Dawn Chen</a> and <a href="https://github.com/derekwaynecarr">Derek Carr</a>.</p> <p>We'd also like to thank the maintainers of container runtimes like Docker, containerd and CRI-O, and the maintainers of components like <a href="https://github.com/google/cadvisor">cAdvisor</a> and <a href="https://github.com/opencontainers/runc">runc, libcontainer</a>, which underpin many container runtimes. Finally, this wouldn't have been possible without support from systemd and upstream Linux Kernel maintainers.</p> <p>It's a team effort!</p></description></item><item><title>Blog: Kubernetes 1.25: CSI Inline Volumes have graduated to GA</title><link>https://kubernetes.io/blog/2022/08/29/csi-inline-volumes-ga/</link><pubDate>Mon, 29 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/08/29/csi-inline-volumes-ga/</guid><description> <p><strong>Author:</strong> Jonathan Dobson (Red Hat)</p> <p>CSI Inline Volumes were introduced as an alpha feature in Kubernetes 1.15 and have been beta since 1.16. We are happy to announce that this feature has graduated to General Availability (GA) status in Kubernetes 1.25.</p> <p>CSI Inline Volumes are similar to other ephemeral volume types, such as <code>configMap</code>, <code>downwardAPI</code> and <code>secret</code>. The important difference is that the storage is provided by a CSI driver, which allows the use of ephemeral storage provided by third-party vendors. The volume is defined as part of the pod spec and follows the lifecycle of the pod, meaning the volume is created once the pod is scheduled and destroyed when the pod is destroyed.</p> <h2 id="what-s-new-in-1-25">What's new in 1.25?</h2> <p>There are a couple of new bug fixes related to this feature in 1.25, and the <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">CSIInlineVolume feature gate</a> has been locked to <code>True</code> with the graduation to GA. There are no new API changes, so users of this feature during beta should not notice any significant changes aside from these bug fixes.</p> <ul> <li><a href="https://github.com/kubernetes/kubernetes/issues/89290">#89290 - CSI inline volumes should support fsGroup</a></li> <li><a href="https://github.com/kubernetes/kubernetes/issues/79980">#79980 - CSI volume reconstruction does not work for ephemeral volumes</a></li> </ul> <h2 id="when-to-use-this-feature">When to use this feature</h2> <p>CSI inline volumes are meant for simple local volumes that should follow the lifecycle of the pod. They may be useful for providing secrets, configuration data, or other special-purpose storage to the pod from a CSI driver.</p> <p>A CSI driver is not suitable for inline use when:</p> <ul> <li>The volume needs to persist longer than the lifecycle of a pod</li> <li>Volume snapshots, cloning, or volume expansion are required</li> <li>The CSI driver requires <code>volumeAttributes</code> that should be restricted to an administrator</li> </ul> <h2 id="how-to-use-this-feature">How to use this feature</h2> <p>In order to use this feature, the <code>CSIDriver</code> spec must explicitly list <code>Ephemeral</code> as one of the supported <code>volumeLifecycleModes</code>. Here is a simple example from the <a href="https://github.com/kubernetes-sigs/secrets-store-csi-driver">Secrets Store CSI Driver</a>.</p> <pre tabindex="0"><code>apiVersion: storage.k8s.io/v1 kind: CSIDriver metadata: name: secrets-store.csi.k8s.io spec: podInfoOnMount: true attachRequired: false volumeLifecycleModes: - Ephemeral </code></pre><p>Any pod spec may then reference that CSI driver to create an inline volume, as in this example.</p> <pre tabindex="0"><code>kind: Pod apiVersion: v1 metadata: name: my-csi-app-inline spec: containers: - name: my-frontend image: busybox volumeMounts: - name: secrets-store-inline mountPath: &#34;/mnt/secrets-store&#34; readOnly: true command: [ &#34;sleep&#34;, &#34;1000000&#34; ] volumes: - name: secrets-store-inline csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: secretProviderClass: &#34;my-provider&#34; </code></pre><p>If the driver supports any volume attributes, you can provide these as part of the <code>spec</code> for the Pod as well:</p> <pre tabindex="0"><code> csi: driver: block.csi.vendor.example volumeAttributes: foo: bar </code></pre><h2 id="example-use-cases">Example Use Cases</h2> <p>Two existing CSI drivers that support the <code>Ephemeral</code> volume lifecycle mode are the Secrets Store CSI Driver and the Cert-Manager CSI Driver.</p> <p>The <a href="https://github.com/kubernetes-sigs/secrets-store-csi-driver">Secrets Store CSI Driver</a> allows users to mount secrets from external secret stores into a pod as an inline volume. This can be useful when the secrets are stored in an external managed service or Vault instance.</p> <p>The <a href="https://github.com/cert-manager/csi-driver">Cert-Manager CSI Driver</a> works along with <a href="https://cert-manager.io/">cert-manager</a> to seamlessly request and mount certificate key pairs into a pod. This allows the certificates to be renewed and updated in the application pod automatically.</p> <h2 id="security-considerations">Security Considerations</h2> <p>Special consideration should be given to which CSI drivers may be used as inline volumes. <code>volumeAttributes</code> are typically controlled through the <code>StorageClass</code>, and may contain attributes that should remain restricted to the cluster administrator. Allowing a CSI driver to be used for inline ephmeral volumes means that any user with permission to create pods may also provide <code>volumeAttributes</code> to the driver through a pod spec.</p> <p>Cluster administrators may choose to omit (or remove) <code>Ephemeral</code> from <code>volumeLifecycleModes</code> in the CSIDriver spec to prevent the driver from being used as an inline ephemeral volume, or use an <a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/">admission webhook</a> to restrict how the driver is used.</p> <h2 id="references">References</h2> <p>For more information on this feature, see:</p> <ul> <li><a href="https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volumes">Kubernetes documentation</a></li> <li><a href="https://kubernetes-csi.github.io/docs/ephemeral-local-volumes.html">CSI documentation</a></li> <li><a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/596-csi-inline-volumes/README.md">KEP-596</a></li> <li><a href="https://kubernetes.io/blog/2020/01/21/csi-ephemeral-inline-volumes/">Beta blog post for CSI Inline Volumes</a></li> </ul></description></item><item><title>Blog: Kubernetes v1.25: Pod Security Admission Controller in Stable</title><link>https://kubernetes.io/blog/2022/08/25/pod-security-admission-stable/</link><pubDate>Thu, 25 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/08/25/pod-security-admission-stable/</guid><description> <p><strong>Authors:</strong> Tim Allclair (Google), Sam Stoelinga (Google)</p> <p>The release of Kubernetes v1.25 marks a major milestone for Kubernetes out-of-the-box pod security controls: Pod Security admission (PSA) graduated to stable, and Pod Security Policy (PSP) has been removed. <a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">PSP was deprecated in Kubernetes v1.21</a>, and no longer functions in Kubernetes v1.25 and later.</p> <p>The Pod Security admission controller replaces PodSecurityPolicy, making it easier to enforce predefined <a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/">Pod Security Standards</a> by simply adding a label to a namespace. The Pod Security Standards are maintained by the K8s community, which means you automatically get updated security policies whenever new security-impacting Kubernetes features are introduced.</p> <h2 id="what-s-new-since-beta">What’s new since Beta?</h2> <p>Pod Security Admission hasn’t changed much since the Beta in Kubernetes v1.23. The focus has been on improving the user experience, while continuing to maintain a high quality bar.</p> <h3 id="improved-violation-messages">Improved violation messages</h3> <p>We improved violation messages so that you get <a href="https://github.com/kubernetes/kubernetes/pull/107698">fewer duplicate messages</a>. For example, instead of the following message when the Baseline and Restricted policies check the same capability:</p> <pre tabindex="0"><code>pods &#34;admin-pod&#34; is forbidden: violates PodSecurity &#34;restricted:latest&#34;: non-default capabilities (container &#34;admin&#34; must not include &#34;SYS_ADMIN&#34; in securityContext.capabilities.add), unrestricted capabilities (container &#34;admin&#34; must not include &#34;SYS_ADMIN&#34; in securityContext.capabilities.add) </code></pre><p>You get this message:</p> <pre tabindex="0"><code>pods &#34;admin-pod&#34; is forbidden: violates PodSecurity &#34;restricted:latest&#34;: unrestricted capabilities (container &#34;admin&#34; must not include &#34;SYS_ADMIN&#34; in securityContext.capabilities.add) </code></pre><h3 id="improved-namespace-warnings">Improved namespace warnings</h3> <p>When you modify the <code>enforce</code> Pod Security labels on a namespace, the Pod Security admission controller checks all existing pods for violations and surfaces a <a href="https://kubernetes.io/blog/2020/09/03/warnings/">warning</a> if any are out of compliance. These <a href="https://github.com/kubernetes/kubernetes/pull/105889">warnings are now aggregated</a> for pods with identical violations, making large namespaces with many replicas much more manageable. For example:</p> <pre tabindex="0"><code>Warning: frontend-h23gf2: allowPrivilegeEscalation != false Warning: myjob-g342hj (and 6 other pods): host namespaces, allowPrivilegeEscalation != false Warning: backend-j23h42 (and 1 other pod): non-default capabilities, unrestricted capabilities </code></pre><p>Additionally, when you apply a non-privileged label to a namespace that has been <a href="https://kubernetes.io/docs/concepts/security/pod-security-admission/#exemptions">configured to be exempt</a>, you will now get a warning alerting you to this fact:</p> <pre tabindex="0"><code>Warning: namespace &#39;kube-system&#39; is exempt from Pod Security, and the policy (enforce=baseline:latest) will be ignored </code></pre><h3 id="changes-to-the-pod-security-standards">Changes to the Pod Security Standards</h3> <p>The <a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/">Pod Security Standards</a>, which Pod Security admission enforces, have been updated with support for the new Pod OS field. In v1.25 and later, if you use the Restricted policy, the following Linux-specific restrictions will no longer be required if you explicitly set the pod's <code>.spec.os.name</code> field to <code>windows</code>:</p> <ul> <li>Seccomp - The <code>seccompProfile.type</code> field for Pod and container security contexts</li> <li>Privilege escalation - The <code>allowPrivilegeEscalation</code> field on container security contexts</li> <li>Capabilities - The requirement to drop <code>ALL</code> capabilities in the <code>capabilities</code> field on containers</li> </ul> <p>In Kubernetes v1.23 and earlier, the kubelet didn't enforce the Pod OS field. If your cluster includes nodes running a v1.23 or older kubelet, you should explicitly <a href="https://kubernetes.io/docs/concepts/security/pod-security-admission/#pod-security-admission-labels-for-namespaces">pin Restricted policies</a> to a version prior to v1.25.</p> <h2 id="migrating-from-podsecuritypolicy-to-the-pod-security-admission-controller">Migrating from PodSecurityPolicy to the Pod Security admission controller</h2> <p>For instructions to migrate from PodSecurityPolicy to the Pod Security admission controller, and for help choosing a migration strategy, refer to the <a href="https://kubernetes.io/docs/tasks/configure-pod-container/migrate-from-psp/">migration guide</a>. We're also developing a tool called <a href="https://github.com/kubernetes-sigs/pspmigrator">pspmigrator</a> to automate parts of the migration process.</p> <p>We'll be talking about PSP migration in more detail at our upcoming KubeCon 2022 NA talk, <a href="https://sched.co/182Jx"><em>Migrating from Pod Security Policy</em></a>. Use the <a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/program/schedule/">KubeCon NA schedule</a> to learn more.</p></description></item><item><title>Blog: PodSecurityPolicy: The Historical Context</title><link>https://kubernetes.io/blog/2022/08/23/podsecuritypolicy-the-historical-context/</link><pubDate>Tue, 23 Aug 2022 15:00:00 -0800</pubDate><guid>https://kubernetes.io/blog/2022/08/23/podsecuritypolicy-the-historical-context/</guid><description> <p><strong>Author:</strong> Mahé Tardy (Quarkslab)</p> <p>The PodSecurityPolicy (PSP) admission controller has been removed, as of Kubernetes v1.25. Its deprecation was announced and detailed in the blog post <a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">PodSecurityPolicy Deprecation: Past, Present, and Future</a>, published for the Kubernetes v1.21 release.</p> <p>This article aims to provide historical context on the birth and evolution of PSP, explain why the feature never made it to stable, and show why it was removed and replaced by Pod Security admission control.</p> <p>PodSecurityPolicy, like other specialized admission control plugins, provided fine-grained permissions on specific fields concerning the pod security settings as a built-in policy API. It acknowledged that cluster administrators and cluster users are usually not the same people, and that creating workloads in the form of a Pod or any resource that will create a Pod should not equal being &quot;root on the cluster&quot;. It could also encourage best practices by configuring more secure defaults through mutation and decoupling low-level Linux security decisions from the deployment process.</p> <h2 id="the-birth-of-podsecuritypolicy">The birth of PodSecurityPolicy</h2> <p>PodSecurityPolicy originated from OpenShift's SecurityContextConstraints (SCC) that were in the very first release of the Red Hat OpenShift Container Platform, even before Kubernetes 1.0. PSP was a stripped-down version of the SCC.</p> <p>The origin of the creation of PodSecurityPolicy is difficult to track, notably because it was mainly added before Kubernetes Enhancements Proposal (KEP) process, when design proposals were still a thing. Indeed, the archive of the final <a href="https://github.com/kubernetes/design-proposals-archive/blob/main/auth/pod-security-policy.md">design proposal</a> is still available. Nevertheless, a <a href="https://github.com/kubernetes/enhancements/issues/5">KEP issue number five</a> was created after the first pull requests were merged.</p> <p>Before adding the first piece of code that created PSP, two main pull requests were merged into Kubernetes, a <a href="https://github.com/kubernetes/kubernetes/pull/7343"><code>SecurityContext</code> subresource</a> that defined new fields on pods' containers, and the first iteration of the <a href="https://github.com/kubernetes/kubernetes/pull/7101">ServiceAccount</a> API.</p> <p>Kubernetes 1.0 was released on 10 July 2015 without any mechanism to restrict the security context and sensitive options of workloads, other than an alpha-quality SecurityContextDeny admission plugin (then known as <code>scdeny</code>). The <a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#securitycontextdeny">SecurityContextDeny plugin</a> is still in Kubernetes today (as an alpha feature) and creates an admission controller that prevents the usage of some fields in the security context.</p> <p>The roots of the PodSecurityPolicy were added with <a href="https://github.com/kubernetes/kubernetes/pull/7893">the very first pull request on security policy</a>, which added the design proposal with the new PSP object, based on the SCC (Security Context Constraints). It was a long discussion of nine months, with back and forth from OpenShift's SCC, many rebases, and the rename to PodSecurityPolicy that finally made it to upstream Kubernetes in February 2016. Now that the PSP object had been created, the next step was to add an admission controller that could enforce these policies. The first step was to add the admission <a href="https://github.com/kubernetes/kubernetes/pull/7893#issuecomment-180410539">without taking into account the users or groups</a>. A specific <a href="https://github.com/kubernetes/kubernetes/issues/23217">issue to bring PodSecurityPolicy to a usable state</a> was added to keep track of the progress and a first version of the admission controller was merged in <a href="https://github.com/kubernetes/kubernetes/pull/24600">pull request named PSP admission</a> in May 2016. Then around two months later, Kubernetes 1.3 was released.</p> <p>Here is a timeline that recaps the main pull requests of the birth of the PodSecurityPolicy and its admission controller with 1.0 and 1.3 releases as reference points.</p> <figure> <img src="./timeline.svg" alt="Timeline of the PodSecurityPolicy creation pull requests"/> </figure> <p>After that, the PSP admission controller was enhanced by adding what was initially left aside. <a href="https://github.com/kubernetes/kubernetes/pull/33080">The authorization mechanism</a>, merged in early November 2016 allowed administrators to use multiple policies in a cluster to grant different levels of access for different types of users. Later, a <a href="https://github.com/kubernetes/kubernetes/pull/52849">pull request</a> merged in October 2017 fixed <a href="https://github.com/kubernetes/kubernetes/issues/36184">a design issue</a> on ordering PodSecurityPolicies between mutating and alphabetical order, and continued to build the PSP admission as we know it. After that, many improvements and fixes followed to build the PodSecurityPolicy feature of recent Kubernetes releases.</p> <h2 id="the-rise-of-pod-security-admission">The rise of Pod Security Admission</h2> <p>Despite the crucial issue it was trying to solve, PodSecurityPolicy presented some major flaws:</p> <ul> <li><strong>Flawed authorization model</strong> - users can create a pod if they have the <strong>use</strong> verb on the PSP that allows that pod or the pod's service account has the <strong>use</strong> permission on the allowing PSP.</li> <li><strong>Difficult to roll out</strong> - PSP fail-closed. That is, in the absence of a policy, all pods are denied. It mostly means that it cannot be enabled by default and that users have to add PSPs for all workloads before enabling the feature, thus providing no audit mode to discover which pods would not be allowed by the new policy. The opt-in model also leads to insufficient test coverage and frequent breakage due to cross-feature incompatibility. And unlike RBAC, there was no strong culture of shipping PSP manifests with projects.</li> <li><strong>Inconsistent unbounded API</strong> - the API has grown with lots of inconsistencies notably because of many requests for niche use cases: e.g. labels, scheduling, fine-grained volume controls, etc. It has poor composability with a weak prioritization model, leading to unexpected mutation priority. It made it really difficult to combine PSP with other third-party admission controllers.</li> <li><strong>Require security knowledge</strong> - effective usage still requires an understanding of Linux security primitives. e.g. MustRunAsNonRoot + AllowPrivilegeEscalation.</li> </ul> <p>The experience with PodSecurityPolicy concluded that most users care for two or three policies, which led to the creation of the <a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/">Pod Security Standards</a>, that define three policies:</p> <ul> <li><strong>Privileged</strong> - unrestricted policy.</li> <li><strong>Baseline</strong> - minimally restrictive policy, allowing the default pod configuration.</li> <li><strong>Restricted</strong> - security best practice policy.</li> </ul> <p>The replacement for PSP, the new <a href="https://kubernetes.io/docs/concepts/security/pod-security-admission/">Pod Security Admission</a> is an in-tree, stable for Kubernetes v1.25, admission plugin to enforce these standards at the namespace level. It makes it easier to enforce basic pod security without deep security knowledge. For more sophisticated use cases, you might need a third-party solution that can be easily combined with Pod Security Admission.</p> <h2 id="what-s-next">What's next</h2> <p>For further details on the SIG Auth processes, covering PodSecurityPolicy removal and creation of Pod Security admission, the <a href="https://www.youtube.com/watch?v=SFtHRmPuhEw">SIG auth update at KubeCon NA 2019</a> and the <a href="https://www.youtube.com/watch?v=HsRRmlTJpls">PodSecurityPolicy Replacement: Past, Present, and Future</a> presentation at KubeCon NA 2021 records are available.</p> <p>Particularly on the PSP removal, the <a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">PodSecurityPolicy Deprecation: Past, Present, and Future</a> blog post is still accurate.</p> <p>And for the new Pod Security admission, <a href="https://kubernetes.io/docs/concepts/security/pod-security-admission/">documentation is available</a>. In addition, the blog post <a href="https://kubernetes.io/blog/2021/12/09/pod-security-admission-beta/">Kubernetes 1.23: Pod Security Graduates to Beta</a> along with the KubeCon EU 2022 presentation <a href="https://www.youtube.com/watch?v=gcz5VsvOYmI">The Hitchhiker's Guide to Pod Security</a> give great hands-on tutorials to learn.</p></description></item><item><title>Blog: Kubernetes v1.25: Combiner</title><link>https://kubernetes.io/blog/2022/08/23/kubernetes-v1-25-release/</link><pubDate>Tue, 23 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/08/23/kubernetes-v1-25-release/</guid><description> <p><strong>Authors</strong>: <a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.25/release-team.md">Kubernetes 1.25 Release Team</a></p> <p>Announcing the release of Kubernetes v1.25!</p> <p>This release includes a total of 40 enhancements. Fifteen of those enhancements are entering Alpha, ten are graduating to Beta, and thirteen are graduating to Stable. We also have two features being deprecated or removed.</p> <h2 id="release-theme-and-logo">Release theme and logo</h2> <p><strong>Kubernetes 1.25: Combiner</strong></p> <figure class="release-logo"> <img src="https://kubernetes.io/images/blog/2022-08-23-kubernetes-1.25-release/kubernetes-1.25.png" alt="Combiner logo"/> </figure> <p>The theme for Kubernetes v1.25 is <em>Combiner</em>.</p> <p>The Kubernetes project itself is made up of many, many individual components that, when combined, take the form of the project you see today. It is also built and maintained by many individuals, all of them with different skills, experiences, histories, and interests, who join forces not just as the release team but as the many SIGs that support the project and the community year-round.</p> <p>With this release, we wish to honor the collaborative, open spirit that takes us from isolated developers, writers, and users spread around the globe to a combined force capable of changing the world. Kubernetes v1.25 includes a staggering 40 enhancements, none of which would exist without the incredible power we have when we work together.</p> <p>Inspired by our release lead's son, Albert Song, Kubernetes v1.25 is named for each and every one of you, no matter how you choose to contribute your unique power to the combined force that becomes Kubernetes.</p> <h2 id="what-s-new-major-themes">What's New (Major Themes)</h2> <h3 id="pod-security-changes">PodSecurityPolicy is removed; Pod Security Admission graduates to Stable</h3> <p>PodSecurityPolicy was initially <a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">deprecated in v1.21</a>, and with the release of v1.25, it has been removed. The updates required to improve its usability would have introduced breaking changes, so it became necessary to remove it in favor of a more friendly replacement. That replacement is <a href="https://kubernetes.io/docs/concepts/security/pod-security-admission/">Pod Security Admission</a>, which graduates to Stable with this release. If you are currently relying on PodSecurityPolicy, please follow the instructions for <a href="https://kubernetes.io/docs/tasks/configure-pod-container/migrate-from-psp/">migration to Pod Security Admission</a>.</p> <h3 id="ephemeral-containers-graduate-to-stable">Ephemeral Containers Graduate to Stable</h3> <p><a href="https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/">Ephemeral Containers</a> are containers that exist for only a limited time within an existing pod. This is particularly useful for troubleshooting when you need to examine another container but cannot use <code>kubectl exec</code> because that container has crashed or its image lacks debugging utilities. Ephemeral containers graduated to Beta in Kubernetes v1.23, and with this release, the feature graduates to Stable.</p> <h3 id="support-for-cgroups-v2-graduates-to-stable">Support for cgroups v2 Graduates to Stable</h3> <p>It has been more than two years since the Linux kernel cgroups v2 API was declared stable. With some distributions now defaulting to this API, Kubernetes must support it to continue operating on those distributions. cgroups v2 offers several improvements over cgroups v1, for more information see the <a href="https://kubernetes.io/docs/concepts/architecture/cgroups/">cgroups v2</a> documentation. While cgroups v1 will continue to be supported, this enhancement puts us in a position to be ready for its eventual deprecation and replacement.</p> <h3 id="improved-windows-support">Improved Windows support</h3> <ul> <li><a href="http://perf-dash.k8s.io/#/?jobname=soak-tests-capz-windows-2019">Performance dashboards</a> added support for Windows</li> <li><a href="https://github.com/kubernetes/kubernetes/issues/51540">Unit tests</a> added support for Windows</li> <li><a href="https://github.com/kubernetes/kubernetes/pull/108592">Conformance tests</a> added support for Windows</li> <li>New GitHub repository created for <a href="https://github.com/kubernetes-sigs/windows-operational-readiness">Windows Operational Readiness</a></li> </ul> <h3 id="moved-container-registry-service-from-k8s-gcr-io-to-registry-k8s-io">Moved container registry service from k8s.gcr.io to registry.k8s.io</h3> <p><a href="https://github.com/kubernetes/kubernetes/pull/109938">Moving container registry from k8s.gcr.io to registry.k8s.io</a> got merged. For more details, see the <a href="https://github.com/kubernetes/k8s.io/wiki/New-Registry-url-for-Kubernetes-(registry.k8s.io)">wiki page</a>, <a href="https://groups.google.com/a/kubernetes.io/g/dev/c/DYZYNQ_A6_c/m/oD9_Q8Q9AAAJ">announcement</a> was sent to the kubernetes development mailing list.</p> <h3 id="promoted-seccompdefault-to-beta">Promoted SeccompDefault to Beta</h3> <p>SeccompDefault promoted to beta, see the tutorial <a href="https://kubernetes.io/docs/tutorials/security/seccomp/#enable-the-use-of-runtimedefault-as-the-default-seccomp-profile-for-all-workloads">Restrict a Container's Syscalls with seccomp</a> for more details.</p> <h3 id="promoted-endport-in-network-policy-to-stable">Promoted endPort in Network Policy to Stable</h3> <p>Promoted <code>endPort</code> in <a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/#targeting-a-range-of-ports">Network Policy</a> to GA. Network Policy providers that support <code>endPort</code> field now can use it to specify a range of ports to apply a Network Policy. Previously, each Network Policy could only target a single port.</p> <p>Please be aware that <code>endPort</code> field <strong>must be supported</strong> by the Network Policy provider. If your provider does not support <code>endPort</code>, and this field is specified in a Network Policy, the Network Policy will be created covering only the port field (single port).</p> <h3 id="promoted-local-ephemeral-storage-capacity-isolation-to-stable">Promoted Local Ephemeral Storage Capacity Isolation to Stable</h3> <p>The <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/361-local-ephemeral-storage-isolation">Local Ephemeral Storage Capacity Isolation</a> feature moved to GA. This was introduced as alpha in 1.8, moved to beta in 1.10, and it is now a stable feature. It provides support for capacity isolation of local ephemeral storage between pods, such as <code>EmptyDir</code>, so that a pod can be hard limited in its consumption of shared resources by evicting Pods if its consumption of local ephemeral storage exceeds that limit.</p> <h3 id="promoted-core-csi-migration-to-stable">Promoted core CSI Migration to Stable</h3> <p><a href="https://kubernetes.io/blog/2021/12/10/storage-in-tree-to-csi-migration-status-update/#quick-recap-what-is-csi-migration-and-why-migrate">CSI Migration</a> is an ongoing effort that SIG Storage has been working on for a few releases. The goal is to move in-tree volume plugins to out-of-tree CSI drivers and eventually remove the in-tree volume plugins. The <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/625-csi-migration">core CSI Migration</a> feature moved to GA. CSI Migration for GCE PD and AWS EBS also moved to GA. CSI Migration for vSphere remains in beta (but is on by default). CSI Migration for Portworx moved to Beta (but is off-by-default).</p> <h3 id="promoted-csi-ephemeral-volume-to-stable">Promoted CSI Ephemeral Volume to Stable</h3> <p>The <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/596-csi-inline-volumes">CSI Ephemeral Volume</a> feature allows CSI volumes to be specified directly in the pod specification for ephemeral use cases. They can be used to inject arbitrary states, such as configuration, secrets, identity, variables or similar information, directly inside pods using a mounted volume. This was initially introduced in 1.15 as an alpha feature, and it moved to GA. This feature is used by some CSI drivers such as the <a href="https://github.com/kubernetes-sigs/secrets-store-csi-driver">secret-store CSI driver</a>.</p> <h3 id="promoted-crd-validation-expression-language-to-beta">Promoted CRD Validation Expression Language to Beta</h3> <p><a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-api-machinery/2876-crd-validation-expression-language/README.md">CRD Validation Expression Language</a> is promoted to beta, which makes it possible to declare how custom resources are validated using the <a href="https://github.com/google/cel-spec">Common Expression Language (CEL)</a>. Please see the <a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules">validation rules</a> guide.</p> <h3 id="promoted-server-side-unknown-field-validation-to-beta">Promoted Server Side Unknown Field Validation to Beta</h3> <p>Promoted the <code>ServerSideFieldValidation</code> feature gate to beta (on by default). This allows optionally triggering schema validation on the API server that errors when unknown fields are detected. This allows the removal of client-side validation from kubectl while maintaining the same core functionality of erroring out on requests that contain unknown or invalid fields.</p> <h3 id="introduced-kms-v2-api">Introduced KMS v2 API</h3> <p>Introduce KMS v2alpha1 API to add performance, rotation, and observability improvements. Encrypt data at rest (ie Kubernetes <code>Secrets</code>) with DEK using AES-GCM instead of AES-CBC for kms data encryption. No user action is required. Reads with AES-GCM and AES-CBC will continue to be allowed. See the guide <a href="https://kubernetes.io/docs/tasks/administer-cluster/kms-provider/">Using a KMS provider for data encryption</a> for more information.</p> <h3 id="kube-proxy-images-are-now-based-on-distroless-images">Kube-proxy images are now based on distroless images</h3> <p>In previous releases, kube-proxy container images were built using Debian as the base image. Starting with this release, the images are now built using <a href="https://github.com/GoogleContainerTools/distroless">distroless</a>. This change reduced image size by almost 50% and decreased the number of installed packages and files to only those strictly required for kube-proxy to do its job.</p> <h2 id="other-updates">Other Updates</h2> <h3 id="graduations-to-stable">Graduations to Stable</h3> <p>This release includes a total of thirteen enhancements promoted to stable:</p> <ul> <li><a href="https://github.com/kubernetes/enhancements/issues/277">Ephemeral Containers</a></li> <li><a href="https://github.com/kubernetes/enhancements/issues/361">Local Ephemeral Storage Resource Management</a></li> <li><a href="https://github.com/kubernetes/enhancements/issues/596">CSI Ephemeral Volumes</a></li> <li><a href="https://github.com/kubernetes/enhancements/issues/625">CSI Migration - Core</a></li> <li><a href="https://github.com/kubernetes/enhancements/issues/785">Graduate the kube-scheduler ComponentConfig to GA</a></li> <li><a href="https://github.com/kubernetes/enhancements/issues/1487">CSI Migration - AWS</a></li> <li><a href="https://github.com/kubernetes/enhancements/issues/1488">CSI Migration - GCE</a></li> <li><a href="https://github.com/kubernetes/enhancements/issues/1591">DaemonSets Support MaxSurge</a></li> <li><a href="https://github.com/kubernetes/enhancements/issues/2079">NetworkPolicy Port Range</a></li> <li><a href="https://github.com/kubernetes/enhancements/issues/2254">cgroups v2</a></li> <li><a href="https://github.com/kubernetes/enhancements/issues/2579">Pod Security Admission</a></li> <li><a href="https://github.com/kubernetes/enhancements/issues/2599">Add <code>minReadySeconds</code> to Statefulsets</a></li> <li><a href="https://github.com/kubernetes/enhancements/issues/2802">Identify Windows pods at API admission level authoritatively</a></li> </ul> <h3 id="deprecations-and-removals">Deprecations and Removals</h3> <p>Two features were <a href="https://kubernetes.io/blog/2022/08/04/upcoming-changes-in-kubernetes-1-25/">deprecated or removed</a> from Kubernetes with this release.</p> <ul> <li><a href="https://github.com/kubernetes/enhancements/issues/5">PodSecurityPolicy is removed</a></li> <li><a href="https://github.com/kubernetes/enhancements/issues/3446">GlusterFS plugin deprecated from available in-tree drivers</a></li> </ul> <h3 id="release-notes">Release Notes</h3> <p>The complete details of the Kubernetes v1.25 release are available in our <a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.25.md">release notes</a>.</p> <h3 id="availability">Availability</h3> <p>Kubernetes v1.25 is available for download on <a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.25.0">GitHub</a>. To get started with Kubernetes, check out these <a href="https://kubernetes.io/docs/tutorials/">interactive tutorials</a> or run local Kubernetes clusters using containers as “nodes”, with <a href="https://kind.sigs.k8s.io/">kind</a>. You can also easily install 1.25 using <a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">kubeadm</a>.</p> <h3 id="release-team">Release Team</h3> <p>Kubernetes is only possible with the support, commitment, and hard work of its community. Each release team is made up of dedicated community volunteers who work together to build the many pieces that, when combined, make up the Kubernetes releases you rely on. This requires the specialized skills of people from all corners of our community, from the code itself to its documentation and project management.</p> <p>We would like to thank the entire release team for the hours spent hard at work to ensure we deliver a solid Kubernetes v1.25 release for our community. Every one of you had a part to play in building this, and you all executed beautifully. We would like to extend special thanks to our fearless release lead, Cici Huang, for all she did to guarantee we had what we needed to succeed.</p> <h3 id="user-highlights">User Highlights</h3> <ul> <li>Finleap Connect operates in a highly regulated environment. <a href="https://www.cncf.io/case-studies/finleap-connect/">In 2019, they had five months to implement mutual TLS (mTLS) across all services in their clusters for their business code to comply with the new European PSD2 payment directive</a>.</li> <li>PNC sought to develop a way to ensure new code would meet security standards and audit compliance requirements automatically—replacing the cumbersome 30-day manual process they had in place. Using Knative, <a href="https://www.cncf.io/case-studies/pnc-bank/">PNC developed internal tools to automatically check new code and changes to existing code</a>.</li> <li>Nexxiot needed highly-reliable, secure, performant, and cost efficient Kubernetes clusters. <a href="https://www.cncf.io/case-studies/nexxiot/">They turned to Cilium as the CNI to lock down their clusters and enable resilient networking with reliable day two operations</a>.</li> <li>Because the process of creating cyber insurance policies is a complicated multi-step process, At-Bay sought to improve operations by using asynchronous message-based communication patterns/facilities. <a href="https://www.cncf.io/case-studies/at-bay/">They determined that Dapr fulfilled its desired list of requirements and much more</a>.</li> </ul> <h3 id="ecosystem-updates">Ecosystem Updates</h3> <ul> <li>KubeCon + CloudNativeCon North America 2022 will take place in Detroit, Michigan from 24 – 28 October 2022! You can find more information about the conference and registration on the <a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/">event site</a>.</li> <li>KubeDay event series kicks off with KubeDay Japan on December 7! Register or submit a proposal on the <a href="https://events.linuxfoundation.org/kubeday-japan/">event site</a></li> <li>In the <a href="https://www.cncf.io/announcements/2022/02/10/cncf-sees-record-kubernetes-and-container-adoption-in-2021-cloud-native-survey/">2021 Cloud Native Survey</a>, the CNCF saw record Kubernetes and container adoption. Take a look at the <a href="https://www.cncf.io/reports/cncf-annual-survey-2021/">results of the survey</a>.</li> </ul> <h3 id="project-velocity">Project Velocity</h3> <p>The <a href="https://k8s.devstats.cncf.io/d/12/dashboards?orgId=1&amp;refresh=15m">CNCF K8s DevStats</a> project aggregates a number of interesting data points related to the velocity of Kubernetes and various sub-projects. This includes everything from individual contributions to the number of companies that are contributing, and is an illustration of the depth and breadth of effort that goes into evolving this ecosystem.</p> <p>In the v1.25 release cycle, which <a href="https://github.com/kubernetes/sig-release/tree/master/releases/release-1.25">ran for 14 weeks</a> (May 23 to August 23), we saw contributions from <a href="https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;var-period_name=v1.24.0%20-%20v1.25.0&amp;var-metric=contributions">1065 companies</a> and <a href="https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;var-period_name=v1.24.0%20-%20v1.25.0&amp;var-metric=contributions&amp;var-repogroup_name=Kubernetes&amp;var-country_name=All&amp;var-companies=All&amp;var-repo_name=kubernetes%2Fkubernetes">1620 individuals</a>.</p> <h2 id="upcoming-release-webinar">Upcoming Release Webinar</h2> <p>Join members of the Kubernetes v1.25 release team on Thursday September 22, 2022 10am – 11am PT to learn about the major features of this release, as well as deprecations and removals to help plan for upgrades. For more information and registration, visit the <a href="https://community.cncf.io/events/details/cncf-cncf-online-programs-presents-cncf-live-webinar-kubernetes-v125-release/">event page</a>.</p> <h2 id="get-involved">Get Involved</h2> <p>The simplest way to get involved with Kubernetes is by joining one of the many <a href="https://github.com/kubernetes/community/blob/master/sig-list.md">Special Interest Groups</a> (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly <a href="https://github.com/kubernetes/community/tree/master/communication">community meeting</a>, and through the channels below:</p> <ul> <li>Find out more about contributing to Kubernetes at the <a href="https://www.kubernetes.dev/">Kubernetes Contributors</a> website</li> <li>Follow us on Twitter <a href="https://twitter.com/kubernetesio">@Kubernetesio</a> for the latest updates</li> <li>Join the community discussion on <a href="https://discuss.kubernetes.io/">Discuss</a></li> <li>Join the community on <a href="http://slack.k8s.io/">Slack</a></li> <li>Post questions (or answer questions) on <a href="https://serverfault.com/questions/tagged/kubernetes">Server Fault</a>.</li> <li>Share your Kubernetes <a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">story</a></li> <li>Read more about what’s happening with Kubernetes on the <a href="https://kubernetes.io/blog/">blog</a></li> <li>Learn more about the <a href="https://github.com/kubernetes/sig-release/tree/master/release-team">Kubernetes Release Team</a></li> </ul></description></item><item><title>Blog: Spotlight on SIG Storage</title><link>https://kubernetes.io/blog/2022/08/22/sig-storage-spotlight/</link><pubDate>Mon, 22 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/08/22/sig-storage-spotlight/</guid><description> <p><strong>Author</strong>: Frederico Muñoz (SAS)</p> <p>Since the very beginning of Kubernetes, the topic of persistent data and how to address the requirement of stateful applications has been an important topic. Support for stateless deployments was natural, present from the start, and garnered attention, becoming very well-known. Work on better support for stateful applications was also present from early on, with each release increasing the scope of what could be run on Kubernetes.</p> <p>Message queues, databases, clustered filesystems: these are some examples of the solutions that have different storage requirements and that are, today, increasingly deployed in Kubernetes. Dealing with ephemeral and persistent storage, local or remote, file or block, from many different vendors, while considering how to provide the needed resiliency and data consistency that users expect, all of this is under SIG Storage's umbrella.</p> <p>In this SIG Storage spotlight, <a href="https://twitter.com/fredericomunoz">Frederico Muñoz</a> (Cloud &amp; Architecture Lead at SAS) talked with <a href="https://twitter.com/2000xyang">Xing Yang</a>, Tech Lead at VMware and co-chair of SIG Storage, on how the SIG is organized, what are the current challenges and how anyone can get involved and contribute.</p> <h2 id="about-sig-storage">About SIG Storage</h2> <p><strong>Frederico (FSM)</strong>: Hello, thank you for the opportunity of learning more about SIG Storage. Could you tell us a bit about yourself, your role, and how you got involved in SIG Storage.</p> <p><strong>Xing Yang (XY)</strong>: I am a Tech Lead at VMware, working on Cloud Native Storage. I am also a Co-Chair of SIG Storage. I started to get involved in K8s SIG Storage at the end of 2017, starting with contributing to the <a href="https://kubernetes.io/docs/concepts/storage/volume-snapshots/">VolumeSnapshot</a> project. At that time, the VolumeSnapshot project was still in an experimental, pre-alpha stage. It needed contributors. So I volunteered to help. Then I worked with other community members to bring VolumeSnapshot to Alpha in K8s 1.12 release in 2018, Beta in K8s 1.17 in 2019, and eventually GA in 1.20 in 2020.</p> <p><strong>FSM</strong>: Reading the <a href="https://github.com/kubernetes/community/blob/master/sig-storage/charter.md">SIG Storage charter</a> alone it’s clear that SIG Storage covers a lot of ground, could you describe how the SIG is organised?</p> <p><strong>XY</strong>: In SIG Storage, there are two Co-Chairs and two Tech Leads. Saad Ali from Google and myself are Co-Chairs. Michelle Au from Google and Jan Šafránek from Red Hat are Tech Leads.</p> <p>We have bi-weekly meetings where we go through features we are working on for each particular release, getting the statuses, making sure each feature has dev owners and reviewers working on it, and reminding people about the release deadlines, etc. More information on the SIG is on the <a href="https://github.com/kubernetes/community/tree/master/sig-storage">community page</a>. People can also add PRs that need attention, design proposals that need discussion, and other topics to the meeting agenda doc. We will go over them after project tracking is done.</p> <p>We also have other regular meetings, i.e., CSI Implementation meeting, Object Bucket API design meeting, and one-off meetings for specific topics if needed. There is also a <a href="https://github.com/kubernetes/community/blob/master/wg-data-protection/README.md">K8s Data Protection Workgroup</a> that is sponsored by SIG Storage and SIG Apps. SIG Storage owns or co-owns features that are being discussed at the Data Protection WG.</p> <h2 id="storage-and-kubernetes">Storage and Kubernetes</h2> <p><strong>FSM</strong>: Storage is such a foundational component in so many things, not least in Kubernetes: what do you think are the Kubernetes-specific challenges in terms of storage management?</p> <p><strong>XY</strong>: In Kubernetes, there are multiple components involved for a volume operation. For example, creating a Pod to use a PVC has multiple components involved. There are the Attach Detach Controller and the external-attacher working on attaching the PVC to the pod. There’s the Kubelet that works on mounting the PVC to the pod. Of course the CSI driver is involved as well. There could be race conditions sometimes when coordinating between multiple components.</p> <p>Another challenge is regarding core vs <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">Custom Resource Definitions</a> (CRD), not really storage specific. CRD is a great way to extend Kubernetes capabilities while not adding too much code to the Kubernetes core itself. However, this also means there are many external components that are needed when running a Kubernetes cluster.</p> <p>From the SIG Storage side, one most notable example is Volume Snapshot. Volume Snapshot APIs are defined as CRDs. API definitions and controllers are out-of-tree. There is a common snapshot controller and a snapshot validation webhook that should be deployed on the control plane, similar to how kube-controller-manager is deployed. Although Volume Snapshot is a CRD, it is a core feature of SIG Storage. It is recommended for the K8s cluster distros to deploy Volume Snapshot CRDs, the snapshot controller, and the snapshot validation webhook, however, most of the time we don’t see distros deploy them. So this becomes a problem for the storage vendors: now it becomes their responsibility to deploy these non-driver specific common components. This could cause conflicts if a customer wants to use more than one storage system and deploy more than one CSI driver.</p> <p><strong>FSM</strong>: Not only the complexity of a single storage system, you have to consider how they will be used together in Kubernetes?</p> <p><strong>XY</strong>: Yes, there are many different storage systems that can provide storage to containers in Kubernetes. They don’t work the same way. It is challenging to find a solution that works for everyone.</p> <p><strong>FSM</strong>: Storage in Kubernetes also involves interacting with external solutions, perhaps more so than other parts of Kubernetes. Is this interaction with vendors and external providers challenging? Has it evolved with time in any way?</p> <p><strong>XY</strong>: Yes, it is definitely challenging. Initially Kubernetes storage had in-tree volume plugin interfaces. Multiple storage vendors implemented in-tree interfaces and have volume plugins in the Kubernetes core code base. This caused lots of problems. If there is a bug in a volume plugin, it affects the entire Kubernetes code base. All volume plugins must be released together with Kubernetes. There was no flexibility if storage vendors need to fix a bug in their plugin or want to align with their own product release.</p> <p><strong>FSM</strong>: That’s where CSI enters the game?</p> <p><strong>XY</strong>: Exactly, then there comes <a href="https://kubernetes-csi.github.io/docs/">Container Storage Interface</a> (CSI). This is an industry standard trying to design common storage interfaces so that a storage vendor can write one plugin and have it work across a range of container orchestration systems (CO). Now Kubernetes is the main CO, but back when CSI just started, there were Docker, Mesos, Cloud Foundry, in addition to Kubernetes. CSI drivers are out-of-tree so bug fixes and releases can happen at their own pace.</p> <p>CSI is definitely a big improvement compared to in-tree volume plugins. Kubernetes implementation of CSI has been GA <a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">since the 1.13 release</a>. It has come a long way. SIG Storage has been working on moving in-tree volume plugins to out-of-tree CSI drivers for several releases now.</p> <p><strong>FSM</strong>: Moving drivers away from the Kubernetes main tree and into CSI was an important improvement.</p> <p><strong>XY</strong>: CSI interface is an improvement over the in-tree volume plugin interface, however, there are still challenges. There are lots of storage systems. Currently <a href="https://kubernetes-csi.github.io/docs/drivers.html">there are more than 100 CSI drivers listed in CSI driver docs</a>. These storage systems are also very diverse. So it is difficult to design a common API that works for all. We introduced capabilities at CSI driver level, but we also have challenges when volumes provisioned by the same driver have different behaviors. The other day we just had a meeting discussing Per Volume CSI Driver Capabilities. We have a problem differentiating some CSI driver capabilities when the same driver supports both block and file volumes. We are going to have follow up meetings to discuss this problem.</p> <h2 id="ongoing-challenges">Ongoing challenges</h2> <p><strong>FSM</strong>: Specifically for the <a href="https://github.com/kubernetes/sig-release/tree/master/releases/release-1.25">1.25 release</a> we can see that there are a relevant number of storage-related <a href="https://bit.ly/k8s125-enhancements">KEPs</a> in the pipeline, would you say that this release is particularly important for the SIG?</p> <p><strong>XY</strong>: I wouldn’t say one release is more important than other releases. In any given release, we are working on a few very important things.</p> <p><strong>FSM</strong>: Indeed, but are there any 1.25 specific specificities and highlights you would like to point out though?</p> <p><strong>XY</strong>: Yes. For the 1.25 release, I want to highlight the following:</p> <ul> <li><a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/625-csi-migration">CSI Migration</a> is an on-going effort that SIG Storage has been working on for a few releases now. The goal is to move in-tree volume plugins to out-of-tree CSI drivers and eventually remove the in-tree volume plugins. There are 7 KEPs that we are targeting in 1.25 are related to CSI migration. There is one core KEP for the general CSI Migration feature. That is targeting GA in 1.25. CSI Migration for GCE PD and AWS EBS are targeting GA. CSI Migration for vSphere is targeting to have the feature gate on by default while staying in 1.25 that are in Beta. Ceph RBD and PortWorx are targeting Beta, with feature gate off by default. Ceph FS is targeting Alpha.</li> <li>The second one I want to highlight is <a href="https://github.com/kubernetes-sigs/container-object-storage-interface-spec">COSI, the Container Object Storage Interface</a>. This is a sub-project under SIG Storage. COSI proposes object storage Kubernetes APIs to support orchestration of object store operations for Kubernetes workloads. It also introduces gRPC interfaces for object storage providers to write drivers to provision buckets. The COSI team has been working on this project for more than two years now. The COSI feature is targeting Alpha in 1.25. The KEP just got merged. The COSI team is working on updating the implementation based on the updated KEP.</li> <li>Another feature I want to mention is <a href="https://github.com/kubernetes/enhancements/issues/596">CSI Ephemeral Volume</a> support. This feature allows CSI volumes to be specified directly in the pod specification for ephemeral use cases. They can be used to inject arbitrary states, such as configuration, secrets, identity, variables or similar information, directly inside pods using a mounted volume. This was initially introduced in 1.15 as an alpha feature, and it is now targeting GA in 1.25.</li> </ul> <p><strong>FSM</strong>: If you had to single something out, what would be the most pressing areas the SIG is working on?</p> <p><strong>XY</strong>: CSI migration is definitely one area that the SIG has put in lots of effort and it has been on-going for multiple releases now. It involves work from multiple cloud providers and storage vendors as well.</p> <h2 id="community-involvement">Community involvement</h2> <p><strong>FSM</strong>: Kubernetes is a community-driven project. Any recommendation for anyone looking into getting involved in SIG Storage work? Where should they start?</p> <p><strong>XY</strong>: Take a look at the <a href="https://github.com/kubernetes/community/tree/master/sig-storage">SIG Storage community page</a>, it has lots of information on how to get started. There are <a href="https://github.com/kubernetes/community/blob/master/sig-storage/annual-report-2021.md">SIG annual reports</a> that tell you what we did each year. Take a look at the Contributing guide. It has links to presentations that can help you get familiar with Kubernetes storage concepts.</p> <p>Join our <a href="https://github.com/kubernetes/community/tree/master/sig-storage#meetings">bi-weekly meetings on Thursdays</a>. Learn how the SIG operates and what we are working on for each release. Find a project that you are interested in and help out. As I mentioned earlier, I got started in SIG Storage by contributing to the Volume Snapshot project.</p> <p><strong>FSM</strong>: Any closing thoughts you would like to add?</p> <p><strong>XY</strong>: SIG Storage always welcomes new contributors. We need contributors to help with building new features, fixing bugs, doing code reviews, writing tests, monitoring test grid health, and improving documentation, etc.</p> <p><strong>FSM</strong>: Thank you so much for your time and insights into the workings of SIG Storage!</p></description></item><item><title>Blog: Stargazing, solutions and staycations: the Kubernetes 1.24 release interview</title><link>https://kubernetes.io/blog/2022/08/18/stargazing-solutions-and-staycations-the-kubernetes-1.24-release-interview/</link><pubDate>Thu, 18 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/08/18/stargazing-solutions-and-staycations-the-kubernetes-1.24-release-interview/</guid><description> <p><strong>Author</strong>: Craig Box (Google)</p> <p>The Kubernetes project has participants from all around the globe. Some are friends, some are colleagues, and some are strangers. The one thing that unifies them, no matter their differences, are that they all have an interesting story. It is my pleasure to be the documentarian for the stories of the Kubernetes community in the weekly <a href="https://kubernetespodcast.com/">Kubernetes Podcast from Google</a>. With every new Kubernetes release comes an interview with the release team lead, telling the story of that release, but also their own personal story.</p> <p>With 1.25 around the corner, <a href="https://www.google.com/search?q=%22release+interview%22+site%3Akubernetes.io%2Fblog">the tradition continues</a> with a look back at the story of 1.24. That release was led by <a href="https://twitter.com/jameslaverack">James Laverack</a> of Jetstack. <a href="https://kubernetespodcast.com/episode/178-kubernetes-1.24/">James was on the podcast</a> in May, and while you can read his story below, if you can, please do listen to it in his own voice.</p> <p>Make sure you <a href="https://kubernetespodcast.com/subscribe/">subscribe, wherever you get your podcasts</a>, so you hear all our stories from the cloud native community, including the story of 1.25 next week.</p> <p><em>This transcript has been lightly edited and condensed for clarity.</em></p> <hr> <p><strong>CRAIG BOX: Your journey to Kubernetes went through the financial technology (fintech) industry. Tell me a little bit about how you came to software?</strong></p> <p>JAMES LAVERACK: I took a pretty traditional path to software engineering. I went through school and then I did a computer science degree at the University of Bristol, and then I just ended up taking a software engineer job from there. Somewhat rather by accident, I ended up doing fintech work, which is pretty interesting, pretty engaging.</p> <p>But in my most recent fintech job before I joined <a href="https://www.jetstack.io/">Jetstack</a>, I ended up working on a software project. We needed Kubernetes to solve a technical problem. So we implemented Kubernetes, and as often happens, I ended up as the one person of a team that understood the infrastructure, while everyone else was doing all of the application development.</p> <p>I ended up enjoying the infrastructure side so much that I decided to move and do that full time. So I looked around and I found Jetstack, whose offices were literally across the road. I could see them out of our office window. And so I decided to just hop across the road and join them, and do all of this Kubernetes stuff more.</p> <p><strong>CRAIG BOX: What's the tech scene like in Bristol? You went there for school and never left?</strong></p> <p>JAMES LAVERACK: Pretty much. It's happened to a lot of people I know and a lot of my friends, is that you go to University somewhere and you're just kind of stuck there forever, so to speak. It's been known for being quite hot in the area in terms of that part of the UK. It has a lot of tech companies, obviously, it was a fintech company I worked at before. I think some larger companies have offices there. For &quot;not London&quot;, it's not doing too bad, I don't think.</p> <p><strong>CRAIG BOX: When you say hot, though, that's tech industry, not weather, I'm assuming.</strong></p> <p>JAMES LAVERACK: Yeah, weather is the usual UK. It's kind of a nice overcast and rainy, which I quite like. I'm quite fond of it.</p> <p><strong>CRAIG BOX: Public transport good?</strong></p> <p>JAMES LAVERACK: Buses are all right. We've got a new bus installed recently, which everyone hated while it was being built. And now it's complete, everyone loves. So, standard I think.</p> <p><strong>CRAIG BOX: That is the way. As someone who lived in London for a long time, it's very easy for me to say &quot;well, London's kind of like Singapore. It's its own little city-state.&quot; But whenever we did go out to that part of the world, Bath especially, a very lovely town</strong></p> <p>JAMES LAVERACK: Oh, Bath's lovely. I've been a couple of times.</p> <p><strong>CRAIG BOX: Have you been to Box?</strong></p> <p>JAMES LAVERACK: To where, sorry?</p> <p><strong>CRAIG BOX: There's <a href="https://en.wikipedia.org/wiki/Box,_Wiltshire">a town called Box</a> just outside Bath. I had my picture taken outside all the buildings. Proclaimed myself the mayor.</strong></p> <p>JAMES LAVERACK: Oh, no, I don't think I have.</p> <p><strong>CRAIG BOX: Well, look it up if you're ever in the region, everybody. Let's get back to Jetstack, though. They were across the road. Great company, the <a href="https://www.jetstack.io/about/mattbarker/">two</a> <a href="https://www.jetstack.io/about/mattbates/">Matts</a>, the co-founders there. What was the interview process like for you?</strong></p> <p>JAMES LAVERACK: It was pretty relaxed. One lunchtime, I just walked down the road and went to a coffee shop with Matt and we had this lovely conversation talking about my background and Jetstack and what I was looking to achieve in a new role and all this. And I'd applied to be a software engineer. And then they kind of at the end of it, he looked over at me and was like, &quot;well, how about being a solutions engineer instead?&quot; And I was like, what's that?</p> <p>And he's like, &quot;well, you know, it's just effectively being a software consultant. You go, you help companies implement Kubernetes, users, saying all that stuff you enjoy. But you do it full time.&quot; I was like, &quot;well, maybe.&quot; And in the end he convinced me. I ended up joining as a solutions engineer with the idea of if I didn't like it, I could transfer to be a software engineer again.</p> <p>Nearly three years later, I've never taken them up on the offer. I've just <a href="https://www.jetstack.io/blog/life-as-a-solutions-engineer/">stayed as a solutions engineer</a> the entire time.</p> <p><strong>CRAIG BOX: At the company you were working at, I guess you were effectively the consultant between the people writing the software and the deployment in Kubernetes. Did it make sense then for you to carry on in that role, as you moved to Jetstack?</strong></p> <p>JAMES LAVERACK: I think so. I think it's something that I enjoyed. Not that I didn't enjoy writing software applications. I always enjoyed it, and we had a really interesting product and a really fun team. But I just found that more interesting. And it was becoming increasingly difficult to justify spending time on it when we had an application to write.</p> <p>Which was just completely fine, and that made sense for the needs of the team at the time. But it's not what I wanted to do.</p> <p><strong>CRAIG BOX: Do you think that talks to the split between Kubernetes being for developers or for operators? Do you think there's always going to be the need to have a different set of people who are maintaining the running infrastructure versus the people who are writing the code that run on it?</strong></p> <p>JAMES LAVERACK: I think to some extent, yes, whether or not that's a separate platform team or whether or not that is because the people running it are consultants of some kind. Or whether or not this has been abstracted away from you in some of the more batteries-included versions of Kubernetes — some of the cloud-hosted ones, especially, somewhat remove that need. So I don't think it's absolutely necessary to employ a platform team. But I think someone needs to do it or you need to implicitly or explicitly pay for someone to do it in some way.</p> <p><strong>CRAIG BOX: In the three years you have been at Jetstack now, how different are the jobs that you do for the customers? Is this just a case of learning one thing and rolling it out to multiple people, or is there always a different challenge with everyone you come across?</strong></p> <p>JAMES LAVERACK: I think there's always a different challenge. My role has varied drastically. For example, a long time ago, I did an Istio install. But it was a relatively complicated, single mesh, multi-cluster install. And that was before multi-cluster support was really as readily available as it is now. Conversely, I've worked building custom orchestration platforms on top of Kubernetes for specific customer use cases.</p> <p>It's all varied and every single customer engagement is different. That is an element I really like about the job, that variability in how things are and how things go.</p> <p><strong>CRAIG BOX: When the platform catches up and does things like makes it easier to manage multi-cluster environments, do you go back to the customers and bring them up to date with the newest methods?</strong></p> <p>JAMES LAVERACK: It depends. Most of our engagements are to solve a specific problem. And once we've solved that problem, they may have us back. But typically speaking, in my line of work, it's not an ongoing engagement. There are some within Jetstack that do that, but not so much in my team.</p> <p><strong>CRAIG BOX: Your bio suggests that you were once called &quot;the reason any corporate policy evolves.&quot; What's the story there?</strong></p> <p>JAMES LAVERACK: [CHUCKLES] I think I just couldn't leave things well enough alone. I was talking to our operations director inside of Jetstack, and he once said to me that whenever he's thinking of a new corporate policy, he asks will it pass the James Laverack test. That is, will I look at it and find some horrendous loophole?</p> <p>For example when I first joined, I took a look at our acceptable use policy for company equipment. And it stated that you're not allowed to have copyrighted material on your laptop. And of course, this makes sense, as you know, you don't want people doing software piracy or anything. But as written, that would imply you're not allowed to have anything that is copyrighted by anyone on your machine.</p> <p><strong>CRAIG BOX: Such as perhaps the operating system that comes installed on it?</strong></p> <p>JAMES LAVERACK: Such as perhaps the operating system, or anything. And you know, this clearly didn't make any sense. So he adjusted that, and I've kind of been fiddling with that sort of policy ever since.</p> <p><strong>CRAIG BOX: The release team is often seen as an administrative role versus a pure coding role. Does that speak to the kind of change you've had in career in previously being a software developer and now being more of a consultant, or was there something else that attracted you to get involved in that particular part of the community?</strong></p> <p>JAMES LAVERACK: I wouldn't really consider it less technical. I mean, yes, you do much less coding. This is something that constantly surprises my friends and some of my colleagues, when I tell them more detail about my role. There's not really any coding involved.</p> <p>I don't think my role has really changed to have less coding. In fact, one of my more recent projects at Jetstack, a client project, involved a lot of coding. But I think that what attracted me to this role within Kubernetes is really the community. I found it really rewarding to engage with SIG Release and to engage with the release team. So I've always just enjoyed doing it, even though there is, as you say, not all that much coding involved.</p> <p><strong>CRAIG BOX: Indeed; your wife said to you, <a href="https://twitter.com/JamesLaverack/status/1483201645286678529">&quot;I don't think your job is to code anymore. You just talk to people all day.&quot;</a> How did that make you feel?</strong></p> <p>JAMES LAVERACK: Ahh, annoyed, because she was right. This was kind of a couple of months ago when I was in the middle of it with all of the Kubernetes meetings. Also, my client project at the time involved a lot of technical discussion. I was in three or four hours of calls every day. And I don't mind that. But I would come out, in part because of course you're working from home, so she sees me all the time. So I'd come out, I'd grab a coffee and be like, &quot;oh, I've got a meeting, I've got to go.&quot; And she'd be like, &quot;do you ever code anymore?&quot; I think it was in fact just after Christmas when she asked me, &quot;when was the last time you programmed anything?&quot; And I had to think about it. Then I realized that perhaps there was a problem there. Well, not a problem, but I realized that perhaps I don't code as much as I used to.</p> <p><strong>CRAIG BOX: Are you the kind of person who will pick up a hobby project to try and fix that?</strong></p> <p>JAMES LAVERACK: Absolutely. I've recently started writing <a href="https://github.com/JamesLaverack/kubernetes-minecraft-operator">a Kubernetes operator for my Minecraft server</a>. That probably tells you about the state I'm in.</p> <p><strong>CRAIG BOX: If it's got Kubernetes in it, it doesn't sound that much of a hobby.</strong></p> <p>JAMES LAVERACK: [LAUGHING] Do you not consider Kubernetes to be a hobby?</p> <p><strong>CRAIG BOX: It depends.</strong></p> <p>JAMES LAVERACK: I think I do.</p> <p><strong>CRAIG BOX: I think by now.</strong></p> <p>JAMES LAVERACK: In some extents.</p> <p><strong>CRAIG BOX: You mentioned observing the release team in process before you decided to get involved. Was that as part of working with customers and looking to see whether a particular feature would make it into a release, or was there some other reason that that was how you saw the Kubernetes community?</strong></p> <p>JAMES LAVERACK: Just after I joined Jetstack, I got the opportunity to go to KubeCon San Diego. I think we actually met there.</p> <p><strong>CRAIG BOX: We did.</strong></p> <p>JAMES LAVERACK: We had dinner, didn't we? So when I went, I'd only been at Jetstack for a few months. I really wasn't involved in the community in any serious way at all. As a result, I just ended up following around my then colleague, James Munnelly. James is lovely. And, you know, I just kind of went around with him, because he knew everyone.</p> <p>I ended up in this hotel bar with a bunch of Kubernetes people, including Stephen Augustus, the co-chair of SIG Release and holder of a bunch of other roles within the community. I happened to ask him, I want to get involved. What is a good way to get involved with the Kubernetes community, if I've never been involved before? And he said, oh, you should join the release team.</p> <p><strong>CRAIG BOX: So it's all down to where you end up in the bar with someone.</strong></p> <p>JAMES LAVERACK: Yeah, pretty much.</p> <p><strong>CRAIG BOX: If I'd got to you sooner, you could have been working on Istio.</strong></p> <p>JAMES LAVERACK: Yeah, I could've been working on Istio, I could have ended up in some other SIG doing something. I just happened to be talking to Stephen. And Stephen suggested it, and I gave it a go. And here I am three years later.</p> <p><strong>CRAIG BOX: I think I remember at the time you were working on an etcd operator?</strong></p> <p>JAMES LAVERACK: Yeah, that's correct. That was part of a client project, which they, thankfully <a href="https://github.com/improbable-eng/etcd-cluster-operator">let us open source</a>. This was an operator for etcd, where they had a requirement to run it in Kubernetes, which of course is the opposite way around to how you'd normally want to run it.</p> <p><strong>CRAIG BOX: And I remember having you up at the time, like I'm pretty sure those things exist already, and asking what the need was for there to be something different.</strong></p> <p>JAMES LAVERACK: It was that they needed something very specific. The ones that existed already were all designed to run clusters that couldn't be shut down. As long as one replica stayed up, you could keep running etcd. But they needed to be able to suspend and restart the entire cluster, which means it needs disk-persistence support, which it turns out is quite complicated.</p> <p><strong>CRAIG BOX: It's easier if you just throw all the data away.</strong></p> <p>JAMES LAVERACK: It's much easier to throw all the data away. We needed to be a little bit careful about how we managed it. We thought about forking and changing an existing one. But we realized it would probably just be as easy to start from scratch, so we did that.</p> <p><strong>CRAIG BOX: You've been a member of every release team since that point, since Kubernetes 1.18 in 2020, in a wide range of roles. Which set of roles have you been through?</strong></p> <p>JAMES LAVERACK: I started out as a release notes shadow, and did that for a couple of releases, in 1.18 and 1.19. In 1.20, I was the release notes lead. And then in 1.21, I moved into being a shadow again as an enhancement shadow, before in 1.22 becoming an enhancements lead, but in 1.23 a release lead shadow, and finally in 1.24, release lead as a whole.</p> <p><strong>CRAIG BOX: That's quite a long time to be with the release team. You're obviously going to move into an emeritus role after this release. Do you see yourself still remaining involved? Is it something that you're clearly very passionate about?</strong></p> <p>JAMES LAVERACK: I think I'm going to be around in SIG Release for as long as people want me there. I find it a really interesting part of the community. And I find the people super-interesting and super-inviting.</p> <p><strong>CRAIG BOX: Let's talk then about <a href="https://kubernetes.io/blog/2022/05/03/kubernetes-1-24-release-announcement/">Kubernetes 1.24</a>. First, as always, congratulations on the release.</strong></p> <p>JAMES LAVERACK: Thank you.</p> <p><strong>CRAIG BOX: This release consists of 46 enhancements. 14 have graduated to stable, 15 have moved to beta, and 13 are in alpha. 2 are deprecated and 2 have been removed. How is that versus other releases recently? Is that an average number? That seems like a lot of stable enhancements, especially.</strong></p> <p>JAMES LAVERACK: I think it's pretty similar. Most of the recent releases have been quite similar in the number of enhancements they have and in what categories. For example, in 1.23, the previous release, there were 47. I think 1.22, before that, had 53, so slightly more. But it's around about that number.</p> <p><strong>CRAIG BOX: You didn't want to sneak in two extra so you could say you were one more than the last one?</strong></p> <p>JAMES LAVERACK: No, I don't think so. I think we had enough going on.</p> <p><strong>CRAIG BOX: The release team is obviously beholden to what features the SIGs are developing and what their plans are. Is there ever any coordination between the release process and the SIGs in terms of things like saying, this release is going to be a catch-up release, like the old Snow Leopard releases for macOS, for example, where we say we don't want as many new features, but we really want more stabilization, and could you please work on those kind of things?</strong></p> <p>JAMES LAVERACK: Not really. The cornerstone of a Kubernetes organization is the SIGs themselves, so the special interest groups that make up the organization. It's really up to them what they want to do. We don't do any particular coordination on the style of thing that should be implemented. A lot of SIGs have roadmaps that are looking over multiple releases to try to get features that they think are important in.</p> <p><strong>CRAIG BOX: Let's talk about some of the new features in 1.24. We have been hearing for many releases now about the impending doom which is the removal of Dockershim. <a href="https://github.com/kubernetes/enhancements/issues/2221">It is gone in 1.24</a>. Do we worry?</strong></p> <p>JAMES LAVERACK: I don't think we worry. This is something that the community has been preparing for for a long time. <a href="https://kubernetes.io/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/">We've</a> <a href="https://kubernetes.io/blog/2022/02/17/dockershim-faq/">published</a> a <a href="https://kubernetes.io/blog/2021/11/12/are-you-ready-for-dockershim-removal/">lot</a> of <a href="https://kubernetes.io/blog/2022/03/31/ready-for-dockershim-removal/">documentation</a> <a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-removal-affects-you/">about</a> <a href="https://kubernetes.io/blog/2022/05/03/dockershim-historical-context/">how</a> you need to approach this. The honest truth is that most users, most application developers in Kubernetes, will simply not notice a difference or have to worry about it.</p> <p>It's only really platform teams that administer Kubernetes clusters and people in very specific circumstances that are using Docker directly, not through the Kubernetes API, that are going to experience any issue at all.</p> <p><strong>CRAIG BOX: And I see that Mirantis and Docker have developed a CRI plugin for Docker anyway, so you can just switch over to that and everything continues.</strong></p> <p>JAMES LAVERACK: Yeah, absolutely, or you can use one of the many other CRI implementations. There are two in the CNCF, <a href="https://containerd.io/">containerd</a>, and <a href="https://cri-o.io/">CRI-O</a>.</p> <p><strong>CRAIG BOX: Having gone through the process of communicating this change over several releases, what has the team learnt in terms of how we will communicate a message like this in future?</strong></p> <p>JAMES LAVERACK: I think that this has been really interesting from the perspective that this is the biggest removal that the Kubernetes project has had to date. We've removed features before. In fact, we're removing another one in this release as well. But this is one of the most user-visible changes we've made.</p> <p>I think there are very good reasons for doing it. But I think we've learned a lot about how and when to communicate, and the importance of having migration guides, the importance of having official documentation that really clarifies the thing. I think that's the real, it's an area in which the Kubernetes project has matured a lot since I've been on the team.</p> <p><strong>CRAIG BOX: What is the other feature that's being removed?</strong></p> <p>JAMES LAVERACK: The other feature that we're removing is dynamic Kubelet configuration. This is a feature that was in beta for a while. But I believe we decided that it just wasn't being used enough to justify keeping it. So we're removing it. We deprecated it back in 1.22 and we're removing it this release.</p> <p><strong>CRAIG BOX: There was a change in policy a few releases ago that talked about features not being allowed to stay in beta forever. Have there been any features that were at risk of being removed due to lack of maintenance, or are all the SIGs pretty good now at keeping their features on track?</strong></p> <p>JAMES LAVERACK: I think the SIGs are getting pretty good at it. We had a spate of a long time when a lot of features were kind of perpetually in beta. As you remember, Ingress was in beta for a long, long time.</p> <p><strong>CRAIG BOX: I choose to believe it still is.</strong></p> <p>JAMES LAVERACK: [LAUGHTER] I think it's really good that we're moving towards that stability approach with things like Kubernetes. I think it's a very positive change.</p> <p><strong>CRAIG BOX: The fact that Ingress was in beta for so long, along with things like the main workload controllers, for example, did lead people to believing that beta APIs were stable and production ready, and could and should be used. Something that's changing in this release is that <a href="https://github.com/kubernetes/enhancements/issues/3136">beta APIs are going to be off by default</a>. Why that change?</strong></p> <p>JAMES LAVERACK: This is really about encouraging the use of stable APIs. There was a perception, like you say, that beta APIs were actually stable. Because they can be removed very quickly, we often ended up in the state where we wanted to follow the policy and remove a beta API, but were unable to, because it was de facto stable, according to the community. This meant that cluster operators and users had a lot of breaking changes when doing upgrades that could have been avoided. This is really just to help stability as we go through more upgrades in the future.</p> <p><strong>CRAIG BOX: I understand that only applies now to new APIs. Things that are in beta at the moment will continue to be available. So there'll be no breaking changes again?</strong></p> <p>JAMES LAVERACK: That's correct. There's no breaking changes in beta APIs other than the ones we've documented this release. It's only new things.</p> <p><strong>CRAIG BOX: Now in this release, <a href="https://github.com/kubernetes/enhancements/issues/3031">the artifacts are signed</a> using Cosign signatures, and there is <a href="https://kubernetes.io/docs/tasks/administer-cluster/verify-signed-images/">experimental support for verification of those signatures</a>. What needed to happen to make that process possible?</strong></p> <p>JAMES LAVERACK: This was a huge process from the other half of SIG Release. SIG Release has the release team, but it also has the release engineering team that handles the mechanics of actually pushing releases out. They have spent, and one of my friends over there, Adolfo, has spent a lot of time trying to bring us in line with <a href="https://slsa.dev/">SLSA</a> compliance. I believe we're <a href="https://github.com/kubernetes/enhancements/issues/3027">looking now at Level 3 compliance</a>.</p> <p>SLSA is a framework that describes software supply chain security. That is, of course, a really big issue in our industry at the moment. And it's really good to see the project adopting the best practices for this.</p> <p><strong>CRAIG BOX: I was looking back at <a href="https://kubernetespodcast.com/episode/167-kubernetes-1.23/">the conversation I had with Rey Lejano about the 1.23 release</a>, and we were basically approaching Level 2. We're now obviously stepping up to Level 3. I think I asked Rey at the time was, is it fair to say that SLSA is inspired by large projects like Kubernetes, and in theory, it should be really easy for these projects to tick the boxes to get to that level, because the SLSA framework is written with a project like Kubernetes in mind?</strong></p> <p>JAMES LAVERACK: I think so. I think it's been somewhat difficult, just because it's one thing to do it, but it's another thing to prove that you're doing it, which is the whole point around these frameworks — the assertation, that proof.</p> <p><strong>CRAIG BOX: As an end user of Kubernetes, whether I install it myself or I take it from a service like GKE, what will this provenance then let me prove? If we think back to <a href="https://kubernetespodcast.com/episode/174-in-toto/">the orange juice example we talked to Santiago about recently</a>, how do I tell that my software is safe to run?</strong></p> <p>JAMES LAVERACK: If you're downloading and running Kubernetes yourself, you can use the verifying image signatures feature to verify the thing you've downloaded, and the thing you are running, is actually the thing that the Kubernetes project has released, and that it has been built from the actual source code in the Kubernetes GitHub repository. This can give you a lot of confidence in what you're running, especially if you're running in a highly secure or regulated environment of some kind.</p> <p>As an end user, this isn't something that will necessarily directly impact you. But it means that service providers that provide managed Kubernetes options, such as Google and GKE, can provide even greater levels of security and safety themselves about the services that they run.</p> <p><strong>CRAIG BOX: A lot of people get access to their Kubernetes server just by being granted an API endpoint, and they start running kubectl against it. They're not actually installing their own Kubernetes. They have a provider or a platform team do it for them. Do you think it's feasible to get to a world where there's something that you can run when you're deploying your workloads which queries the API server, for example, and gets access to that same provenance data?</strong></p> <p>JAMES LAVERACK: I think it's going to be very difficult to do it that way, simply because this provenance and assertation data implies that you actually have access to the underlying executables, which typically, when you're running in a managed platform, you don't. If you're having Kubernetes provided to you, I think you're still going to have to trust the platform team or the organization that's providing it to you.</p> <p><strong>CRAIG BOX: Just like when you go to the hotel breakfast bar, you have to trust that they've been good with their orange juice.</strong></p> <p>JAMES LAVERACK: Yeah, I think the orange juice example is great. If you're making it yourself, then you can use assertation. If you're not, if you've just been given a glass, then you're going to have to trust who's pouring it.</p> <p><strong>CRAIG BOX: Continuing with our exploration of new stable features, <a href="https://github.com/kubernetes/enhancements/issues/1472">storage capacity tracking</a> and <a href="https://github.com/kubernetes/enhancements/issues/284">volume expansion</a> are generally available. What do those features enable me to do?</strong></p> <p>JAMES LAVERACK: This is a really great set of stable features coming out of SIG Storage. Storage capacity tracking allows applications on Kubernetes to use the Kubernetes API to understand how much storage is available, which can drive application decisions. With volume expansion, that again allows an application to use the Kubernetes API to request additional storage, which can enable applications to make all kinds of operational decisions.</p> <p><strong>CRAIG BOX: SIG Storage are also working through <a href="https://github.com/kubernetes/enhancements/issues/625">a project to migrate all of their in-tree storage plugins out to CSI plugins</a>. How are they going with that process?</strong></p> <p>JAMES LAVERACK: In 1.24 we have a couple of them that have been migrated out. The <a href="https://github.com/kubernetes/enhancements/issues/1490">Azure Disk</a> and <a href="https://github.com/kubernetes/enhancements/issues/1489">OpenStack Cinder</a> plugins have both been migrated. They're maintaining the original API, but the actual implementation now happens in those CSI plugins.</p> <p><strong>CRAIG BOX: Do they have a long way to go, or are they just cutting off a couple every release?</strong></p> <p>JAMES LAVERACK: They're just doing a couple every release from what I see. There are a couple of others to go. This is really part of a larger theme within Kubernetes, which is pushing application-specific things out behind interfaces, such as the container storage interface and the container runtime interface.</p> <p><strong>CRAIG BOX: That obviously sets up a situation where you have a stable interface and you can have beta implementations of that that are outside of Kubernetes and get around the problem we talked about before with not being able to run beta things.</strong></p> <p>JAMES LAVERACK: Yeah, exactly. It also makes it easy to expand Kubernetes. You don't have to try to get code in-tree in order to implement a new storage engine, for example.</p> <p><strong>CRAIG BOX: <a href="https://github.com/kubernetes/enhancements/issues/2727">gRPC probes have graduated to beta in 1.24</a>. What does that functionality provide?</strong></p> <p>JAMES LAVERACK: This is one of the changes that's going to be most visible to application developers in Kubernetes, I think. Until now, Kubernetes has had the ability to do readiness and liveness checks on containers and be able to make intelligent routing and pod restart decisions based on those. But those checks had to be HTTP REST endpoints.</p> <p>With Kubernetes 1.24, we're enabling a beta feature that allows them to use gRPC. This means that if you're building an application that is primarily gRPC-based, as many microservices applications are, you can now use that same technology in order to implement your probes without having to bundle an HTTP server as well.</p> <p><strong>CRAIG BOX: Are there any other enhancements that are particularly notable or relevant perhaps to the work you've been doing?</strong></p> <p>JAMES LAVERACK: There's a really interesting one from SIG Network which is about <a href="https://kubernetes.io/blog/2022/05/03/kubernetes-1-24-release-announcement/#avoiding-collisions-in-ip-allocation-to-services">avoiding collisions in IP allocations to services</a>. In existing versions of Kubernetes, you can allocate a service to have a particular internal cluster IP, or you can leave it blank and it will generate its own IP.</p> <p>In Kubernetes 1.24, there's an opt-in feature, which allows you to specify a pool for dynamic IPs to be generated from. This means that you can statically allocate an IP to a service and know that IP can not be accidentally dynamically allocated. This is a problem I've actually had in my local Kubernetes cluster, where I use static IP addresses for a bunch of port forwarding rules. I've always worried that during server start-up, they're going to get dynamically allocated to one of the other services. Now, with 1.24, and this feature, I won't have to worry about it more.</p> <p><strong>CRAIG BOX: This is like the analog of allocating an IP in your DHCP server rather than just claiming it statically on your local machine?</strong></p> <p>JAMES LAVERACK: Pretty much. It means that you can't accidentally double allocate something.</p> <p><strong>CRAIG BOX: Why don't we all just use IPv6?</strong></p> <p>JAMES LAVERACK: That is a very deep question I don't think we have time for.</p> <p><strong>CRAIG BOX: The margins of this podcast would be unable to contain it even if we did.</strong></p> <p>JAMES LAVERACK: [LAUGHING]</p> <p><strong>CRAIG BOX: <a href="https://kubernetes.io/blog/2022/05/03/kubernetes-1-24-release-announcement/#release-theme-and-logo">The theme for Kubernetes 1.24 is Stargazer</a>. How did you pick that as the theme?</strong></p> <p>JAMES LAVERACK: Every release lead gets to pick their theme, pretty much by themselves. When I started, I asked Rey, the previous release lead, how he picked his theme, because he picked the Next Frontier for Kubernetes 1.23. And he told me that he'd actually picked it before the release even started, which meant for the first couple of weeks and months of the release, I was really worried about it, because I hadn't picked one yet, and I wasn't sure what to pick.</p> <p>Then again, I was speaking to another former release lead, and they told me that they picked theirs like two weeks out. It seems to really vary. About halfway through the release, I had some ideas down. I thought maybe we could talk about — I live in a city called Bristol in the UK, which has a very famous bridge — and I thought, oh, we could talk about bridges and architectural and a metaphor for community bridging gaps and things like this. I kind of liked the idea, but it didn't really grab me.</p> <p>One thing about me is that I am a serious night owl. I cannot work effectively in the mornings. I've always enjoyed the night. And that got me thinking about astronomy and the stars. I think one night I was trying to get to sleep, because I couldn't sleep, and I was watching <a href="https://www.youtube.com/channel/UC7_gcs09iThXybpVgjHZ_7g">PBS Space Time</a>, which is this fantastic YouTube channel talking about physics. And I'm not a physicist. I don't understand any of the maths. But I find it really interesting as a topic.</p> <p>I just thought, well, why don't I make a theme about stars. Kubernetes has often had a space theme in many releases. As I'm sure you're aware, its original name was based off of Star Trek. The previous release had a Star Trek-based theme. I thought, well, let's do that. So I came up with the idea of Stargazer.</p> <p><strong>CRAIG BOX: Once you have a theme, you then need a release logo. I understand you have a household artist?</strong></p> <p>JAMES LAVERACK: [LAUGHS] I don't think she'd appreciate being called that, but, yes. My wife is an artist, and in particular, a digital artist. I had a bit of a conversation with the SIG Release folks to see if they'd be comfortable with my wife doing it, and they said they'd be completely fine with that.</p> <p>I asked if she would be willing to spend some time creating a logo for us. And thankfully for me, she was. She has produced this — well, I'm somewhat obliged to say — she produced us a beautiful logo, which you can see in our release blog and probably around social media. It is a telescope set over starry skies, and I absolutely love it.</p> <p><strong>CRAIG BOX: It is objectively very nice. It obviously has the seven stars or the Seven Sisters of the Pleiades. Do the colors have any particular meaning?</strong></p> <p>JAMES LAVERACK: The colors are based on the Kubernetes blue. If you look in the background, that haze is actually in the shape of a Kubernetes wheel from the original Kubernetes logo.</p> <p><strong>CRAIG BOX: You must have to squint at it the right way. Very abstract. As is the wont of art.</strong></p> <p>JAMES LAVERACK: As is the wont.</p> <p><strong>CRAIG BOX: You mentioned before Rey Lejano, the 1.23 release lead. We ask every interview what the person learned from the last lead and what they're going to put in the proverbial envelope for the next. At the time, Rey said that he would encourage you to use teachable moments in the release team meetings. Was that something you were able to do?</strong></p> <p>JAMES LAVERACK: Not as much as I would have liked. I think the thing that I really took from Rey was communicate more. I've made a big effort this time to put as much communication in the open as possible. I was actually worried that I was going to be spamming the SIG Release Slack channel too much. I asked our SIG Release chairs Stephen and Sasha about it. And they said, just don't worry about it. Just spam as much as you want.</p> <p>And so I think the majority of the conversation in SIG Release Slack over the past few months has just been me. [LAUGHING] That seemed to work out pretty well.</p> <p><strong>CRAIG BOX: That's what it's for.</strong></p> <p>JAMES LAVERACK: It is what it's for. But SIG Release does more than just the individual release process, of course. It's release engineering, too.</p> <p><strong>CRAIG BOX: I'm sure they'd be interested in what's going on anyway?</strong></p> <p>JAMES LAVERACK: It's true. It's true. It's been really nice to be able to talk to everyone that way, I think.</p> <p><strong>CRAIG BOX: We talked before about your introduction to Kubernetes being at a KubeCon, and meeting people in person. How has it been running the release almost entirely virtually?</strong></p> <p>JAMES LAVERACK: It's not been so bad. The release team has always been geographically distributed, somewhat by design. It's always been a very virtual engagement, so I don't think it's been impacted too, too much by the pandemic and travel restrictions. Of course, I'm looking forward to KubeCon Valencia and being able to see everyone again. But I think the release team has handled excellently in the current situation.</p> <p><strong>CRAIG BOX: What is the advice that you will pass on to <a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.25/release-team.md">the next release lead</a>, which has been announced to be Cici Huang from Google?</strong></p> <p>JAMES LAVERACK: I would say to Cici that open communication is really important. I made a habit of posting every single week in SIG Release a summary of what's happened. I'm super-glad that I did that, and I'm going to encourage her to do the same if she wants to.</p> <p><strong>CRAIG BOX: This release was originally due out two weeks earlier, but <a href="https://groups.google.com/a/kubernetes.io/g/dev/c/9IZaUGVMnmo">it was delayed</a>. What happened?</strong></p> <p>JAMES LAVERACK: That delay was the result of a release-blocking bug — an absolute showstopper. This was in the underlying Go implementation of TLS certificate verification. It meant that a lot of clients simply would not be able to connect to clusters or anything else. So we took the decision that we can't release with a bug this big. Thus the term release-blocking.</p> <p>The fix had to be merged upstream in Go 1.18.1, and then we had to, of course, rebuild and release release candidates. Given the time we like to have things to sit and stabilize after we make a lot of changes like that, we felt it was more prudent to push out the release by a couple of weeks than risk shipping a broken point-zero.</p> <p><strong>CRAIG BOX: Go 1.18 is itself quite new. How does the project decide how quickly to upgrade its underlying programming language?</strong></p> <p>JAMES LAVERACK: A lot of it is driven by support requirements. We support each release for three releases. So Kubernetes 1.24 will be most likely in support until this time next year, in 2023, as we do three releases per year. That means that right up until May, 2023, we're probably going to be shipping updates for Kubernetes 1.24, which means that the version of Go we're using, and other dependencies, have to be supported as well. My understanding is that the older version of Go, Go 1.17, just wouldn't be supported long enough.</p> <p>Any underlying critical bug fixes that were coming in, they wouldn't have been back ported to Go 1.17, and therefore we might not be able to adequately support Kubernetes 1.24.</p> <p><strong>CRAIG BOX: A side effect of the unfortunate delay was an unfortunate holiday situation, where you were booked to take the week after the release off and instead you ended up taking the week before the release off. Were you able to actually have any holiday and relax in that situation?</strong></p> <p>JAMES LAVERACK: Well, I didn't go anywhere, if that's what you're asking.</p> <p><strong>CRAIG BOX: No one ever does. This is what the pandemic's been, staycations.</strong></p> <p>JAMES LAVERACK: Yeah, staycations. It's been interesting. On the one hand, I've done a lot of Kubernetes work in that time. So you could argue it's not really been a holiday. On the other hand, my highly annoying friends have gotten me into playing an MMO, so I've been spending a lot of time playing that.</p> <p><strong>CRAIG BOX: I hear also you have a new vacuum cleaner?</strong></p> <p>JAMES LAVERACK: [LAUGHS] You've been following my Twitter. Yes, I couldn't find the charging cord for my old vacuum cleaner. And so I decided just to buy a new one. I decided, at long last, just to buy one of the nice brand-name ones. And it is just better.</p> <p><strong>CRAIG BOX: This isn't the BBC. You're allowed to name it if you want.</strong></p> <p>JAMES LAVERACK: Yes, we went and bought one of these nice Dyson vacuum cleaners, and the first time I've gotten one so expensive. On the one hand, I feel a little bit bad spending a lot of money on a vacuum cleaner. On the other hand, it's so much easier.</p> <p><strong>CRAIG BOX: Is it one of those handheld ones, like a giant Dust-Buster with a long leg?</strong></p> <p>JAMES LAVERACK: No, I got one of the corded floor ones, because the problem was, of course, I lost the charger for the last one, so I didn't want that to happen again. So I got a wall plug-in one.</p> <p><strong>CRAIG BOX: I must say, going from a standard <a href="https://www.myhenry.com/">Henry Hoover</a> to — the place we're staying at the moment has what I'll call a knock-off Dyson portable vacuum cleaner — having something that you can just pick up and carry around with you, and not have to worry about the cord, actually does encourage me to keep the place tidier.</strong></p> <p>JAMES LAVERACK: Really? I think our last one was corded, but it didn't encourage us to use it anymore, just because it was so useless.</p> <hr> <p><em><a href="https://twitter.com/jameslaverack">James Laverack</a> is a Staff Solutions Engineer at Jetstack, and was the release team lead for Kubernetes 1.24.</em></p> <p><em>You can find the <a href="http://www.kubernetespodcast.com/">Kubernetes Podcast from Google</a> at <a href="https://twitter.com/KubernetesPod">@KubernetesPod</a> on Twitter, and you can <a href="https://kubernetespodcast.com/subscribe/">subscribe</a> so you never miss an episode.</em></p></description></item><item><title>Blog: Meet Our Contributors - APAC (China region)</title><link>https://kubernetes.io/blog/2022/08/15/meet-our-contributors-china-ep-03/</link><pubDate>Mon, 15 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/08/15/meet-our-contributors-china-ep-03/</guid><description> <p><strong>Authors &amp; Interviewers:</strong> <a href="https://github.com/AvineshTripathi">Avinesh Tripathi</a>, <a href="https://github.com/Debanitrkl">Debabrata Panigrahi</a>, <a href="https://github.com/jayesh-srivastava">Jayesh Srivastava</a>, <a href="https://github.com/Priyankasaggu11929/">Priyanka Saggu</a>, <a href="https://github.com/PurneswarPrasad">Purneswar Prasad</a>, <a href="https://github.com/vedant-kakde">Vedant Kakde</a></p> <hr> <p>Hello, everyone 👋</p> <p>Welcome back to the third edition of the &quot;Meet Our Contributors&quot; blog post series for APAC.</p> <p>This post features four outstanding contributors from China, who have played diverse leadership and community roles in the upstream Kubernetes project.</p> <p>So, without further ado, let's get straight to the article.</p> <h2 id="andy-zhang-https-github-com-andyzhangx"><a href="https://github.com/andyzhangx">Andy Zhang</a></h2> <p>Andy Zhang currently works for Microsoft China at the Shanghai site. His main focus is on Kubernetes storage drivers. Andy started contributing to Kubernetes about 5 years ago.</p> <p>He states that as he is working in Azure Kubernetes Service team and spends most of his time contributing to the Kubernetes community project. Now he is the main contributor of quite a lot Kubernetes subprojects such as Kubernetes cloud provider code.</p> <p>His open source contributions are mainly self-motivated. In the last two years he has mentored a few students contributing to Kubernetes through the LFX Mentorship program, some of whom got jobs due to their expertise and contributions on Kubernetes projects.</p> <p>Andy is an active member of the China Kubernetes community. He adds that the Kubernetes community has a good guide about how to become members, code reviewers, approvers and finally when he found out that some open source projects are in the very early stage, he actively contributed to those projects and became the project maintainer.</p> <h2 id="shiming-zhang-https-github-com-wzshiming"><a href="https://github.com/wzshiming">Shiming Zhang</a></h2> <p>Shiming Zhang is a Software Engineer working on Kubernetes for DaoCloud in Shanghai, China.</p> <p>He has mostly been involved with SIG Node as a reviewer. His major contributions have mainly been bug fixes and feature improvements in an ongoing <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2712-pod-priority-based-graceful-node-shutdown">KEP</a>, all revolving around SIG Node.</p> <p>Some of his major PRs are <a href="https://github.com/kubernetes/kubernetes/pull/100326">fixing watchForLockfileContention memory leak</a>, <a href="https://github.com/kubernetes/kubernetes/pull/101093">fixing startupProbe behaviour</a>, <a href="https://github.com/kubernetes/enhancements/pull/2661">adding Field status.hostIPs for Pod</a>.</p> <h2 id="paco-xu-https-github-com-pacoxu"><a href="https://github.com/pacoxu">Paco Xu</a></h2> <p>Paco Xu works at DaoCloud, a Shanghai-based cloud-native firm. He works with the infra and the open source team, focusing on enterprise cloud native platforms based on Kubernetes.</p> <p>He started with Kubernetes in early 2017 and his first contribution was in March 2018. He started with a bug that he found, but his solution was not that graceful, hence wasn't accepted. He then started with some good first issues, which helped him to a great extent. In addition to this, from 2016 to 2017, he made some minor contributions to Docker.</p> <p>Currently, Paco is a reviewer for <code>kubeadm</code> (a SIG Cluster Lifecycle product), and for SIG Node.</p> <p>Paco says that you should contribute to open source projects you use. For him, an open source project is like a book to learn, getting inspired through discussions with the project maintainers.</p> <blockquote> <p>In my opinion, the best way for me is learning how owners work on the project.</p> </blockquote> <h2 id="jintao-zhang-https-github-com-tao12345666333"><a href="https://github.com/tao12345666333">Jintao Zhang</a></h2> <p>Jintao Zhang is presently employed at API7, where he focuses on ingress and service mesh.</p> <p>In 2017, he encountered an issue which led to a community discussion and his contributions to Kubernetes started. Before contributing to Kubernetes, Jintao was a long-time contributor to Docker-related open source projects.</p> <p>Currently Jintao is a maintainer for the <a href="https://kubernetes.github.io/ingress-nginx/">ingress-nginx</a> project.</p> <p>He suggests keeping track of job opportunities at open source companies so that you can find one that allows you to contribute full time. For new contributors Jintao says that if anyone wants to make a significant contribution to an open source project, then they should choose the project based on their interests and should generously invest time.</p> <hr> <p>If you have any recommendations/suggestions for who we should interview next, please let us know in the <a href="https://kubernetes.slack.com/archives/C1TU9EB9S">#sig-contribex channel</a> channel on the Kubernetes Slack. Your suggestions would be much appreciated. We're thrilled to have additional folks assisting us in reaching out to even more wonderful individuals of the community.</p> <p>We'll see you all in the next one. Everyone, till then, have a happy contributing! 👋</p></description></item><item><title>Blog: Enhancing Kubernetes one KEP at a Time</title><link>https://kubernetes.io/blog/2022/08/11/enhancing-kubernetes-one-kep-at-a-time/</link><pubDate>Thu, 11 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/08/11/enhancing-kubernetes-one-kep-at-a-time/</guid><description> <p><strong>Author:</strong> Ryler Hockenbury (Mastercard)</p> <p>Did you know that Kubernetes v1.24 has <a href="https://kubernetes.io/blog/2022/05/03/kubernetes-1-24-release-announcement/">46 enhancements</a>? That's a lot of new functionality packed into a 4-month release cycle. The Kubernetes release team coordinates the logistics of the release, from remediating test flakes to publishing updated docs. It's a ton of work, but they always deliver.</p> <p>The release team comprises around 30 people across six subteams - Bug Triage, CI Signal, Enhancements, Release Notes, Communications, and Docs.  Each of these subteams manages a component of the release. This post will focus on the role of the enhancements subteam and how you can get involved.</p> <h2 id="what-s-the-enhancements-subteam">What's the enhancements subteam?</h2> <p>Great question. We'll get to that in a second but first, let's talk about how features are managed in Kubernetes.</p> <p>Each new feature requires a <a href="https://github.com/kubernetes/enhancements/blob/master/keps/README.md">Kubernetes Enhancement Proposal</a> - KEP for short. KEPs are small structured design documents that provide a way to propose and coordinate new features. The KEP author describes the motivation, design (and alternatives), risks, and tests - then community members provide feedback to build consensus.</p> <p>KEPs are submitted and updated through a pull request (PR) workflow on the <a href="https://github.com/kubernetes/enhancements">k/enhancements repo</a>. Features start in alpha and move through a graduation process to beta and stable as they mature. For example, here's a cool KEP about <a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-windows/1981-windows-privileged-container-support/kep.yaml">privileged container support on Windows Server</a>.  It was introduced as alpha in Kubernetes v1.22 and graduated to beta in v1.23.</p> <p>Now getting back to the question - the enhancements subteam coordinates the lifecycle tracking of the KEPs for each release. Each KEP is required to meet a set of requirements to be cleared for inclusion in a release. The enhancements subteam verifies each requirement for each KEP and tracks the status.</p> <p>At the start of a release, <a href="https://github.com/kubernetes/community/blob/master/sig-list.md">Kubernetes Special Interest Groups</a> (SIGs) submit their enhancements to opt into a release. A typical release might have from 60 to 90 enhancements at the beginning.  During the release, many enhancements will drop out. Some do not quite meet the KEP requirements, and others do not complete their implementation in code. About 60%-70% of the opted-in KEPs will make it into the final release.</p> <h2 id="what-does-the-enhancements-subteam-do">What does the enhancements subteam do?</h2> <p>Another great question, keep them coming! The enhancements team is involved in two crucial milestones during each release: enhancements freeze and code freeze.</p> <h4 id="enhancements-freeze">Enhancements Freeze</h4> <p>Enhancements freeze is the deadline for a KEP to be complete in order for the enhancement to be included in a release. It's a quality gate to enforce alignment around maintaining and updating KEPs. The most notable requirements are a (1) <a href="https://github.com/kubernetes/community/blob/master/sig-architecture/production-readiness.md">production readiness review </a>(PRR) and a (2) <a href="https://github.com/kubernetes/enhancements/tree/master/keps/NNNN-kep-template">KEP file</a> with a complete test plan and graduation criteria.</p> <p>The enhancements subteam communicates to each KEP author through comments on the KEP issue on Github. As a first step, they'll verify the status and check if it meets the requirements.  The KEP gets marked as tracked after satisfying the requirements; otherwise, it's considered at risk. If a KEP is still at risk when enhancement freeze is in effect, the KEP is removed from the release.</p> <p>This part of the cycle is typically the busiest for the enhancements subteam because of the large number of KEPs to groom, and each KEP might need to be visited multiple times to verify whether it meets requirements.</p> <h4 id="code-freeze">Code Freeze</h4> <p>Code freeze is the implementation deadline for all enhancements. The code must be implemented, reviewed, and merged by this point if a code change or update is needed for the enhancement. The latter third of the release is focused on stabilizing the codebase - fixing flaky tests, resolving various regressions, and preparing docs - and all the code needs to be in place before those steps can happen.</p> <p>The enhancements subteam verifies that all PRs for an enhancement are merged into the <a href="https://github.com/kubernetes/kubernetes">Kubernetes codebase</a> (k/k). During this period, the subteam reaches out to KEP authors to understand what PRs are part of the KEP, verifies that those PRs get merged, and then updates the status of the KEP. The enhancement is removed from the release if the code isn't all merged before the code freeze deadline.</p> <h2 id="how-can-i-get-involved-with-the-release-team">How can I get involved with the release team?</h2> <p>I'm glad you asked. The most direct way is to apply to be a <a href="https://github.com/kubernetes/sig-release/blob/master/release-team/shadows.md">release team shadow</a>. The shadow role is a hands-on apprenticeship intended to prepare individuals for leadership positions on the release team. Many shadow roles are non-technical and do not require prior contributions to the Kubernetes codebase.</p> <p>With 3 Kubernetes releases every year and roughly 25 shadows per release, the release team is always in need of individuals wanting to contribute. Before each release cycle, the release team opens the application for the shadow program. When the application goes live, it's posted in the <a href="https://groups.google.com/a/kubernetes.io/g/dev">Kubernetes Dev Mailing List</a>.  You can subscribe to notifications from that list (or check it regularly!) to watch when the application opens. The announcement will typically go out in mid-April, mid-July, and mid-December - or roughly a month before the start of each release.</p> <h2 id="how-can-i-find-out-more">How can I find out more?</h2> <p>Check out the <a href="https://github.com/kubernetes/sig-release/tree/master/release-team/role-handbooks">role handbooks</a> if you're curious about the specifics of all the Kubernetes release subteams. The handbooks capture the logistics of each subteam, including a week-by-week breakdown of the subteam activities.  It's an excellent reference for getting to know each team better.</p> <p>You can also check out the release-related Kubernetes slack channels - particularly #release, #sig-release, and #sig-arch. These channels have discussions and updates surrounding many aspects of the release.</p></description></item><item><title>Blog: Kubernetes Removals and Major Changes In 1.25</title><link>https://kubernetes.io/blog/2022/08/04/upcoming-changes-in-kubernetes-1-25/</link><pubDate>Thu, 04 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/08/04/upcoming-changes-in-kubernetes-1-25/</guid><description> <p><strong>Authors</strong>: Kat Cosgrove, Frederico Muñoz, Debabrata Panigrahi</p> <p>As Kubernetes grows and matures, features may be deprecated, removed, or replaced with improvements for the health of the project. Kubernetes v1.25 includes several major changes and one major removal.</p> <h2 id="the-kubernetes-api-removal-and-deprecation-process">The Kubernetes API Removal and Deprecation process</h2> <p>The Kubernetes project has a well-documented <a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/">deprecation policy</a> for features. This policy states that stable APIs may only be deprecated when a newer, stable version of that same API is available and that APIs have a minimum lifetime for each stability level. A deprecated API is one that has been marked for removal in a future Kubernetes release; it will continue to function until removal (at least one year from the deprecation), but usage will result in a warning being displayed. Removed APIs are no longer available in the current version, at which point you must migrate to using the replacement.</p> <ul> <li>Generally available (GA) or stable API versions may be marked as deprecated but must not be removed within a major version of Kubernetes.</li> <li>Beta or pre-release API versions must be supported for 3 releases after deprecation.</li> <li>Alpha or experimental API versions may be removed in any release without prior deprecation notice.</li> </ul> <p>Whether an API is removed as a result of a feature graduating from beta to stable or because that API simply did not succeed, all removals comply with this deprecation policy. Whenever an API is removed, migration options are communicated in the documentation.</p> <h2 id="podsecuritypolicy-removal">A note about PodSecurityPolicy</h2> <p>In Kubernetes v1.25, we will be removing PodSecurityPolicy <a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">after its deprecation in v1.21</a>. PodSecurityPolicy has served us honorably, but its complex and often confusing usage necessitated changes, which unfortunately would have been breaking changes. To address this, it is being removed in favor of a replacement, Pod Security Admission, which is graduating to stable in this release as well. If you are currently relying on PodSecurityPolicy, follow the instructions for <a href="https://kubernetes.io/docs/tasks/configure-pod-container/migrate-from-psp/">migration to Pod Security Admission</a>.</p> <h2 id="major-changes-for-kubernetes-v1-25">Major Changes for Kubernetes v1.25</h2> <p>Kubernetes v1.25 will include several major changes, in addition to the removal of PodSecurityPolicy.</p> <h3 id="csi-migration-https-github-com-kubernetes-enhancements-issues-625"><a href="https://github.com/kubernetes/enhancements/issues/625">CSI Migration</a></h3> <p>The effort to move the in-tree volume plugins to out-of-tree CSI drivers continues, with the core CSI Migration feature going GA in v1.25. This is an important step towards removing the in-tree volume plugins entirely.</p> <h3 id="deprecations-and-removals-for-storage-drivers">Deprecations and removals for storage drivers</h3> <p>Several volume plugins are being deprecated or removed.</p> <p><a href="https://github.com/kubernetes/enhancements/issues/3446">GlusterFS will be deprecated in v1.25</a>. While a CSI driver was built for it, it has not been maintained. The possibility of migration to a compatible CSI driver <a href="https://github.com/kubernetes/kubernetes/issues/100897">was discussed</a>, but a decision was ultimately made to begin the deprecation of the GlusterFS plugin from in-tree drivers. The <a href="https://github.com/kubernetes/enhancements/issues/2589">Portworx in-tree volume plugin</a> is also being deprecated with this release. The Flocker, Quobyte, and StorageOS in-tree volume plugins are being removed.</p> <p><a href="https://github.com/kubernetes/kubernetes/pull/111618">Flocker</a>, <a href="https://github.com/kubernetes/kubernetes/pull/111619">Quobyte</a>, and <a href="https://github.com/kubernetes/kubernetes/pull/111620">StorageOS</a> in-tree volume plugins will be removed in v1.25 as part of the <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/625-csi-migration">CSI Migration</a>.</p> <h3 id="change-to-vsphere-version-support-https-github-com-kubernetes-kubernetes-pull-111255"><a href="https://github.com/kubernetes/kubernetes/pull/111255">Change to vSphere version support</a></h3> <p>From Kubernetes v1.25, the in-tree vSphere volume driver will not support any vSphere release before 7.0u2. Once Kubernetes v1.25 is released, check the v1.25 detailed release notes for more advice on how to handle this.</p> <h3 id="cleaning-up-iptables-chain-ownership-https-github-com-kubernetes-enhancements-issues-3178"><a href="https://github.com/kubernetes/enhancements/issues/3178">Cleaning up IPTables Chain Ownership</a></h3> <p>On Linux, Kubernetes (usually) creates iptables chains to ensure that network packets reach Although these chains and their names have been an internal implementation detail, some tooling has relied upon that behavior. will only support for internal Kubernetes use cases. Starting with v1.25, the Kubelet will gradually move towards not creating the following iptables chains in the <code>nat</code> table:</p> <ul> <li><code>KUBE-MARK-DROP</code></li> <li><code>KUBE-MARK-MASQ</code></li> <li><code>KUBE-POSTROUTING</code></li> </ul> <p>This change will be phased in via the <code>IPTablesCleanup</code> feature gate. Although this is not formally a deprecation, some end users have come to rely on specific internal behavior of <code>kube-proxy</code>. The Kubernetes project overall wants to make it clear that depending on these internal details is not supported, and that future implementations will change their behavior here.</p> <h2 id="looking-ahead">Looking ahead</h2> <p>The official <a href="https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-26">list of API removals planned for Kubernetes 1.26</a> is:</p> <ul> <li>The beta FlowSchema and PriorityLevelConfiguration APIs (flowcontrol.apiserver.k8s.io/v1beta1)</li> <li>The beta HorizontalPodAutoscaler API (autoscaling/v2beta2)</li> </ul> <h3 id="want-to-know-more">Want to know more?</h3> <p>Deprecations are announced in the Kubernetes release notes. You can see the announcements of pending deprecations in the release notes for:</p> <ul> <li><a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.21.md#deprecation">Kubernetes 1.21</a></li> <li><a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.22.md#deprecation">Kubernetes 1.22</a></li> <li><a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.23.md#deprecation">Kubernetes 1.23</a></li> <li><a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md#deprecation">Kubernetes 1.24</a></li> <li>We will formally announce the deprecations that come with <a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.25.md#deprecation">Kubernetes 1.25</a> as part of the CHANGELOG for that release.</li> </ul> <p>For information on the process of deprecation and removal, check out the official Kubernetes <a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/#deprecating-parts-of-the-api">deprecation policy</a> document.</p></description></item><item><title>Blog: Spotlight on SIG Docs</title><link>https://kubernetes.io/blog/2022/08/02/sig-docs-spotlight-2022/</link><pubDate>Tue, 02 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/08/02/sig-docs-spotlight-2022/</guid><description> <p><strong>Author:</strong> Purneswar Prasad</p> <h2 id="introduction">Introduction</h2> <p>The official documentation is the go-to source for any open source project. For Kubernetes, it's an ever-evolving Special Interest Group (SIG) with people constantly putting in their efforts to make details about the project easier to consume for new contributors and users. SIG Docs publishes the official documentation on <a href="https://kubernetes.io">kubernetes.io</a> which includes, but is not limited to, documentation of the core APIs, core architectural details, and CLI tools shipped with the Kubernetes release.</p> <p>To learn more about the work of SIG Docs and its future ahead in shaping the community, I have summarised my conversation with the co-chairs, <a href="https://twitter.com/Divya_Mohan02">Divya Mohan</a> (DM), <a href="https://twitter.com/reylejano">Rey Lejano</a> (RL) and Natali Vlatko (NV), who ran through the SIG's goals and how fellow contributors can help.</p> <h2 id="a-summary-of-the-conversation">A summary of the conversation</h2> <h3 id="could-you-tell-us-a-little-bit-about-what-sig-docs-does">Could you tell us a little bit about what SIG Docs does?</h3> <p>SIG Docs is the special interest group for documentation for the Kubernetes project on kubernetes.io, generating reference guides for the Kubernetes API, kubeadm and kubectl as well as maintaining the official website’s infrastructure and analytics. The remit of their work also extends to docs releases, translation of docs, improvement and adding new features to existing documentation, pushing and reviewing content for the official Kubernetes blog and engaging with the Release Team for each cycle to get docs and blogs reviewed.</p> <h3 id="there-are-2-subprojects-under-docs-blogs-and-localization-how-has-the-community-benefited-from-it-and-are-there-some-interesting-contributions-by-those-teams-you-want-to-highlight">There are 2 subprojects under Docs: blogs and localization. How has the community benefited from it and are there some interesting contributions by those teams you want to highlight?</h3> <p><strong>Blogs</strong>: This subproject highlights new or graduated Kubernetes enhancements, community reports, SIG updates or any relevant news to the Kubernetes community such as thought leadership, tutorials and project updates, such as the Dockershim removal and removal of PodSecurityPolicy, which is upcoming in the 1.25 release. Tim Bannister, one of the SIG Docs tech leads, does awesome work and is a major force when pushing contributions through to the docs and blogs.</p> <p><strong>Localization</strong>: With this subproject, the Kubernetes community has been able to achieve greater inclusivity and diversity among both users and contributors. This has also helped the project gain more contributors, especially students, since a couple of years ago. One of the major highlights and up-and-coming localizations are Hindi and Bengali. The efforts for Hindi localization are currently being spearheaded by students in India.</p> <p>In addition to that, there are two other subprojects: <a href="https://github.com/kubernetes-sigs/reference-docs">reference-docs</a> and the <a href="https://github.com/kubernetes/website">website</a>, which is built with Hugo and is an important ownership area.</p> <h3 id="dockershim-removal">Recently there has been a lot of buzz around the Kubernetes ecosystem as well as the industry regarding the removal of dockershim in the latest 1.24 release. How has SIG Docs helped the project to ensure a smooth change among the end-users?</h3> <p>Documenting the removal of Dockershim was a mammoth task, requiring the revamping of existing documentation and communicating to the various stakeholders regarding the deprecation efforts. It needed a community effort, so ahead of the 1.24 release, SIG Docs partnered with Docs and Comms verticals, the Release Lead from the Release Team, and also the CNCF to help put the word out. Weekly meetings and a GitHub project board were set up to track progress, review issues and approve PRs and keep the Kubernetes website updated. This has also helped new contributors know about the depreciation, so that if any good-first-issue pops up, they could chip in. A dedicated Slack channel was used to communicate meeting updates, invite feedback or to solicit help on outstanding issues and PRs. The weekly meeting also continued for a month after the 1.24 release to review related issues and fix them. A huge shoutout to <a href="https://twitter.com/celeste_horgan">Celeste Horgan</a>, who kept the ball rolling on this conversation throughout the deprecation process.</p> <h3 id="why-should-new-and-existing-contributors-consider-joining-this-sig">Why should new and existing contributors consider joining this SIG?</h3> <p>Kubernetes is a vast project and can be intimidating at first for a lot of folks to find a place to start. Any open source project is defined by its quality of documentation and SIG Docs aims to be a welcoming, helpful place for new contributors to get onboard. One gets the perks of working with the project docs as well as learning by reading it. They can also bring their own, new perspective to create and improve the documentation. In the long run if they stick to SIG Docs, they can rise up the ladder to be maintainers. This will help make a big project like Kubernetes easier to parse and navigate.</p> <h3 id="how-do-you-help-new-contributors-get-started-are-there-any-prerequisites-to-join">How do you help new contributors get started? Are there any prerequisites to join?</h3> <p>There are no such prerequisites to get started with contributing to Docs. But there is certainly a fantastic Contribution to Docs guide which is always kept as updated and relevant as possible and new contributors are urged to read it and keep it handy. Also, there are a lot of useful pins and bookmarks in the community Slack channel <a href="https://kubernetes.slack.com/archives/C1J0BPD2M">#sig-docs</a>. GitHub issues with the good-first-issue labels in the kubernetes/website repo is a great place to create your first PR. Now, SIG Docs has a monthly New Contributor Meet and Greet on the first Tuesday of the month with the first occupant of the New Contributor Ambassador role, <a href="https://twitter.com/RinkiyaKeDad">Arsh Sharma</a>. This has helped in making a more accessible point of contact within the SIG for new contributors.</p> <h3 id="any-sig-related-accomplishment-that-you-re-really-proud-of">Any SIG related accomplishment that you’re really proud of?</h3> <p><strong>DM &amp; RL</strong> : The formalization of the localization subproject in the last few months has been a big win for SIG Docs, given all the great work put in by contributors from different countries. Earlier the localization efforts didn’t have any streamlined process and focus was given to provide a structure by drafting a KEP over the past couple of months for localization to be formalized as a subproject, which is planned to be pushed through by the end of third quarter.</p> <p><strong>DM</strong> : Another area where there has been a lot of success is the New Contributor Ambassador role, which has helped in making a more accessible point of contact for the onboarding of new contributors into the project.</p> <p><strong>NV</strong> : For each release cycle, SIG Docs have to review release docs and feature blogs highlighting release updates within a short window. This is always a big effort for the docs and blogs reviewers.</p> <h3 id="is-there-something-exciting-coming-up-for-the-future-of-sig-docs-that-you-want-the-community-to-know">Is there something exciting coming up for the future of SIG Docs that you want the community to know?</h3> <p>SIG Docs is now looking forward to establishing a roadmap, having a steady pipeline of folks being able to push improvements to the documentation and streamlining community involvement in triaging issues and reviewing PRs being filed. To build one such contributor and reviewership base, a mentorship program is being set up to help current contributors become reviewers. This definitely is a space to watch out for more!</p> <h2 id="wrap-up">Wrap Up</h2> <p>SIG Docs hosted a <a href="https://www.youtube.com/watch?v=GDfcBF5et3Q">deep dive talk</a> during on KubeCon + CloudNativeCon North America 2021, covering their awesome SIG. They are very welcoming and have been the starting ground into Kubernetes for a lot of new folks who want to contribute to the project. Join the <a href="https://github.com/kubernetes/community/blob/master/sig-docs/README.md">SIG's meetings</a> to find out about the most recent research results, their plans for the forthcoming year, and how to get involved in the upstream Docs team as a contributor!</p></description></item><item><title>Blog: Kubernetes Gateway API Graduates to Beta</title><link>https://kubernetes.io/blog/2022/07/13/gateway-api-graduates-to-beta/</link><pubDate>Wed, 13 Jul 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/07/13/gateway-api-graduates-to-beta/</guid><description> <p><strong>Authors:</strong> Shane Utt (Kong), Rob Scott (Google), Nick Young (VMware), Jeff Apple (HashiCorp)</p> <p>We are excited to announce the v0.5.0 release of Gateway API. For the first time, several of our most important Gateway API resources are graduating to beta. Additionally, we are starting a new initiative to explore how Gateway API can be used for mesh and introducing new experimental concepts such as URL rewrites. We'll cover all of this and more below.</p> <h2 id="what-is-gateway-api">What is Gateway API?</h2> <p>Gateway API is a collection of resources centered around <a href="https://gateway-api.sigs.k8s.io/api-types/gateway/">Gateway</a> resources (which represent the underlying network gateways / proxy servers) to enable robust Kubernetes service networking through expressive, extensible and role-oriented interfaces that are implemented by many vendors and have broad industry support.</p> <p>Originally conceived as a successor to the well known <a href="https://kubernetes.io/docs/reference/kubernetes-api/service-resources/ingress-v1/">Ingress</a> API, the benefits of Gateway API include (but are not limited to) explicit support for many commonly used networking protocols (e.g. <code>HTTP</code>, <code>TLS</code>, <code>TCP</code>, <code>UDP</code>) as well as tightly integrated support for Transport Layer Security (TLS). The <code>Gateway</code> resource in particular enables implementations to manage the lifecycle of network gateways as a Kubernetes API.</p> <p>If you're an end-user interested in some of the benefits of Gateway API we invite you to jump in and find an implementation that suits you. At the time of this release there are over a dozen <a href="https://gateway-api.sigs.k8s.io/implementations/">implementations</a> for popular API gateways and service meshes and guides are available to start exploring quickly.</p> <h3 id="getting-started">Getting started</h3> <p>Gateway API is an official Kubernetes API like <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Ingress</a>. Gateway API represents a superset of Ingress functionality, enabling more advanced concepts. Similar to Ingress, there is no default implementation of Gateway API built into Kubernetes. Instead, there are many different <a href="https://gateway-api.sigs.k8s.io/implementations/">implementations</a> available, providing significant choice in terms of underlying technologies while providing a consistent and portable experience.</p> <p>Take a look at the <a href="https://gateway-api.sigs.k8s.io/concepts/api-overview/">API concepts documentation</a> and check out some of the <a href="https://gateway-api.sigs.k8s.io/guides/getting-started/">Guides</a> to start familiarizing yourself with the APIs and how they work. When you're ready for a practical application open the <a href="https://gateway-api.sigs.k8s.io/implementations/">implementations page</a> and select an implementation that belongs to an existing technology you may already be familiar with or the one your cluster provider uses as a default (if applicable). Gateway API is a <a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/">Custom Resource Definition (CRD)</a> based API so you'll need to <a href="https://gateway-api.sigs.k8s.io/guides/getting-started/#install-the-crds">install the CRDs</a> onto a cluster to use the API.</p> <p>If you're specifically interested in helping to contribute to Gateway API, we would love to have you! Please feel free to <a href="https://github.com/kubernetes-sigs/gateway-api/issues/new/choose">open a new issue</a> on the repository, or join in the <a href="https://github.com/kubernetes-sigs/gateway-api/discussions">discussions</a>. Also check out the <a href="https://gateway-api.sigs.k8s.io/contributing/community/">community page</a> which includes links to the Slack channel and community meetings.</p> <h2 id="release-highlights">Release highlights</h2> <h3 id="graduation-to-beta">Graduation to beta</h3> <p>The <code>v0.5.0</code> release is particularly historic because it marks the growth in maturity to a beta API version (<code>v1beta1</code>) release for some of the key APIs:</p> <ul> <li><a href="https://gateway-api.sigs.k8s.io/api-types/gatewayclass/">GatewayClass</a></li> <li><a href="https://gateway-api.sigs.k8s.io/api-types/gateway/">Gateway</a></li> <li><a href="https://gateway-api.sigs.k8s.io/api-types/httproute/">HTTPRoute</a></li> </ul> <p>This achievement was marked by the completion of several graduation criteria:</p> <ul> <li>API has been <a href="https://gateway-api.sigs.k8s.io/implementations/">widely implemented</a>.</li> <li>Conformance tests provide basic coverage for all resources and have multiple implementations passing tests.</li> <li>Most of the API surface is actively being used.</li> <li>Kubernetes SIG Network API reviewers have approved graduation to beta.</li> </ul> <p>For more information on Gateway API versioning, refer to the <a href="https://gateway-api.sigs.k8s.io/concepts/versioning/">official documentation</a>. To see what's in store for future releases check out the <a href="#next-steps">next steps</a> section.</p> <h3 id="release-channels">Release channels</h3> <p>This release introduces the <code>experimental</code> and <code>standard</code> <a href="https://gateway-api.sigs.k8s.io/concepts/versioning/#release-channels-eg-experimental-standard">release channels</a> which enable a better balance of maintaining stability while still enabling experimentation and iterative development.</p> <p>The <code>standard</code> release channel includes:</p> <ul> <li>resources that have graduated to beta</li> <li>fields that have graduated to standard (no longer considered experimental)</li> </ul> <p>The <code>experimental</code> release channel includes everything in the <code>standard</code> release channel, plus:</p> <ul> <li><code>alpha</code> API resources</li> <li>fields that are considered experimental and have not graduated to <code>standard</code> channel</li> </ul> <p>Release channels are used internally to enable iterative development with quick turnaround, and externally to indicate feature stability to implementors and end-users.</p> <p>For this release we've added the following experimental features:</p> <ul> <li><a href="https://gateway-api.sigs.k8s.io/geps/gep-957/">Routes can attach to Gateways by specifying port numbers</a></li> <li><a href="https://gateway-api.sigs.k8s.io/geps/gep-726/">URL rewrites and path redirects</a></li> </ul> <h3 id="other-improvements">Other improvements</h3> <p>For an exhaustive list of changes included in the <code>v0.5.0</code> release, please see the <a href="https://github.com/kubernetes-sigs/gateway-api/releases/tag/v0.5.0">v0.5.0 release notes</a>.</p> <h2 id="gateway-api-for-service-mesh-the-gamma-initiative">Gateway API for service mesh: the GAMMA Initiative</h2> <p>Some service mesh projects have <a href="https://gateway-api.sigs.k8s.io/implementations/">already implemented support for the Gateway API</a>. Significant overlap between the Service Mesh Interface (SMI) APIs and the Gateway API has <a href="https://github.com/servicemeshinterface/smi-spec/issues/249">inspired discussion in the SMI community</a> about possible integration.</p> <p>We are pleased to announce that the service mesh community, including representatives from Cilium Service Mesh, Consul, Istio, Kuma, Linkerd, NGINX Service Mesh and Open Service Mesh, is coming together to form the <a href="https://gateway-api.sigs.k8s.io/contributing/gamma/">GAMMA Initiative</a>, a dedicated workstream within the Gateway API subproject focused on Gateway API for Mesh Management and Administration.</p> <p>This group will deliver <a href="https://gateway-api.sigs.k8s.io/v1beta1/contributing/gep/">enhancement proposals</a> consisting of resources, additions, and modifications to the Gateway API specification for mesh and mesh-adjacent use-cases.</p> <p>This work has begun with <a href="https://docs.google.com/document/d/1T_DtMQoq2tccLAtJTpo3c0ohjm25vRS35MsestSL9QU/edit#heading=h.jt37re3yi6k5">an exploration of using Gateway API for service-to-service traffic</a> and will continue with enhancement in areas such as authentication and authorization policy.</p> <h2 id="next-steps">Next steps</h2> <p>As we continue to mature the API for production use cases, here are some of the highlights of what we'll be working on for the next Gateway API releases:</p> <ul> <li><a href="https://github.com/kubernetes-sigs/gateway-api/blob/master/site-src/geps/gep-1016.md">GRPCRoute</a> for <a href="https://grpc.io/">gRPC</a> traffic routing</li> <li><a href="https://github.com/kubernetes-sigs/gateway-api/pull/1085">Route delegation</a></li> <li>Layer 4 API maturity: Graduating <a href="https://github.com/kubernetes-sigs/gateway-api/blob/main/apis/v1alpha2/tcproute_types.go">TCPRoute</a>, <a href="https://github.com/kubernetes-sigs/gateway-api/blob/main/apis/v1alpha2/udproute_types.go">UDPRoute</a> and <a href="https://github.com/kubernetes-sigs/gateway-api/blob/main/apis/v1alpha2/tlsroute_types.go">TLSRoute</a> to beta</li> <li><a href="https://gateway-api.sigs.k8s.io/contributing/gamma/">GAMMA Initiative</a> - Gateway API for Service Mesh</li> </ul> <p>If there's something on this list you want to get involved in, or there's something not on this list that you want to advocate for to get on the roadmap please join us in the #sig-network-gateway-api channel on Kubernetes Slack or our weekly <a href="https://gateway-api.sigs.k8s.io/contributing/community/#meetings">community calls</a>.</p></description></item><item><title>Blog: Annual Report Summary 2021</title><link>https://kubernetes.io/blog/2022/06/01/annual-report-summary-2021/</link><pubDate>Wed, 01 Jun 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/06/01/annual-report-summary-2021/</guid><description> <p><strong>Author:</strong> Paris Pittman (Steering Committee)</p> <p>Last year, we published our first <a href="https://kubernetes.io/blog/2021/06/28/announcing-kubernetes-community-group-annual-reports/">Annual Report Summary</a> for 2020 and it's already time for our second edition!</p> <p><a href="https://www.cncf.io/reports/kubernetes-annual-report-2021/">2021 Annual Report Summary</a></p> <p>This summary reflects the work that has been done in 2021 and the initiatives on deck for the rest of 2022. Please forward to organizations and indidviduals participating in upstream activities, planning cloud native strategies, and/or those looking to help out. To find a specific community group's complete report, go to the <a href="https://github.com/kubernetes/community">kubernetes/community repo</a> under the groups folder. Example: <a href="https://github.com/kubernetes/community/blob/master/sig-api-machinery/annual-report-2021.md">sig-api-machinery/annual-report-2021.md</a></p> <p>You’ll see that this report summary is a growth area in itself. It takes us roughly 6 months to prepare and execute, which isn’t helpful or valuable to anyone as a fast moving project with short and long term needs. How can we make this better? Provide your feedback here: <a href="https://github.com/kubernetes/steering/issues/242">https://github.com/kubernetes/steering/issues/242</a></p> <p>Reference: <a href="https://github.com/kubernetes/community/blob/master/committee-steering/governance/annual-reports.md">Annual Report Documentation</a></p></description></item><item><title>Blog: Kubernetes 1.24: Maximum Unavailable Replicas for StatefulSet</title><link>https://kubernetes.io/blog/2022/05/27/maxunavailable-for-statefulset/</link><pubDate>Fri, 27 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/05/27/maxunavailable-for-statefulset/</guid><description> <p><strong>Author:</strong> Mayank Kumar (Salesforce)</p> <p>Kubernetes <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSets</a>, since their introduction in 1.5 and becoming stable in 1.9, have been widely used to run stateful applications. They provide stable pod identity, persistent per pod storage and ordered graceful deployment, scaling and rolling updates. You can think of StatefulSet as the atomic building block for running complex stateful applications. As the use of Kubernetes has grown, so has the number of scenarios requiring StatefulSets. Many of these scenarios, require faster rolling updates than the currently supported one-pod-at-a-time updates, in the case where you're using the <code>OrderedReady</code> Pod management policy for a StatefulSet.</p> <p>Here are some examples:</p> <ul> <li> <p>I am using a StatefulSet to orchestrate a multi-instance, cache based application where the size of the cache is large. The cache starts cold and requires some siginificant amount of time before the container can start. There could be more initial startup tasks that are required. A RollingUpdate on this StatefulSet would take a lot of time before the application is fully updated. If the StatefulSet supported updating more than one pod at a time, it would result in a much faster update.</p> </li> <li> <p>My stateful application is composed of leaders and followers or one writer and multiple readers. I have multiple readers or followers and my application can tolerate multiple pods going down at the same time. I want to update this application more than one pod at a time so that i get the new updates rolled out quickly, especially if the number of instances of my application are large. Note that my application still requires unique identity per pod.</p> </li> </ul> <p>In order to support such scenarios, Kubernetes 1.24 includes a new alpha feature to help. Before you can use the new feature you must enable the <code>MaxUnavailableStatefulSet</code> feature flag. Once you enable that, you can specify a new field called <code>maxUnavailable</code>, part of the <code>spec</code> for a StatefulSet. For example:</p> <pre tabindex="0"><code>apiVersion: apps/v1 kind: StatefulSet metadata: name: web namespace: default spec: podManagementPolicy: OrderedReady # you must set OrderedReady replicas: 5 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - image: k8s.gcr.io/nginx-slim:0.8 imagePullPolicy: IfNotPresent name: nginx updateStrategy: rollingUpdate: maxUnavailable: 2 # this is the new alpha field, whose default value is 1 partition: 0 type: RollingUpdate </code></pre><p>If you enable the new feature and you don't specify a value for <code>maxUnavailable</code> in a StatefulSet, Kubernetes applies a default <code>maxUnavailable: 1</code>. This matches the behavior you would see if you don't enable the new feature.</p> <p>I'll run through a scenario based on that example manifest to demonstrate how this feature works. I will deploy a StatefulSet that has 5 replicas, with <code>maxUnavailable</code> set to 2 and <code>partition</code> set to 0.</p> <p>I can trigger a rolling update by changing the image to <code>k8s.gcr.io/nginx-slim:0.9</code>. Once I initiate the rolling update, I can watch the pods update 2 at a time as the current value of maxUnavailable is 2. The below output shows a span of time and is not complete. The maxUnavailable can be an absolute number (for example, 2) or a percentage of desired Pods (for example, 10%). The absolute number is calculated from percentage by rounding down.</p> <pre tabindex="0"><code>kubectl get pods --watch </code></pre><pre tabindex="0"><code>NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 85s web-1 1/1 Running 0 2m6s web-2 1/1 Running 0 106s web-3 1/1 Running 0 2m47s web-4 1/1 Running 0 2m27s web-4 1/1 Terminating 0 5m43s ----&gt; start terminating 4 web-3 1/1 Terminating 0 6m3s ----&gt; start terminating 3 web-3 0/1 Terminating 0 6m7s web-3 0/1 Pending 0 0s web-3 0/1 Pending 0 0s web-4 0/1 Terminating 0 5m48s web-4 0/1 Terminating 0 5m48s web-3 0/1 ContainerCreating 0 2s web-3 1/1 Running 0 2s web-4 0/1 Pending 0 0s web-4 0/1 Pending 0 0s web-4 0/1 ContainerCreating 0 0s web-4 1/1 Running 0 1s web-2 1/1 Terminating 0 5m46s ----&gt; start terminating 2 (only after both 4 and 3 are running) web-1 1/1 Terminating 0 6m6s ----&gt; start terminating 1 web-2 0/1 Terminating 0 5m47s web-1 0/1 Terminating 0 6m7s web-1 0/1 Pending 0 0s web-1 0/1 Pending 0 0s web-1 0/1 ContainerCreating 0 1s web-1 1/1 Running 0 2s web-2 0/1 Pending 0 0s web-2 0/1 Pending 0 0s web-2 0/1 ContainerCreating 0 0s web-2 1/1 Running 0 1s web-0 1/1 Terminating 0 6m6s ----&gt; start terminating 0 (only after 2 and 1 are running) web-0 0/1 Terminating 0 6m7s web-0 0/1 Pending 0 0s web-0 0/1 Pending 0 0s web-0 0/1 ContainerCreating 0 0s web-0 1/1 Running 0 1s </code></pre><p>Note that as soon as the rolling update starts, both 4 and 3 (the two highest ordinal pods) start terminating at the same time. Pods with ordinal 4 and 3 may become ready at their own pace. As soon as both pods 4 and 3 are ready, pods 2 and 1 start terminating at the same time. When pods 2 and 1 are both running and ready, pod 0 starts terminating.</p> <p>In Kubernetes, updates to StatefulSets follow a strict ordering when updating Pods. In this example, the update starts at replica 4, then replica 3, then replica 2, and so on, one pod at a time. When going one pod at a time, its not possible for 3 to be running and ready before 4. When <code>maxUnavailable</code> is more than 1 (in the example scenario I set <code>maxUnavailable</code> to 2), it is possible that replica 3 becomes ready and running before replica 4 is ready—and that is ok. If you're a developer and you set <code>maxUnavailable</code> to more than 1, you should know that this outcome is possible and you must ensure that your application is able to handle such ordering issues that occur if any. When you set <code>maxUnavailable</code> greater than 1, the ordering is guaranteed in between each batch of pods being updated. That guarantee means that pods in update batch 2 (replicas 2 and 1) cannot start updating until the pods from batch 0 (replicas 4 and 3) are ready.</p> <p>Although Kubernetes refers to these as <em>replicas</em>, your stateful application may have a different view and each pod of the StatefulSet may be holding completely different data than other pods. The important thing here is that updates to StatefulSets happen in batches, and you can now have a batch size larger than 1 (as an alpha feature).</p> <p>Also note, that the above behavior is with <code>podManagementPolicy: OrderedReady</code>. If you defined a StatefulSet as <code>podManagementPolicy: Parallel</code>, not only <code>maxUnavailable</code> number of replicas are terminated at the same time; <code>maxUnavailable</code> number of replicas start in <code>ContainerCreating</code> phase at the same time as well. This is called bursting.</p> <p>So, now you may have a lot of questions about:-</p> <ul> <li>What is the behavior when you set <code>podManagementPolicy: Parallel</code>?</li> <li>What is the behavior when <code>partition</code> to a value other than <code>0</code>?</li> </ul> <p>It might be better to try and see it for yourself. This is an alpha feature, and the Kubernetes contributors are looking for feedback on this feature. Did this help you achieve your stateful scenarios Did you find a bug or do you think the behavior as implemented is not intuitive or can break applications or catch them by surprise? Please <a href="https://github.com/kubernetes/kubernetes/issues">open an issue</a> to let us know.</p> <h2 id="next-steps">Further reading and next steps</h2> <ul> <li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#maximum-unavailable-pods">Maximum unavailable Pods</a></li> <li><a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-apps/961-maxunavailable-for-statefulset">KEP for MaxUnavailable for StatefulSet</a></li> <li><a href="https://github.com/kubernetes/kubernetes/pull/82162/files">Implementation</a></li> <li><a href="https://github.com/kubernetes/enhancements/issues/961">Enhancement Tracking Issue</a></li> </ul></description></item><item><title>Blog: Contextual Logging in Kubernetes 1.24</title><link>https://kubernetes.io/blog/2022/05/25/contextual-logging/</link><pubDate>Wed, 25 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/05/25/contextual-logging/</guid><description> <p><strong>Authors:</strong> Patrick Ohly (Intel)</p> <p>The <a href="https://github.com/kubernetes/community/blob/master/wg-structured-logging/README.md">Structured Logging Working Group</a> has added new capabilities to the logging infrastructure in Kubernetes 1.24. This blog post explains how developers can take advantage of those to make log output more useful and how they can get involved with improving Kubernetes.</p> <h2 id="structured-logging">Structured logging</h2> <p>The goal of <a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-instrumentation/1602-structured-logging/README.md">structured logging</a> is to replace C-style formatting and the resulting opaque log strings with log entries that have a well-defined syntax for storing message and parameters separately, for example as a JSON struct.</p> <p>When using the traditional klog text output format for structured log calls, strings were originally printed with <code>\n</code> escape sequences, except when embedded inside a struct. For structs, log entries could still span multiple lines, with no clean way to split the log stream into individual entries:</p> <pre tabindex="0"><code>I1112 14:06:35.783529 328441 structured_logging.go:51] &#34;using InfoS&#34; longData={Name:long Data:Multiple lines with quite a bit of text. internal:0} I1112 14:06:35.783549 328441 structured_logging.go:52] &#34;using InfoS with\nthe message across multiple lines&#34; int=1 stringData=&#34;long: Multiple\nlines\nwith quite a bit\nof text.&#34; str=&#34;another value&#34; </code></pre><p>Now, the <code>&lt;</code> and <code>&gt;</code> markers along with indentation are used to ensure that splitting at a klog header at the start of a line is reliable and the resulting output is human-readable:</p> <pre tabindex="0"><code>I1126 10:31:50.378204 121736 structured_logging.go:59] &#34;using InfoS&#34; longData=&lt; {Name:long Data:Multiple lines with quite a bit of text. internal:0} &gt; I1126 10:31:50.378228 121736 structured_logging.go:60] &#34;using InfoS with\nthe message across multiple lines&#34; int=1 stringData=&lt; long: Multiple lines with quite a bit of text. &gt; str=&#34;another value&#34; </code></pre><p>Note that the log message itself is printed with quoting. It is meant to be a fixed string that identifies a log entry, so newlines should be avoided there.</p> <p>Before Kubernetes 1.24, some log calls in kube-scheduler still used <code>klog.Info</code> for multi-line strings to avoid the unreadable output. Now all log calls have been updated to support structured logging.</p> <h2 id="contextual-logging">Contextual logging</h2> <p><a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-instrumentation/3077-contextual-logging/README.md">Contextual logging</a> is based on the <a href="https://github.com/go-logr/logr#a-minimal-logging-api-for-go">go-logr API</a>. The key idea is that libraries are passed a logger instance by their caller and use that for logging instead of accessing a global logger. The binary decides about the logging implementation, not the libraries. The go-logr API is designed around structured logging and supports attaching additional information to a logger.</p> <p>This enables additional use cases:</p> <ul> <li> <p>The caller can attach additional information to a logger:</p> <ul> <li><a href="https://pkg.go.dev/github.com/go-logr/logr#Logger.WithName"><code>WithName</code></a> adds a prefix</li> <li><a href="https://pkg.go.dev/github.com/go-logr/logr#Logger.WithValues"><code>WithValues</code></a> adds key/value pairs</li> </ul> <p>When passing this extended logger into a function and a function uses it instead of the global logger, the additional information is then included in all log entries, without having to modify the code that generates the log entries. This is useful in highly parallel applications where it can become hard to identify all log entries for a certain operation because the output from different operations gets interleaved.</p> </li> <li> <p>When running unit tests, log output can be associated with the current test. Then when a test fails, only the log output of the failed test gets shown by <code>go test</code>. That output can also be more verbose by default because it will not get shown for successful tests. Tests can be run in parallel without interleaving their output.</p> </li> </ul> <p>One of the design decisions for contextual logging was to allow attaching a logger as value to a <code>context.Context</code>. Since the logger encapsulates all aspects of the intended logging for the call, it is <em>part</em> of the context and not just <em>using</em> it. A practical advantage is that many APIs already have a <code>ctx</code> parameter or adding one has additional advantages, like being able to get rid of <code>context.TODO()</code> calls inside the functions.</p> <p>Another decision was to not break compatibility with klog v2:</p> <ul> <li> <p>Libraries that use the traditional klog logging calls in a binary that has set up contextual logging will work and log through the logging backend chosen by the binary. However, such log output will not include the additional information and will not work well in unit tests, so libraries should be modified to support contextual logging. The <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/migration-to-structured-logging.md">migration guide</a> for structured logging has been extended to also cover contextual logging.</p> </li> <li> <p>When a library supports contextual logging and retrieves a logger from its context, it will still work in a binary that does not initialize contextual logging because it will get a logger that logs through klog.</p> </li> </ul> <p>In Kubernetes 1.24, contextual logging is a new alpha feature with <code>ContextualLogging</code> as feature gate. When disabled (the default), the new klog API calls for contextual logging (see below) become no-ops to avoid performance or functional regressions.</p> <p>No Kubernetes component has been converted yet. An <a href="https://github.com/kubernetes/kubernetes/blob/v1.24.0-beta.0/staging/src/k8s.io/component-base/logs/example/cmd/logger.go">example program</a> in the Kubernetes repository demonstrates how to enable contextual logging in a binary and how the output depends on the binary's parameters:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span><span style="color:#000080;font-weight:bold">$</span> <span style="color:#a2f">cd</span> <span style="color:#b8860b">$GOPATH</span>/src/k8s.io/kubernetes/staging/src/k8s.io/component-base/logs/example/cmd/ </span></span><span style="display:flex;"><span><span style="color:#000080;font-weight:bold">$</span> go run . --help </span></span><span style="display:flex;"><span><span style="color:#888">... </span></span></span><span style="display:flex;"><span><span style="color:#888"> --feature-gates mapStringBool A set of key=value pairs that describe feature gates for alpha/experimental features. Options are: </span></span></span><span style="display:flex;"><span><span style="color:#888"> AllAlpha=true|false (ALPHA - default=false) </span></span></span><span style="display:flex;"><span><span style="color:#888"> AllBeta=true|false (BETA - default=false) </span></span></span><span style="display:flex;"><span><span style="color:#888"> ContextualLogging=true|false (ALPHA - default=false) </span></span></span><span style="display:flex;"><span><span style="color:#888"></span><span style="color:#000080;font-weight:bold">$</span> go run . --feature-gates <span style="color:#b8860b">ContextualLogging</span><span style="color:#666">=</span><span style="color:#a2f">true</span> </span></span><span style="display:flex;"><span><span style="color:#888">... </span></span></span><span style="display:flex;"><span><span style="color:#888">I0404 18:00:02.916429 451895 logger.go:94] &#34;example/myname: runtime&#34; foo=&#34;bar&#34; duration=&#34;1m0s&#34; </span></span></span><span style="display:flex;"><span><span style="color:#888">I0404 18:00:02.916447 451895 logger.go:95] &#34;example: another runtime&#34; foo=&#34;bar&#34; duration=&#34;1m0s&#34; </span></span></span></code></pre></div><p>The <code>example</code> prefix and <code>foo=&quot;bar&quot;</code> were added by the caller of the function which logs the <code>runtime</code> message and <code>duration=&quot;1m0s&quot;</code> value.</p> <p>The sample code for klog includes an <a href="https://github.com/kubernetes/klog/blob/v2.60.1/ktesting/example/example_test.go">example</a> for a unit test with per-test output.</p> <h2 id="klog-enhancements">klog enhancements</h2> <h3 id="contextual-logging-api">Contextual logging API</h3> <p>The following calls manage the lookup of a logger:</p> <dl> <dt><a href="https://pkg.go.dev/k8s.io/klog/v2#FromContext"><code>FromContext</code></a></dt> <dd>from a <code>context</code> parameter, with fallback to the global logger</dd> <dt><a href="https://pkg.go.dev/k8s.io/klog/v2#Background"><code>Background</code></a></dt> <dd>the global fallback, with no intention to support contextual logging</dd> <dt><a href="https://pkg.go.dev/k8s.io/klog/v2#TODO"><code>TODO</code></a></dt> <dd>the global fallback, but only as a temporary solution until the function gets extended to accept a logger through its parameters</dd> <dt><a href="https://pkg.go.dev/k8s.io/klog/v2#SetLoggerWithOptions"><code>SetLoggerWithOptions</code></a></dt> <dd>changes the fallback logger; when called with <a href="https://pkg.go.dev/k8s.io/klog/v2#ContextualLogger"><code>ContextualLogger(true)</code></a>, the logger is ready to be called directly, in which case logging will be done without going through klog</dd> </dl> <p>To support the feature gate mechanism in Kubernetes, klog has wrapper calls for the corresponding go-logr calls and a global boolean controlling their behavior:</p> <ul> <li><a href="https://pkg.go.dev/k8s.io/klog/v2#LoggerWithName"><code>LoggerWithName</code></a></li> <li><a href="https://pkg.go.dev/k8s.io/klog/v2#LoggerWithValues"><code>LoggerWithValues</code></a></li> <li><a href="https://pkg.go.dev/k8s.io/klog/v2#NewContext"><code>NewContext</code></a></li> <li><a href="https://pkg.go.dev/k8s.io/klog/v2#EnableContextualLogging"><code>EnableContextualLogging</code></a></li> </ul> <p>Usage of those functions in Kubernetes code is enforced with a linter check. The klog default for contextual logging is to enable the functionality because it is considered stable in klog. It is only in Kubernetes binaries where that default gets overridden and (in some binaries) controlled via the <code>--feature-gate</code> parameter.</p> <h3 id="ktesting-logger">ktesting logger</h3> <p>The new <a href="https://pkg.go.dev/k8s.io/klog/v2@v2.60.1/ktesting">ktesting</a> package implements logging through <code>testing.T</code> using klog's text output format. It has a <a href="https://pkg.go.dev/k8s.io/klog/v2@v2.60.1/ktesting#NewTestContext">single API call</a> for instrumenting a test case and <a href="https://pkg.go.dev/k8s.io/klog/v2@v2.60.1/ktesting/init">support for command line flags</a>.</p> <h3 id="klogr">klogr</h3> <p><a href="https://pkg.go.dev/k8s.io/klog/v2@v2.60.1/klogr"><code>klog/klogr</code></a> continues to be supported and it's default behavior is unchanged: it formats structured log entries using its own, custom format and prints the result via klog.</p> <p>However, this usage is discouraged because that format is neither machine-readable (in contrast to real JSON output as produced by zapr, the go-logr implementation used by Kubernetes) nor human-friendly (in contrast to the klog text format).</p> <p>Instead, a klogr instance should be created with <a href="https://pkg.go.dev/k8s.io/klog/v2@v2.60.1/klogr#WithFormat"><code>WithFormat(FormatKlog)</code></a> which chooses the klog text format. A simpler construction method with the same result is the new <a href="https://pkg.go.dev/k8s.io/klog/v2#NewKlogr"><code>klog.NewKlogr</code></a>. That is the logger that klog returns as fallback when nothing else is configured.</p> <h3 id="reusable-output-test">Reusable output test</h3> <p>A lot of go-logr implementations have very similar unit tests where they check the result of certain log calls. If a developer didn't know about certain caveats like for example a <code>String</code> function that panics when called, then it is likely that both the handling of such caveats and the unit test are missing.</p> <p><a href="https://pkg.go.dev/k8s.io/klog/v2@v2.60.1/test"><code>klog.test</code></a> is a reusable set of test cases that can be applied to a go-logr implementation.</p> <h3 id="output-flushing">Output flushing</h3> <p>klog used to start a goroutine unconditionally during <code>init</code> which flushed buffered data at a hard-coded interval. Now that goroutine is only started on demand (i.e. when writing to files with buffering) and can be controlled with <a href="https://pkg.go.dev/k8s.io/klog/v2#StopFlushDaemon"><code>StopFlushDaemon</code></a> and <a href="https://pkg.go.dev/k8s.io/klog/v2#StartFlushDaemon"><code>StartFlushDaemon</code></a>.</p> <p>When a go-logr implementation buffers data, flushing that data can be integrated into <a href="https://pkg.go.dev/k8s.io/klog/v2#Flush"><code>klog.Flush</code></a> by registering the logger with the <a href="https://pkg.go.dev/k8s.io/klog/v2#FlushLogger"><code>FlushLogger</code></a> option.</p> <h3 id="various-other-changes">Various other changes</h3> <p>For a description of all other enhancements see in the <a href="https://github.com/kubernetes/klog/releases">release notes</a>.</p> <h2 id="logcheck">logcheck</h2> <p>Originally designed as a linter for structured log calls, the <a href="https://github.com/kubernetes/klog/tree/788efcdee1e9be0bfbe5b076343d447314f2377e/hack/tools/logcheck"><code>logcheck</code></a> tool has been enhanced to support also contextual logging and traditional klog log calls. These enhanced checks already found bugs in Kubernetes, like calling <code>klog.Info</code> instead of <code>klog.Infof</code> with a format string and parameters.</p> <p>It can be included as a plugin in a <code>golangci-lint</code> invocation, which is how <a href="https://github.com/kubernetes/kubernetes/commit/17e3c555c5115f8c9176bae10ba45baa04d23a7b">Kubernetes uses it now</a>, or get invoked stand-alone.</p> <p>We are in the process of <a href="https://github.com/kubernetes/klog/issues/312">moving the tool</a> into a new repository because it isn't really related to klog and its releases should be tracked and tagged properly.</p> <h2 id="next-steps">Next steps</h2> <p>The <a href="https://github.com/kubernetes/community/tree/master/wg-structured-logging">Structured Logging WG</a> is always looking for new contributors. The migration away from C-style logging is now going to target structured, contextual logging in one step to reduce the overall code churn and number of PRs. Changing log calls is good first contribution to Kubernetes and an opportunity to get to know code in various different areas.</p></description></item><item><title>Blog: Kubernetes 1.24: Avoid Collisions Assigning IP Addresses to Services</title><link>https://kubernetes.io/blog/2022/05/23/service-ip-dynamic-and-static-allocation/</link><pubDate>Mon, 23 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/05/23/service-ip-dynamic-and-static-allocation/</guid><description> <p><strong>Author:</strong> Antonio Ojea (Red Hat)</p> <p>In Kubernetes, <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Services</a> are an abstract way to expose an application running on a set of Pods. Services can have a cluster-scoped virtual IP address (using a Service of <code>type: ClusterIP</code>). Clients can connect using that virtual IP address, and Kubernetes then load-balances traffic to that Service across the different backing Pods.</p> <h2 id="how-service-clusterips-are-allocated">How Service ClusterIPs are allocated?</h2> <p>A Service <code>ClusterIP</code> can be assigned:</p> <dl> <dt><em>dynamically</em></dt> <dd>the cluster's control plane automatically picks a free IP address from within the configured IP range for <code>type: ClusterIP</code> Services.</dd> <dt><em>statically</em></dt> <dd>you specify an IP address of your choice, from within the configured IP range for Services.</dd> </dl> <p>Across your whole cluster, every Service <code>ClusterIP</code> must be unique. Trying to create a Service with a specific <code>ClusterIP</code> that has already been allocated will return an error.</p> <h2 id="why-do-you-need-to-reserve-service-cluster-ips">Why do you need to reserve Service Cluster IPs?</h2> <p>Sometimes you may want to have Services running in well-known IP addresses, so other components and users in the cluster can use them.</p> <p>The best example is the DNS Service for the cluster. Some Kubernetes installers assign the 10th address from the Service IP range to the DNS service. Assuming you configured your cluster with Service IP range 10.96.0.0/16 and you want your DNS Service IP to be 10.96.0.10, you'd have to create a Service like this:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Service<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">labels</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">k8s-app</span>:<span style="color:#bbb"> </span>kube-dns<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">kubernetes.io/cluster-service</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">kubernetes.io/name</span>:<span style="color:#bbb"> </span>CoreDNS<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>kube-dns<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>kube-system<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">clusterIP</span>:<span style="color:#bbb"> </span><span style="color:#666">10.96.0.10</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dns<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">53</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>UDP<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">53</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>dns-tcp<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">53</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">protocol</span>:<span style="color:#bbb"> </span>TCP<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">targetPort</span>:<span style="color:#bbb"> </span><span style="color:#666">53</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">selector</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">k8s-app</span>:<span style="color:#bbb"> </span>kube-dns<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">type</span>:<span style="color:#bbb"> </span>ClusterIP<span style="color:#bbb"> </span></span></span></code></pre></div><p>but as I explained before, the IP address 10.96.0.10 has not been reserved; if other Services are created before or in parallel with dynamic allocation, there is a chance they can allocate this IP, hence, you will not be able to create the DNS Service because it will fail with a conflict error.</p> <h2 id="avoid-ClusterIP-conflict">How can you avoid Service ClusterIP conflicts?</h2> <p>In Kubernetes 1.24, you can enable a new feature gate <code>ServiceIPStaticSubrange</code>. Turning this on allows you to use a different IP allocation strategy for Services, reducing the risk of collision.</p> <p>The <code>ClusterIP</code> range will be divided, based on the formula <code>min(max(16, cidrSize / 16), 256)</code>, described as <em>never less than 16 or more than 256 with a graduated step between them</em>.</p> <p>Dynamic IP assignment will use the upper band by default, once this has been exhausted it will use the lower range. This will allow users to use static allocations on the lower band with a low risk of collision.</p> <p>Examples:</p> <h4 id="service-ip-cidr-block-10-96-0-0-24">Service IP CIDR block: 10.96.0.0/24</h4> <p>Range Size: 2<sup>8</sup> - 2 = 254<br> Band Offset: <code>min(max(16, 256/16), 256)</code> = <code>min(16, 256)</code> = 16<br> Static band start: 10.96.0.1<br> Static band end: 10.96.0.16<br> Range end: 10.96.0.254</p> <figure> <div class="mermaid"> pie showData title 10.96.0.0/24 "Static" : 16 "Dynamic" : 238 </div> </figure> <noscript> <div class="alert alert-secondary callout" role="alert"> <em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em> </div> </noscript> <h4 id="service-ip-cidr-block-10-96-0-0-20">Service IP CIDR block: 10.96.0.0/20</h4> <p>Range Size: 2<sup>12</sup> - 2 = 4094<br> Band Offset: <code>min(max(16, 4096/16), 256)</code> = <code>min(256, 256)</code> = 256<br> Static band start: 10.96.0.1<br> Static band end: 10.96.1.0<br> Range end: 10.96.15.254</p> <figure> <div class="mermaid"> pie showData title 10.96.0.0/20 "Static" : 256 "Dynamic" : 3838 </div> </figure> <noscript> <div class="alert alert-secondary callout" role="alert"> <em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em> </div> </noscript> <h4 id="service-ip-cidr-block-10-96-0-0-16">Service IP CIDR block: 10.96.0.0/16</h4> <p>Range Size: 2<sup>16</sup> - 2 = 65534<br> Band Offset: <code>min(max(16, 65536/16), 256)</code> = <code>min(4096, 256)</code> = 256<br> Static band start: 10.96.0.1<br> Static band ends: 10.96.1.0<br> Range end: 10.96.255.254</p> <figure> <div class="mermaid"> pie showData title 10.96.0.0/16 "Static" : 256 "Dynamic" : 65278 </div> </figure> <noscript> <div class="alert alert-secondary callout" role="alert"> <em class="javascript-required">JavaScript must be <a href="https://www.enable-javascript.com/">enabled</a> to view this content</em> </div> </noscript> <h2 id="get-involved-with-sig-network">Get involved with SIG Network</h2> <p>The current SIG-Network <a href="https://github.com/orgs/kubernetes/projects/10">KEPs</a> and <a href="https://github.com/kubernetes/kubernetes/issues?q=is%3Aopen+is%3Aissue+label%3Asig%2Fnetwork">issues</a> on GitHub illustrate the SIG’s areas of emphasis.</p> <p><a href="https://github.com/kubernetes/community/tree/master/sig-network">SIG Network meetings</a> are a friendly, welcoming venue for you to connect with the community and share your ideas. Looking forward to hearing from you!</p></description></item><item><title>Blog: Kubernetes 1.24: Introducing Non-Graceful Node Shutdown Alpha</title><link>https://kubernetes.io/blog/2022/05/20/kubernetes-1-24-non-graceful-node-shutdown-alpha/</link><pubDate>Fri, 20 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/05/20/kubernetes-1-24-non-graceful-node-shutdown-alpha/</guid><description> <p><strong>Authors</strong> Xing Yang and Yassine Tijani (VMware)</p> <p>Kubernetes v1.24 introduces alpha support for <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/2268-non-graceful-shutdown">Non-Graceful Node Shutdown</a>. This feature allows stateful workloads to failover to a different node after the original node is shutdown or in a non-recoverable state such as hardware failure or broken OS.</p> <h2 id="how-is-this-different-from-graceful-node-shutdown">How is this different from Graceful Node Shutdown</h2> <p>You might have heard about the <a href="https://kubernetes.io/docs/concepts/architecture/nodes/#graceful-node-shutdown">Graceful Node Shutdown</a> capability of Kubernetes, and are wondering how the Non-Graceful Node Shutdown feature is different from that. Graceful Node Shutdown allows Kubernetes to detect when a node is shutting down cleanly, and handles that situation appropriately. A Node Shutdown can be &quot;graceful&quot; only if the node shutdown action can be detected by the kubelet ahead of the actual shutdown. However, there are cases where a node shutdown action may not be detected by the kubelet. This could happen either because the shutdown command does not trigger the systemd inhibitor locks mechanism that kubelet relies upon, or because of a configuration error (the <code>ShutdownGracePeriod</code> and <code>ShutdownGracePeriodCriticalPods</code> are not configured properly).</p> <p>Graceful node shutdown relies on Linux-specific support. The kubelet does not watch for upcoming shutdowns on Windows nodes (this may change in a future Kubernetes release).</p> <p>When a node is shutdown but without the kubelet detecting it, pods on that node also shut down ungracefully. For stateless apps, that's often not a problem (a ReplicaSet adds a new pod once the cluster detects that the affected node or pod has failed). For stateful apps, the story is more complicated. If you use a StatefulSet and have a pod from that StatefulSet on a node that fails uncleanly, that affected pod will be marked as terminating; the StatefulSet cannot create a replacement pod because the pod still exists in the cluster. As a result, the application running on the StatefulSet may be degraded or even offline. If the original, shut down node comes up again, the kubelet on that original node reports in, deletes the existing pods, and the control plane makes a replacement pod for that StatefulSet on a different running node. If the original node has failed and does not come up, those stateful pods would be stuck in a terminating status on that failed node indefinitely.</p> <pre tabindex="0"><code>$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES web-0 1/1 Running 0 100m 10.244.2.4 k8s-node-876-1639279816 &lt;none&gt; &lt;none&gt; web-1 1/1 Terminating 0 100m 10.244.1.3 k8s-node-433-1639279804 &lt;none&gt; &lt;none&gt; </code></pre><h2 id="try-out-the-new-non-graceful-shutdown-handling">Try out the new non-graceful shutdown handling</h2> <p>To use the non-graceful node shutdown handling, you must enable the <code>NodeOutOfServiceVolumeDetach</code> <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a> for the <code>kube-controller-manager</code> component.</p> <p>In the case of a node shutdown, you can manually taint that node as out of service. You should make certain that the node is truly shutdown (not in the middle of restarting) before you add that taint. You could add that taint following a shutdown that the kubelet did not detect and handle in advance; another case where you can use that taint is when the node is in a non-recoverable state due to a hardware failure or a broken OS. The values you set for that taint can be <code>node.kubernetes.io/out-of-service=nodeshutdown: &quot;NoExecute&quot;</code> or <code>node.kubernetes.io/out-of-service=nodeshutdown:&quot; NoSchedule&quot;</code>. Provided you have enabled the feature gate mentioned earlier, setting the out-of-service taint on a Node means that pods on the node will be deleted unless if there are matching tolerations on the pods. Persistent volumes attached to the shutdown node will be detached, and for StatefulSets, replacement pods will be created successfully on a different running node.</p> <pre tabindex="0"><code>$ kubectl taint nodes &lt;node-name&gt; node.kubernetes.io/out-of-service=nodeshutdown:NoExecute $ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES web-0 1/1 Running 0 150m 10.244.2.4 k8s-node-876-1639279816 &lt;none&gt; &lt;none&gt; web-1 1/1 Running 0 10m 10.244.1.7 k8s-node-433-1639279804 &lt;none&gt; &lt;none&gt; </code></pre><p>Note: Before applying the out-of-service taint, you <strong>must</strong> verify that a node is already in shutdown or power off state (not in the middle of restarting), either because the user intentionally shut it down or the node is down due to hardware failures, OS issues, etc.</p> <p>Once all the workload pods that are linked to the out-of-service node are moved to a new running node, and the shutdown node has been recovered, you should remove that taint on the affected node after the node is recovered. If you know that the node will not return to service, you could instead delete the node from the cluster.</p> <h2 id="what-s-next">What’s next?</h2> <p>Depending on feedback and adoption, the Kubernetes team plans to push the Non-Graceful Node Shutdown implementation to Beta in either 1.25 or 1.26.</p> <p>This feature requires a user to manually add a taint to the node to trigger workloads failover and remove the taint after the node is recovered. In the future, we plan to find ways to automatically detect and fence nodes that are shutdown/failed and automatically failover workloads to another node.</p> <h2 id="how-can-i-learn-more">How can I learn more?</h2> <p>Check out the <a href="https://kubernetes.io/docs/concepts/architecture/nodes/#non-graceful-node-shutdown">documentation</a> for non-graceful node shutdown.</p> <h2 id="how-to-get-involved">How to get involved?</h2> <p>This feature has a long story. Yassine Tijani (<a href="https://github.com/yastij">yastij</a>) started the KEP more than two years ago. Xing Yang (<a href="https://github.com/xing-yang">xing-yang</a>) continued to drive the effort. There were many discussions among SIG Storage, SIG Node, and API reviewers to nail down the design details. Ashutosh Kumar (<a href="https://github.com/sonasingh46">sonasingh46</a>) did most of the implementation and brought it to Alpha in Kubernetes 1.24.</p> <p>We want to thank the following people for their insightful reviews: Tim Hockin (<a href="https://github.com/thockin">thockin</a>) for his guidance on the design, Jing Xu (<a href="https://github.com/jingxu97">jingxu97</a>), Hemant Kumar (<a href="https://github.com/gnufied">gnufied</a>), and Michelle Au (<a href="https://github.com/msau42">msau42</a>) for reviews from SIG Storage side, and Mrunal Patel (<a href="https://github.com/mrunalp">mrunalp</a>), David Porter (<a href="https://github.com/bobbypage">bobbypage</a>), Derek Carr (<a href="https://github.com/derekwaynecarr">derekwaynecarr</a>), and Danielle Endocrimes (<a href="https://github.com/endocrimes">endocrimes</a>) for reviews from SIG Node side.</p> <p>There are many people who have helped review the design and implementation along the way. We want to thank everyone who has contributed to this effort including the about 30 people who have reviewed the <a href="https://github.com/kubernetes/enhancements/pull/1116">KEP</a> and implementation over the last couple of years.</p> <p>This feature is a collaboration between SIG Storage and SIG Node. For those interested in getting involved with the design and development of any part of the Kubernetes Storage system, join the <a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes Storage Special Interest Group</a> (SIG). For those interested in getting involved with the design and development of the components that support the controlled interactions between pods and host resources, join the <a href="https://github.com/kubernetes/community/tree/master/sig-node">Kubernetes Node SIG</a>.</p></description></item><item><title>Blog: Kubernetes 1.24: Prevent unauthorised volume mode conversion</title><link>https://kubernetes.io/blog/2022/05/18/prevent-unauthorised-volume-mode-conversion-alpha/</link><pubDate>Wed, 18 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/05/18/prevent-unauthorised-volume-mode-conversion-alpha/</guid><description> <p><strong>Author:</strong> Raunak Pradip Shah (Mirantis)</p> <p>Kubernetes v1.24 introduces a new alpha-level feature that prevents unauthorised users from modifying the volume mode of a <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/"><code>PersistentVolumeClaim</code></a> created from an existing <a href="https://kubernetes.io/docs/concepts/storage/volume-snapshots/"><code>VolumeSnapshot</code></a> in the Kubernetes cluster.</p> <h3 id="the-problem">The problem</h3> <p>The <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#volume-mode">Volume Mode</a> determines whether a volume is formatted into a filesystem or presented as a raw block device.</p> <p>Users can leverage the <code>VolumeSnapshot</code> feature, which has been stable since Kubernetes v1.20, to create a <code>PersistentVolumeClaim</code> (shortened as PVC) from an existing <code>VolumeSnapshot</code> in the Kubernetes cluster. The PVC spec includes a <code>dataSource</code> field, which can point to an existing <code>VolumeSnapshot</code> instance. Visit <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#create-persistent-volume-claim-from-volume-snapshot">Create a PersistentVolumeClaim from a Volume Snapshot</a> for more details.</p> <p>When leveraging the above capability, there is no logic that validates whether the mode of the original volume, whose snapshot was taken, matches the mode of the newly created volume.</p> <p>This presents a security gap that allows malicious users to potentially exploit an as-yet-unknown vulnerability in the host operating system.</p> <p>Many popular storage backup vendors convert the volume mode during the course of a backup operation, for efficiency purposes, which prevents Kubernetes from blocking the operation completely and presents a challenge in distinguishing trusted users from malicious ones.</p> <h3 id="preventing-unauthorised-users-from-converting-the-volume-mode">Preventing unauthorised users from converting the volume mode</h3> <p>In this context, an authorised user is one who has access rights to perform <code>Update</code> or <code>Patch</code> operations on <code>VolumeSnapshotContents</code>, which is a cluster-level resource.<br> It is upto the cluster administrator to provide these rights only to trusted users or applications, like backup vendors.</p> <p>If the alpha feature is <a href="https://kubernetes-csi.github.io/docs/">enabled</a> in <code>snapshot-controller</code>, <code>snapshot-validation-webhook</code> and <code>external-provisioner</code>, then unauthorised users will not be allowed to modify the volume mode of a PVC when it is being created from a <code>VolumeSnapshot</code>.</p> <p>To convert the volume mode, an authorised user must do the following:</p> <ol> <li>Identify the <code>VolumeSnapshot</code> that is to be used as the data source for a newly created PVC in the given namespace.</li> <li>Identify the <code>VolumeSnapshotContent</code> bound to the above <code>VolumeSnapshot</code>.</li> </ol> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>kubectl get volumesnapshot -n &lt;namespace&gt; </span></span></code></pre></div><ol start="3"> <li> <p>Add the annotation <a href="https://kubernetes.io/docs/reference/labels-annotations-taints/#snapshot-storage-kubernetes-io-allowvolumemodechange"><code>snapshot.storage.kubernetes.io/allowVolumeModeChange</code></a> to the <code>VolumeSnapshotContent</code>.</p> </li> <li> <p>This annotation can be added either via software or manually by the authorised user. The <code>VolumeSnapshotContent</code> annotation must look like following manifest fragment:</p> </li> </ol> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>VolumeSnapshotContent<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- <span style="color:#008000;font-weight:bold">snapshot.storage.kubernetes.io/allowVolumeModeChange</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#00f;font-weight:bold">...</span><span style="color:#bbb"> </span></span></span></code></pre></div><p><strong>Note</strong>: For pre-provisioned <code>VolumeSnapshotContents</code>, you must take an extra step of setting <code>spec.sourceVolumeMode</code> field to either <code>Filesystem</code> or <code>Block</code>, depending on the mode of the volume from which this snapshot was taken.</p> <p>An example is shown below:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>snapshot.storage.k8s.io/v1<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>VolumeSnapshotContent<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">annotations</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- <span style="color:#008000;font-weight:bold">snapshot.storage.kubernetes.io/allowVolumeModeChange</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;true&#34;</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>new-snapshot-content-test<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">deletionPolicy</span>:<span style="color:#bbb"> </span>Delete<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">driver</span>:<span style="color:#bbb"> </span>hostpath.csi.k8s.io<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">source</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">snapshotHandle</span>:<span style="color:#bbb"> </span>7bdd0de3-aaeb-11e8-9aae-0242ac110002<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">sourceVolumeMode</span>:<span style="color:#bbb"> </span>Filesystem<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">volumeSnapshotRef</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>new-snapshot-test<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">namespace</span>:<span style="color:#bbb"> </span>default<span style="color:#bbb"> </span></span></span></code></pre></div><p>Repeat steps 1 to 3 for all <code>VolumeSnapshotContents</code> whose volume mode needs to be converted during a backup or restore operation.</p> <p>If the annotation shown in step 4 above is present on a <code>VolumeSnapshotContent</code> object, Kubernetes will not prevent the volume mode from being converted. Users should keep this in mind before they attempt to add the annotation to any <code>VolumeSnapshotContent</code>.</p> <h3 id="what-s-next">What's next</h3> <p><a href="https://kubernetes-csi.github.io/docs/">Enable this feature</a> and let us know what you think!</p> <p>We hope this feature causes no disruption to existing workflows while preventing malicious users from exploiting security vulnerabilities in their clusters.</p> <p>For any queries or issues, join <a href="https://slack.k8s.io/">Kubernetes on Slack</a> and create a thread in the #sig-storage channel. Alternately, create an issue in the CSI external-snapshotter <a href="https://github.com/kubernetes-csi/external-snapshotter">repository</a>.</p></description></item><item><title>Blog: Kubernetes 1.24: Volume Populators Graduate to Beta</title><link>https://kubernetes.io/blog/2022/05/16/volume-populators-beta/</link><pubDate>Mon, 16 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/05/16/volume-populators-beta/</guid><description> <p><strong>Author:</strong> Ben Swartzlander (NetApp)</p> <p>The volume populators feature is now two releases old and entering beta! The <code>AnyVolumeDataSource</code> feature gate defaults to enabled in Kubernetes v1.24, which means that users can specify any custom resource as the data source of a PVC.</p> <p>An <a href="https://kubernetes.io/blog/2021/08/30/volume-populators-redesigned/">earlier blog article</a> detailed how the volume populators feature works. In short, a cluster administrator can install a CRD and associated populator controller in the cluster, and any user who can create instances of the CR can create pre-populated volumes by taking advantage of the populator.</p> <p>Multiple populators can be installed side by side for different purposes. The SIG storage community is already seeing some implementations in public, and more prototypes should appear soon.</p> <p>Cluster administrations are <strong>strongly encouraged</strong> to install the volume-data-source-validator controller and associated <code>VolumePopulator</code> CRD before installing any populators so that users can get feedback about invalid PVC data sources.</p> <h2 id="new-features">New Features</h2> <p>The <a href="https://github.com/kubernetes-csi/lib-volume-populator">lib-volume-populator</a> library on which populators are built now includes metrics to help operators monitor and detect problems. This library is now beta and latest release is v1.0.1.</p> <p>The <a href="https://github.com/kubernetes-csi/volume-data-source-validator">volume data source validator</a> controller also has metrics support added, and is in beta. The <code>VolumePopulator</code> CRD is beta and the latest release is v1.0.1.</p> <h2 id="trying-it-out">Trying it out</h2> <p>To see how this works, you can install the sample &quot;hello&quot; populator and try it out.</p> <p>First install the volume-data-source-validator controller.</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/volume-data-source-validator/v1.0.1/client/config/crd/populator.storage.k8s.io_volumepopulators.yaml </span></span><span style="display:flex;"><span>kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/volume-data-source-validator/v1.0.1/deploy/kubernetes/rbac-data-source-validator.yaml </span></span><span style="display:flex;"><span>kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/volume-data-source-validator/v1.0.1/deploy/kubernetes/setup-data-source-validator.yaml </span></span></code></pre></div><p>Next install the example populator.</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/lib-volume-populator/v1.0.1/example/hello-populator/crd.yaml </span></span><span style="display:flex;"><span>kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/lib-volume-populator/87a47467b86052819e9ad13d15036d65b9a32fbb/example/hello-populator/deploy.yaml </span></span></code></pre></div><p>Your cluster now has a new CustomResourceDefinition that provides a test API named Hello. Create an instance of the <code>Hello</code> custom resource, with some text:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>hello.example.com/v1alpha1<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Hello<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>example-hello<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">fileName</span>:<span style="color:#bbb"> </span>example.txt<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">fileContents</span>:<span style="color:#bbb"> </span>Hello, world!<span style="color:#bbb"> </span></span></span></code></pre></div><p>Create a PVC that refers to that CR as its data source.</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>PersistentVolumeClaim<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>example-pvc<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">accessModes</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- ReadWriteOnce<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">resources</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">requests</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">storage</span>:<span style="color:#bbb"> </span>10Mi<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">dataSourceRef</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">apiGroup</span>:<span style="color:#bbb"> </span>hello.example.com<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Hello<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>example-hello<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">volumeMode</span>:<span style="color:#bbb"> </span>Filesystem<span style="color:#bbb"> </span></span></span></code></pre></div><p>Next, run a Job that reads the file in the PVC.</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>batch/v1<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Job<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>example-job<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">template</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>example-container<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>busybox:latest<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- cat<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- /mnt/example.txt<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">volumeMounts</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>vol<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span>/mnt<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">restartPolicy</span>:<span style="color:#bbb"> </span>Never<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">volumes</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>vol<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">persistentVolumeClaim</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">claimName</span>:<span style="color:#bbb"> </span>example-pvc<span style="color:#bbb"> </span></span></span></code></pre></div><p>Wait for the job to complete (including all of its dependencies).</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>kubectl <span style="color:#a2f">wait</span> --for<span style="color:#666">=</span><span style="color:#b8860b">condition</span><span style="color:#666">=</span>Complete job/example-job </span></span></code></pre></div><p>And last examine the log from the job.</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>kubectl logs job/example-job </span></span></code></pre></div><p>The output should be:</p> <pre tabindex="0"><code class="language-terminal" data-lang="terminal">Hello, world! </code></pre><p>Note that the volume already contained a text file with the string contents from the CR. This is only the simplest example. Actual populators can set up the volume to contain arbitrary contents.</p> <h2 id="how-to-write-your-own-volume-populator">How to write your own volume populator</h2> <p>Developers interested in writing new poplators are encouraged to use the <a href="https://github.com/kubernetes-csi/lib-volume-populator">lib-volume-populator</a> library and to only supply a small controller wrapper around the library, and a pod image capable of attaching to volumes and writing the appropriate data to the volume.</p> <p>Individual populators can be extremely generic such that they work with every type of PVC, or they can do vendor specific things to rapidly fill a volume with data if the volume was provisioned by a specific CSI driver from the same vendor, for example, by communicating directly with the storage for that volume.</p> <h2 id="how-can-i-learn-more">How can I learn more?</h2> <p>The enhancement proposal, <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1495-volume-populators">Volume Populators</a>, includes lots of detail about the history and technical implementation of this feature.</p> <p><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#volume-populators-and-data-sources">Volume populators and data sources</a>, within the documentation topic about persistent volumes, explains how to use this feature in your cluster.</p> <p>Please get involved by joining the Kubernetes storage SIG to help us enhance this feature. There are a lot of good ideas already and we'd be thrilled to have more!</p></description></item><item><title>Blog: Kubernetes 1.24: gRPC container probes in beta</title><link>https://kubernetes.io/blog/2022/05/13/grpc-probes-now-in-beta/</link><pubDate>Fri, 13 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/05/13/grpc-probes-now-in-beta/</guid><description> <p><strong>Author</strong>: Sergey Kanzhelev (Google)</p> <p>With Kubernetes 1.24 the gRPC probes functionality entered beta and is available by default. Now you can configure startup, liveness, and readiness probes for your gRPC app without exposing any HTTP endpoint, nor do you need an executable. Kubernetes can natively connect to your your workload via gRPC and query its status.</p> <h2 id="some-history">Some history</h2> <p>It's useful to let the system managing your workload check that the app is healthy, has started OK, and whether the app considers itself good to accept traffic. Before the gRPC support was added, Kubernetes already allowed you to check for health based on running an executable from inside the container image, by making an HTTP request, or by checking whether a TCP connection succeeded.</p> <p>For most apps, those checks are enough. If your app provides a gRPC endpoint for a health (or readiness) check, it is easy to repurpose the <code>exec</code> probe to use it for gRPC health checking. In the blog article <a href="https://kubernetes.io/blog/2018/10/01/health-checking-grpc-servers-on-kubernetes/">Health checking gRPC servers on Kubernetes</a>, Ahmet Alp Balkan described how you can do that — a mechanism that still works today.</p> <p>There is a commonly used tool to enable this that was <a href="https://github.com/grpc-ecosystem/grpc-health-probe/commit/2df4478982e95c9a57d5fe3f555667f4365c025d">created</a> on August 21, 2018, and with the first release at <a href="https://github.com/grpc-ecosystem/grpc-health-probe/releases/tag/v0.1.0-alpha.1">Sep 19, 2018</a>.</p> <p>This approach for gRPC apps health checking is very popular. There are <a href="https://github.com/search?l=Dockerfile&amp;q=grpc_health_probe&amp;type=code">3,626 Dockerfiles</a> with the <code>grpc_health_probe</code> and <a href="https://github.com/search?l=YAML&amp;q=grpc_health_probe&amp;type=Code">6,621 yaml</a> files that are discovered with the basic search on GitHub (at the moment of writing). This is good indication of the tool popularity and the need to support this natively.</p> <p>Kubernetes v1.23 introduced an alpha-quality implementation of native support for querying a workload status using gRPC. Because it was an alpha feature, this was disabled by default for the v1.23 release.</p> <h2 id="using-the-feature">Using the feature</h2> <p>We built gRPC health checking in similar way with other probes and believe it will be <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-a-grpc-liveness-probe">easy to use</a> if you are familiar with other probe types in Kubernetes. The natively supported health probe has many benefits over the workaround involving <code>grpc_health_probe</code> executable.</p> <p>With the native gRPC support you don't need to download and carry <code>10MB</code> of an additional executable with your image. Exec probes are generally slower than a gRPC call as they require instantiating a new process to run an executable. It also makes the checks less sensible for edge cases when the pod is running at maximum resources and has troubles instantiating new processes.</p> <p>There are a few limitations though. Since configuring a client certificate for probes is hard, services that require client authentication are not supported. The built-in probes are also not checking the server certificates and ignore related problems.</p> <p>Built-in checks also cannot be configured to ignore certain types of errors (<code>grpc_health_probe</code> returns different exit codes for different errors), and cannot be &quot;chained&quot; to run the health check on multiple services in a single probe.</p> <p>But all these limitations are quite standard for gRPC and there are easy workarounds for those.</p> <h2 id="try-it-for-yourself">Try it for yourself</h2> <h3 id="cluster-level-setup">Cluster-level setup</h3> <p>You can try this feature today. To try native gRPC probes, you can spin up a Kubernetes cluster yourself with the <code>GRPCContainerProbe</code> feature gate enabled, there are many <a href="https://kubernetes.io/docs/tasks/tools/">tools available</a>.</p> <p>Since the feature gate <code>GRPCContainerProbe</code> is enabled by default in 1.24, many vendors will have this functionality working out of the box. So you may just create an 1.24 cluster on platform of your choice. Some vendors allow to enable alpha features on 1.23 clusters.</p> <p>For example, at the moment of writing, you can spin up the test cluster on GKE for a quick test. Other vendors may also have similar capabilities, especially if you are reading this blog post long after the Kubernetes 1.24 release.</p> <p>On GKE use the following command (note, version is <code>1.23</code> and <code>enable-kubernetes-alpha</code> are specified).</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>gcloud container clusters create test-grpc <span style="color:#b62;font-weight:bold">\ </span></span></span><span style="display:flex;"><span><span style="color:#b62;font-weight:bold"></span> --enable-kubernetes-alpha <span style="color:#b62;font-weight:bold">\ </span></span></span><span style="display:flex;"><span><span style="color:#b62;font-weight:bold"></span> --no-enable-autorepair <span style="color:#b62;font-weight:bold">\ </span></span></span><span style="display:flex;"><span><span style="color:#b62;font-weight:bold"></span> --no-enable-autoupgrade <span style="color:#b62;font-weight:bold">\ </span></span></span><span style="display:flex;"><span><span style="color:#b62;font-weight:bold"></span> --release-channel<span style="color:#666">=</span>rapid <span style="color:#b62;font-weight:bold">\ </span></span></span><span style="display:flex;"><span><span style="color:#b62;font-weight:bold"></span> --cluster-version<span style="color:#666">=</span>1.23 </span></span></code></pre></div><p>You will also need to configure <code>kubectl</code> to access the cluster:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>gcloud container clusters get-credentials test-grpc </span></span></code></pre></div><h3 id="trying-the-feature-out">Trying the feature out</h3> <p>Let's create the pod to test how gRPC probes work. For this test we will use the <code>agnhost</code> image. This is a k8s maintained image with that can be used for all sorts of workload testing. For example, it has a useful <a href="https://github.com/kubernetes/kubernetes/blob/b2c5bd2a278288b5ef19e25bf7413ecb872577a4/test/images/agnhost/README.md#grpc-health-checking">grpc-health-checking</a> module that exposes two ports - one is serving health checking service, another - http port to react on commands <code>make-serving</code> and <code>make-not-serving</code>.</p> <p>Here is an example pod definition. It starts the <code>grpc-health-checking</code> module, exposes ports <code>5000</code> and <code>8080</code>, and configures gRPC readiness probe:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#00f;font-weight:bold">---</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">metadata</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>test-grpc<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"></span><span style="color:#008000;font-weight:bold">spec</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">containers</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- <span style="color:#008000;font-weight:bold">name</span>:<span style="color:#bbb"> </span>agnhost<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span>k8s.gcr.io/e2e-test-images/agnhost:2.35<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">command</span>:<span style="color:#bbb"> </span>[<span style="color:#b44">&#34;/agnhost&#34;</span>,<span style="color:#bbb"> </span><span style="color:#b44">&#34;grpc-health-checking&#34;</span>]<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">ports</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">5000</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- <span style="color:#008000;font-weight:bold">containerPort</span>:<span style="color:#bbb"> </span><span style="color:#666">8080</span><span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">readinessProbe</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">grpc</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">port</span>:<span style="color:#bbb"> </span><span style="color:#666">5000</span><span style="color:#bbb"> </span></span></span></code></pre></div><p>If the file called <code>test.yaml</code>, you can create the pod and check it's status. The pod will be in ready state as indicated by the snippet of the output.</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>kubectl apply -f test.yaml </span></span><span style="display:flex;"><span>kubectl describe test-grpc </span></span></code></pre></div><p>The output will contain something like this:</p> <pre tabindex="0"><code>Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True </code></pre><p>Now let's change the health checking endpoint status to NOT_SERVING. In order to call the http port of the Pod, let's create a port forward:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>kubectl port-forward test-grpc 8080:8080 </span></span></code></pre></div><p>You can <code>curl</code> to call the command...</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>curl http://localhost:8080/make-not-serving </span></span></code></pre></div><p>... and in a few seconds the port status will switch to not ready.</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>kubectl describe pod test-grpc </span></span></code></pre></div><p>The output now will have:</p> <pre tabindex="0"><code>Conditions: Type Status Initialized True Ready False ContainersReady False PodScheduled True ... Warning Unhealthy 2s (x6 over 42s) kubelet Readiness probe failed: service unhealthy (responded with &#34;NOT_SERVING&#34;) </code></pre><p>Once it is switched back, in about one second the Pod will get back to ready status:</p> <pre tabindex="0"><code class="language-bsh" data-lang="bsh">curl http://localhost:8080/make-serving kubectl describe test-grpc </code></pre><p>The output indicates that the Pod went back to being <code>Ready</code>:</p> <pre tabindex="0"><code>Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True </code></pre><p>This new built-in gRPC health probing on Kubernetes makes implementing a health-check via gRPC much easier than the older approach that relied on using a separate <code>exec</code> probe. Read through the official <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-a-grpc-liveness-probe">documentation</a> to learn more and provide feedback before the feature will be promoted to GA.</p> <h2 id="summary">Summary</h2> <p>Kubernetes is a popular workload orchestration platform and we add features based on feedback and demand. Features like gRPC probes support is a minor improvement that will make life of many app developers easier and apps more resilient. Try it today and give feedback, before the feature went into GA.</p></description></item><item><title>Blog: Kubernetes 1.24: Storage Capacity Tracking Now Generally Available</title><link>https://kubernetes.io/blog/2022/05/06/storage-capacity-ga/</link><pubDate>Fri, 06 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/05/06/storage-capacity-ga/</guid><description> <p><strong>Authors:</strong> Patrick Ohly (Intel)</p> <p>The v1.24 release of Kubernetes brings <a href="https://kubernetes.io/docs/concepts/storage/storage-capacity/">storage capacity</a> tracking as a generally available feature.</p> <h2 id="problems-we-have-solved">Problems we have solved</h2> <p>As explained in more detail in the <a href="https://kubernetes.io/blog/2021/04/14/local-storage-features-go-beta/">previous blog post about this feature</a>, storage capacity tracking allows a CSI driver to publish information about remaining capacity. The kube-scheduler then uses that information to pick suitable nodes for a Pod when that Pod has volumes that still need to be provisioned.</p> <p>Without this information, a Pod may get stuck without ever being scheduled onto a suitable node because kube-scheduler has to choose blindly and always ends up picking a node for which the volume cannot be provisioned because the underlying storage system managed by the CSI driver does not have sufficient capacity left.</p> <p>Because CSI drivers publish storage capacity information that gets used at a later time when it might not be up-to-date anymore, it can still happen that a node is picked that doesn't work out after all. Volume provisioning recovers from that by informing the scheduler that it needs to try again with a different node.</p> <p><a href="https://github.com/kubernetes-csi/csi-driver-host-path/blob/master/docs/storage-capacity-tracking.md">Load tests</a> that were done again for promotion to GA confirmed that all storage in a cluster can be consumed by Pods with storage capacity tracking whereas Pods got stuck without it.</p> <h2 id="problems-we-have-not-solved">Problems we have <em>not</em> solved</h2> <p>Recovery from a failed volume provisioning attempt has one known limitation: if a Pod uses two volumes and only one of them could be provisioned, then all future scheduling decisions are limited by the already provisioned volume. If that volume is local to a node and the other volume cannot be provisioned there, the Pod is stuck. This problem pre-dates storage capacity tracking and while the additional information makes it less likely to occur, it cannot be avoided in all cases, except of course by only using one volume per Pod.</p> <p>An idea for solving this was proposed in a <a href="https://github.com/kubernetes/enhancements/pull/1703">KEP draft</a>: volumes that were provisioned and haven't been used yet cannot have any valuable data and therefore could be freed and provisioned again elsewhere. SIG Storage is looking for interested developers who want to continue working on this.</p> <p>Also not solved is support in Cluster Autoscaler for Pods with volumes. For CSI drivers with storage capacity tracking, a prototype was developed and discussed in <a href="https://github.com/kubernetes/autoscaler/pull/3887">a PR</a>. It was meant to work with arbitrary CSI drivers, but that flexibility made it hard to configure and slowed down scale up operations: because autoscaler was unable to simulate volume provisioning, it only scaled the cluster by one node at a time, which was seen as insufficient.</p> <p>Therefore that PR was not merged and a different approach with tighter coupling between autoscaler and CSI driver will be needed. For this a better understanding is needed about which local storage CSI drivers are used in combination with cluster autoscaling. Should this lead to a new KEP, then users will have to try out an implementation in practice before it can move to beta or GA. So please reach out to SIG Storage if you have an interest in this topic.</p> <h2 id="acknowledgements">Acknowledgements</h2> <p>Thanks a lot to the members of the community who have contributed to this feature or given feedback including members of <a href="https://github.com/kubernetes/community/tree/master/sig-scheduling">SIG Scheduling</a>, <a href="https://github.com/kubernetes/community/tree/master/sig-autoscaling">SIG Autoscaling</a>, and of course <a href="https://github.com/kubernetes/community/tree/master/sig-storage">SIG Storage</a>!</p></description></item><item><title>Blog: Kubernetes 1.24: Volume Expansion Now A Stable Feature</title><link>https://kubernetes.io/blog/2022/05/05/volume-expansion-ga/</link><pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/05/05/volume-expansion-ga/</guid><description> <p><strong>Author:</strong> Hemant Kumar (Red Hat)</p> <p>Volume expansion was introduced as a alpha feature in Kubernetes 1.8 and it went beta in 1.11 and with Kubernetes 1.24 we are excited to announce general availability(GA) of volume expansion.</p> <p>This feature allows Kubernetes users to simply edit their <code>PersistentVolumeClaim</code> objects and specify new size in PVC Spec and Kubernetes will automatically expand the volume using storage backend and also expand the underlying file system in-use by the Pod without requiring any downtime at all if possible.</p> <h3 id="how-to-use-volume-expansion">How to use volume expansion</h3> <p>You can trigger expansion for a PersistentVolume by editing the <code>spec</code> field of a PVC, specifying a different (and larger) storage request. For example, given following PVC:</p> <pre tabindex="0"><code>kind: PersistentVolumeClaim apiVersion: v1 metadata: name: myclaim spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi # specify new size here </code></pre><p>You can request expansion of the underlying PersistentVolume by specifying a new value instead of old <code>1Gi</code> size. Once you've changed the requested size, watch the <code>status.conditions</code> field of the PVC to see if the resize has completed.</p> <p>When Kubernetes starts expanding the volume - it will add <code>Resizing</code> condition to the PVC, which will be removed once expansion completes. More information about progress of expansion operation can also be obtained by monitoring events associated with the PVC:</p> <pre tabindex="0"><code>kubectl describe pvc &lt;pvc&gt; </code></pre><h3 id="storage-driver-support">Storage driver support</h3> <p>Not every volume type however is expandable by default. Some volume types such as - intree hostpath volumes are not expandable at all. For CSI volumes - the CSI driver must have capability <code>EXPAND_VOLUME</code> in controller or node service (or both if appropriate). Please refer to documentation of your CSI driver, to find out if it supports volume expansion.</p> <p>Please refer to volume expansion documentation for intree volume types which support volume expansion - <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#expanding-persistent-volumes-claims">Expanding Persistent Volumes</a>.</p> <p>In general to provide some degree of control over volumes that can be expanded, only dynamically provisioned PVCs whose storage class has <code>allowVolumeExpansion</code> parameter set to <code>true</code> are expandable.</p> <p>A Kubernetes cluster administrator must edit the appropriate StorageClass object and set the <code>allowVolumeExpansion</code> field to <code>true</code>. For example:</p> <pre tabindex="0"><code>apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: gp2-default provisioner: kubernetes.io/aws-ebs parameters: secretNamespace: &#34;&#34; secretName: &#34;&#34; allowVolumeExpansion: true </code></pre><h3 id="online-expansion-compared-to-offline-expansion">Online expansion compared to offline expansion</h3> <p>By default, Kubernetes attempts to expand volumes immediately after user requests a resize. If one or more Pods are using the volume, Kubernetes tries to expands the volume using an online resize; as a result volume expansion usually requires no application downtime. Filesystem expansion on the node is also performed online and hence does not require shutting down any Pod that was using the PVC.</p> <p>If you expand a PersistentVolume that is not in use, Kubernetes does an offline resize (and, because the volume isn't in use, there is again no workload disruption).</p> <p>In some cases though - if underlying Storage Driver can only support offline expansion, users of the PVC must take down their Pod before expansion can succeed. Please refer to documentation of your storage provider to find out - what mode of volume expansion it supports.</p> <p>When volume expansion was introduced as an alpha feature, Kubernetes only supported offline filesystem expansion on the node and hence required users to restart their pods for file system resizing to finish. His behaviour has been changed and Kubernetes tries its best to fulfil any resize request regardless of whether the underlying PersistentVolume volume is online or offline. If your storage provider supports online expansion then no Pod restart should be necessary for volume expansion to finish.</p> <h2 id="next-steps">Next steps</h2> <p>Although volume expansion is now stable as part of the recent v1.24 release, SIG Storage are working to make it even simpler for users of Kubernetes to expand their persistent storage. Kubernetes 1.23 introduced features for triggering recovery from failed volume expansion, allowing users to attempt self-service healing after a failed resize. See <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#recovering-from-failure-when-expanding-volumes">Recovering from volume expansion failure</a> for more details.</p> <p>The Kubernetes contributor community is also discussing the potential for StatefulSet-driven storage expansion. This proposed feature would let you trigger expansion for all underlying PVs that are providing storage to a StatefulSet, by directly editing the StatefulSet object. See the <a href="https://github.com/kubernetes/enhancements/issues/661">Support Volume Expansion Through StatefulSets</a> enhancement proposal for more details.</p></description></item><item><title>Blog: Dockershim: The Historical Context</title><link>https://kubernetes.io/blog/2022/05/03/dockershim-historical-context/</link><pubDate>Tue, 03 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/05/03/dockershim-historical-context/</guid><description> <p><strong>Author:</strong> Kat Cosgrove</p> <p>Dockershim has been removed as of Kubernetes v1.24, and this is a positive move for the project. However, context is important for fully understanding something, be it socially or in software development, and this deserves a more in-depth review. Alongside the dockershim removal in Kubernetes v1.24, we’ve seen some confusion (sometimes at a panic level) and dissatisfaction with this decision in the community, largely due to a lack of context around this removal. The decision to deprecate and eventually remove dockershim from Kubernetes was not made quickly or lightly. Still, it’s been in the works for so long that many of today’s users are newer than that decision, and certainly newer than the choices that led to the dockershim being necessary in the first place.</p> <p>So what is the dockershim, and why is it going away?</p> <p>In the early days of Kubernetes, we only supported one container runtime. That runtime was Docker Engine. Back then, there weren’t really a lot of other options out there and Docker was the dominant tool for working with containers, so this was not a controversial choice. Eventually, we started adding more container runtimes, like rkt and hypernetes, and it became clear that Kubernetes users want a choice of runtimes working best for them. So Kubernetes needed a way to allow cluster operators the flexibility to use whatever runtime they choose.</p> <p>The <a href="https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/">Container Runtime Interface</a> (CRI) was released to allow that flexibility. The introduction of CRI was great for the project and users alike, but it did introduce a problem: Docker Engine’s use as a container runtime predates CRI, and Docker Engine is not CRI-compatible. To solve this issue, a small software shim (dockershim) was introduced as part of the kubelet component specifically to fill in the gaps between Docker Engine and CRI, allowing cluster operators to continue using Docker Engine as their container runtime largely uninterrupted.</p> <p>However, this little software shim was never intended to be a permanent solution. Over the course of years, its existence has introduced a lot of unnecessary complexity to the kubelet itself. Some integrations are inconsistently implemented for Docker because of this shim, resulting in an increased burden on maintainers, and maintaining vendor-specific code is not in line with our open source philosophy. To reduce this maintenance burden and move towards a more collaborative community in support of open standards, <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim">KEP-2221 was introduced</a>, proposing the removal of the dockershim. With the release of Kubernetes v1.20, the deprecation was official.</p> <p>We didn’t do a great job communicating this, and unfortunately, the deprecation announcement led to some panic within the community. Confusion around what this meant for Docker as a company, if container images built by Docker would still run, and what Docker Engine actually is led to a conflagration on social media. This was our fault; we should have more clearly communicated what was happening and why at the time. To combat this, we released <a href="https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/">a blog</a> and <a href="https://kubernetes.io/blog/2020/12/02/dockershim-faq/">accompanying FAQ</a> to allay the community’s fears and correct some misconceptions about what Docker is and how containers work within Kubernetes. As a result of the community’s concerns, Docker and Mirantis jointly agreed to continue supporting the dockershim code in the form of <a href="https://www.mirantis.com/blog/the-future-of-dockershim-is-cri-dockerd/">cri-dockerd</a>, allowing you to continue using Docker Engine as your container runtime if need be. For the interest of users who want to try other runtimes, like containerd or cri-o, <a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/change-runtime-containerd/">migration documentation was written</a>.</p> <p>We later <a href="https://kubernetes.io/blog/2021/11/12/are-you-ready-for-dockershim-removal/">surveyed the community</a> and <a href="https://kubernetes.io/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim">discovered that there are still many users with questions and concerns</a>. In response, Kubernetes maintainers and the CNCF committed to addressing these concerns by extending documentation and other programs. In fact, this blog post is a part of this program. With so many end users successfully migrated to other runtimes, and improved documentation, we believe that everyone has a paved way to migration now.</p> <p>Docker is not going away, either as a tool or as a company. It’s an important part of the cloud native community and the history of the Kubernetes project. We wouldn’t be where we are without them. That said, removing dockershim from kubelet is ultimately good for the community, the ecosystem, the project, and open source at large. This is an opportunity for all of us to come together to support open standards, and we’re glad to be doing so with the help of Docker and the community.</p></description></item><item><title>Blog: Kubernetes 1.24: Stargazer</title><link>https://kubernetes.io/blog/2022/05/03/kubernetes-1-24-release-announcement/</link><pubDate>Tue, 03 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/05/03/kubernetes-1-24-release-announcement/</guid><description> <p><strong>Authors</strong>: <a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.24/release-team.md">Kubernetes 1.24 Release Team</a></p> <p>We are excited to announce the release of Kubernetes 1.24, the first release of 2022!</p> <p>This release consists of 46 enhancements: fourteen enhancements have graduated to stable, fifteen enhancements are moving to beta, and thirteen enhancements are entering alpha. Also, two features have been deprecated, and two features have been removed.</p> <h2 id="major-themes">Major Themes</h2> <h3 id="dockershim-removed-from-kubelet">Dockershim Removed from kubelet</h3> <p>After its deprecation in v1.20, the dockershim component has been removed from the kubelet in Kubernetes v1.24. From v1.24 onwards, you will need to either use one of the other <a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/">supported runtimes</a> (such as containerd or CRI-O) or use cri-dockerd if you are relying on Docker Engine as your container runtime. For more information about ensuring your cluster is ready for this removal, please see <a href="https://kubernetes.io/blog/2022/03/31/ready-for-dockershim-removal/">this guide</a>.</p> <h3 id="beta-apis-off-by-default">Beta APIs Off by Default</h3> <p><a href="https://github.com/kubernetes/enhancements/issues/3136">New beta APIs will not be enabled in clusters by default</a>. Existing beta APIs and new versions of existing beta APIs will continue to be enabled by default.</p> <h3 id="signing-release-artifacts">Signing Release Artifacts</h3> <p>Release artifacts are <a href="https://github.com/kubernetes/enhancements/issues/3031">signed</a> using <a href="https://github.com/sigstore/cosign">cosign</a> signatures, and there is experimental support for <a href="https://kubernetes.io/docs/tasks/administer-cluster/verify-signed-images/">verifying image signatures</a>. Signing and verification of release artifacts is part of <a href="https://github.com/kubernetes/enhancements/issues/3027">increasing software supply chain security for the Kubernetes release process</a>.</p> <h3 id="openapi-v3">OpenAPI v3</h3> <p>Kubernetes 1.24 offers beta support for publishing its APIs in the <a href="https://github.com/kubernetes/enhancements/issues/2896">OpenAPI v3 format</a>.</p> <h3 id="storage-capacity-and-volume-expansion-are-generally-available">Storage Capacity and Volume Expansion Are Generally Available</h3> <p><a href="https://github.com/kubernetes/enhancements/issues/1472">Storage capacity tracking</a> supports exposing currently available storage capacity via <a href="https://kubernetes.io/docs/concepts/storage/storage-capacity/#api">CSIStorageCapacity objects</a> and enhances scheduling of pods that use CSI volumes with late binding.</p> <p><a href="https://github.com/kubernetes/enhancements/issues/284">Volume expansion</a> adds support for resizing existing persistent volumes.</p> <h3 id="nonpreemptingpriority-to-stable">NonPreemptingPriority to Stable</h3> <p>This feature adds <a href="https://github.com/kubernetes/enhancements/issues/902">a new option to PriorityClasses</a>, which can enable or disable pod preemption.</p> <h3 id="storage-plugin-migration">Storage Plugin Migration</h3> <p>Work is underway to <a href="https://github.com/kubernetes/enhancements/issues/625">migrate the internals of in-tree storage plugins</a> to call out to CSI Plugins while maintaining the original API. The <a href="https://github.com/kubernetes/enhancements/issues/1490">Azure Disk</a> and <a href="https://github.com/kubernetes/enhancements/issues/1489">OpenStack Cinder</a> plugins have both been migrated.</p> <h3 id="grpc-probes-graduate-to-beta">gRPC Probes Graduate to Beta</h3> <p>With Kubernetes 1.24, the <a href="https://github.com/kubernetes/enhancements/issues/2727">gRPC probes functionality</a> has entered beta and is available by default. You can now <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes">configure startup, liveness, and readiness probes</a> for your gRPC app natively within Kubernetes without exposing an HTTP endpoint or using an extra executable.</p> <h3 id="kubelet-credential-provider-graduates-to-beta">Kubelet Credential Provider Graduates to Beta</h3> <p>Originally released as Alpha in Kubernetes 1.20, the kubelet's support for <a href="https://kubernetes.io/docs/tasks/kubelet-credential-provider/kubelet-credential-provider/">image credential providers</a> has now graduated to Beta. This allows the kubelet to dynamically retrieve credentials for a container image registry using exec plugins rather than storing credentials on the node's filesystem.</p> <h3 id="contextual-logging-in-alpha">Contextual Logging in Alpha</h3> <p>Kubernetes 1.24 has introduced <a href="https://github.com/kubernetes/enhancements/issues/3077">contextual logging</a> that enables the caller of a function to control all aspects of logging (output formatting, verbosity, additional values, and names).</p> <h3 id="avoiding-collisions-in-ip-allocation-to-services">Avoiding Collisions in IP allocation to Services</h3> <p>Kubernetes 1.24 introduces a new opt-in feature that allows you to <a href="https://kubernetes.io/docs/concepts/services-networking/service/#service-ip-static-sub-range">soft-reserve a range for static IP address assignments</a> to Services. With the manual enablement of this feature, the cluster will prefer automatic assignment from the pool of Service IP addresses, thereby reducing the risk of collision.</p> <p>A Service <code>ClusterIP</code> can be assigned:</p> <ul> <li>dynamically, which means the cluster will automatically pick a free IP within the configured Service IP range.</li> <li>statically, which means the user will set one IP within the configured Service IP range.</li> </ul> <p>Service <code>ClusterIP</code> are unique; hence, trying to create a Service with a <code>ClusterIP</code> that has already been allocated will return an error.</p> <h3 id="dynamic-kubelet-configuration-is-removed-from-the-kubelet">Dynamic Kubelet Configuration is Removed from the Kubelet</h3> <p>After being deprecated in Kubernetes 1.22, Dynamic Kubelet Configuration has been removed from the kubelet. The feature will be removed from the API server in Kubernetes 1.26.</p> <h2 id="cni-version-related-breaking-change">CNI Version-Related Breaking Change</h2> <p>Before you upgrade to Kubernetes 1.24, please verify that you are using/upgrading to a container runtime that has been tested to work correctly with this release.</p> <p>For example, the following container runtimes are being prepared, or have already been prepared, for Kubernetes:</p> <ul> <li>containerd v1.6.4 and later, v1.5.11 and later</li> <li>CRI-O 1.24 and later</li> </ul> <p>Service issues exist for pod CNI network setup and tear down in containerd v1.6.0–v1.6.3 when the CNI plugins have not been upgraded and/or the CNI config version is not declared in the CNI config files. The containerd team reports, &quot;these issues are resolved in containerd v1.6.4.&quot;</p> <p>With containerd v1.6.0–v1.6.3, if you do not upgrade the CNI plugins and/or declare the CNI config version, you might encounter the following &quot;Incompatible CNI versions&quot; or &quot;Failed to destroy network for sandbox&quot; error conditions.</p> <h2 id="csi-snapshot">CSI Snapshot</h2> <p><em>This information was added after initial publication.</em></p> <p><a href="https://github.com/kubernetes/enhancements/issues/177">VolumeSnapshot v1beta1 CRD has been removed</a>. Volume snapshot and restore functionality for Kubernetes and the Container Storage Interface (CSI), which provides standardized APIs design (CRDs) and adds PV snapshot/restore support for CSI volume drivers, moved to GA in v1.20. VolumeSnapshot v1beta1 was deprecated in v1.20 and is now unsupported. Refer to <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/177-volume-snapshot#kep-177-csi-snapshot">KEP-177: CSI Snapshot</a> and <a href="https://kubernetes.io/blog/2020/12/10/kubernetes-1.20-volume-snapshot-moves-to-ga/">Volume Snapshot GA blog</a> for more information.</p> <h2 id="other-updates">Other Updates</h2> <h3 id="graduations-to-stable">Graduations to Stable</h3> <p>This release saw fourteen enhancements promoted to stable:</p> <ul> <li><a href="https://github.com/kubernetes/enhancements/issues/284">Container Storage Interface (CSI) Volume Expansion</a></li> <li><a href="https://github.com/kubernetes/enhancements/issues/688">Pod Overhead</a>: Account for resources tied to the pod sandbox but not specific containers.</li> <li><a href="https://github.com/kubernetes/enhancements/issues/902">Add non-preempting option to PriorityClasses</a></li> <li><a href="https://github.com/kubernetes/enhancements/issues/1472">Storage Capacity Tracking</a></li> <li><a href="https://github.com/kubernetes/enhancements/issues/1489">OpenStack Cinder In-Tree to CSI Driver Migration</a></li> <li><a href="https://github.com/kubernetes/enhancements/issues/1490">Azure Disk In-Tree to CSI Driver Migration</a></li> <li><a href="https://github.com/kubernetes/enhancements/issues/1904">Efficient Watch Resumption</a>: Watch can be efficiently resumed after kube-apiserver reboot.</li> <li><a href="https://github.com/kubernetes/enhancements/issues/1959">Service Type=LoadBalancer Class Field</a>: Introduce a new Service annotation <code>service.kubernetes.io/load-balancer-class</code> that allows multiple implementations of <code>type: LoadBalancer</code> Services in the same cluster.</li> <li><a href="https://github.com/kubernetes/enhancements/issues/2214">Indexed Job</a>: Add a completion index to Pods of Jobs with a fixed completion count.</li> <li><a href="https://github.com/kubernetes/enhancements/issues/2232">Add Suspend Field to Jobs API</a>: Add a suspend field to the Jobs API to allow orchestrators to create jobs with more control over when pods are created.</li> <li><a href="https://github.com/kubernetes/enhancements/issues/2249">Pod Affinity NamespaceSelector</a>: Add a <code>namespaceSelector</code> field for to pod affinity/anti-affinity spec.</li> <li><a href="https://github.com/kubernetes/enhancements/issues/2436">Leader Migration for Controller Managers</a>: kube-controller-manager and cloud-controller-manager can apply new controller-to-controller-manager assignment in HA control plane without downtime.</li> <li><a href="https://github.com/kubernetes/enhancements/issues/2784">CSR Duration</a>: Extend the CertificateSigningRequest API with a mechanism to allow clients to request a specific duration for the issued certificate.</li> </ul> <h3 id="major-changes">Major Changes</h3> <p>This release saw two major changes:</p> <ul> <li><a href="https://github.com/kubernetes/enhancements/issues/2221">Dockershim Removal</a></li> <li><a href="https://github.com/kubernetes/enhancements/issues/3136">Beta APIs are off by Default</a></li> </ul> <h3 id="release-notes">Release Notes</h3> <p>Check out the full details of the Kubernetes 1.24 release in our <a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md">release notes</a>.</p> <h3 id="availability">Availability</h3> <p>Kubernetes 1.24 is available for download on <a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.24.0">GitHub</a>. To get started with Kubernetes, check out these <a href="https://kubernetes.io/docs/tutorials/">interactive tutorials</a> or run local Kubernetes clusters using containers as “nodes”, with <a href="https://kind.sigs.k8s.io/">kind</a>. You can also easily install 1.24 using <a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">kubeadm</a>.</p> <h3 id="release-team">Release Team</h3> <p>This release would not have been possible without the combined efforts of committed individuals comprising the Kubernetes 1.24 release team. This team came together to deliver all of the components that go into each Kubernetes release, including code, documentation, release notes, and more.</p> <p>Special thanks to James Laverack, our release lead, for guiding us through a successful release cycle, and to all of the release team members for the time and effort they put in to deliver the v1.24 release for the Kubernetes community.</p> <h3 id="release-theme-and-logo">Release Theme and Logo</h3> <p><strong>Kubernetes 1.24: Stargazer</strong></p> <figure class="release-logo"> <img src="https://kubernetes.io/images/blog/2022-05-03-kubernetes-release-1.24/kubernetes-1.24.png"/> </figure> <p>The theme for Kubernetes 1.24 is <em>Stargazer</em>.</p> <p>Generations of people have looked to the stars in awe and wonder, from ancient astronomers to the scientists who built the James Webb Space Telescope. The stars have inspired us, set our imagination alight, and guided us through long nights on difficult seas.</p> <p>With this release we gaze upwards, to what is possible when our community comes together. Kubernetes is the work of hundreds of contributors across the globe and thousands of end-users supporting applications that serve millions. Every one is a star in our sky, helping us chart our course.</p> <p>The release logo is made by <a href="https://www.instagram.com/artsyfie/">Britnee Laverack</a>, and depicts a telescope set upon starry skies and the <a href="https://en.wikipedia.org/wiki/Pleiades">Pleiades</a>, often known in mythology as the “Seven Sisters”. The number seven is especially auspicious for the Kubernetes project, and is a reference back to our original “Project Seven” name.</p> <p>This release of Kubernetes is named for those that would look towards the night sky and wonder — for all the stargazers out there. ✨</p> <h3 id="user-highlights">User Highlights</h3> <ul> <li>Check out how leading retail e-commerce company <a href="https://www.cncf.io/case-studies/la-redoute/">La Redoute used Kubernetes, alongside other CNCF projects, to transform and streamline its software delivery lifecycle</a> - from development to operations.</li> <li>Trying to ensure no change to an API call would cause any breaks, <a href="https://www.cncf.io/case-studies/salt-security/">Salt Security built its microservices entirely on Kubernetes, and it communicates via gRPC while Linkerd ensures messages are encrypted</a>.</li> <li>In their effort to migrate from private to public cloud, <a href="https://www.cncf.io/case-studies/allianz/">Allainz Direct engineers redesigned its CI/CD pipeline in just three months while managing to condense 200 workflows down to 10-15</a>.</li> <li>Check out how <a href="https://www.cncf.io/case-studies/bink/">Bink, a UK based fintech company, updated its in-house Kubernetes distribution with Linkerd to build a cloud-agnostic platform that scales as needed whilst allowing them to keep a close eye on performance and stability</a>.</li> <li>Using Kubernetes, the Dutch organization <a href="http://www.stichtingopennederland.nl/">Stichting Open Nederland</a> created a testing portal in just one-and-a-half months to help safely reopen events in the Netherlands. The <a href="https://www.testenvoortoegang.org/">Testing for Entry (Testen voor Toegang)</a> platform <a href="https://www.cncf.io/case-studies/true/">leveraged the performance and scalability of Kubernetes to help individuals book over 400,000 COVID-19 testing appointments per day. </a></li> <li>Working alongside SparkFabrik and utilizing Backstage, <a href="https://www.cncf.io/case-studies/santagostino/">Santagostino created the developer platform Samaritan to centralize services and documentation, manage the entire lifecycle of services, and simplify the work of Santagostino developers</a>.</li> </ul> <h3 id="ecosystem-updates">Ecosystem Updates</h3> <ul> <li>KubeCon + CloudNativeCon Europe 2022 will take place in Valencia, Spain, from 16 – 20 May 2022! You can find more information about the conference and registration on the <a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/">event site</a>.</li> <li>In the <a href="https://www.cncf.io/announcements/2022/02/10/cncf-sees-record-kubernetes-and-container-adoption-in-2021-cloud-native-survey/">2021 Cloud Native Survey</a>, the CNCF saw record Kubernetes and container adoption. Take a look at the <a href="https://www.cncf.io/reports/cncf-annual-survey-2021/">results of the survey</a>.</li> <li>The <a href="https://www.linuxfoundation.org/">Linux Foundation</a> and <a href="https://www.cncf.io/">The Cloud Native Computing Foundation</a> (CNCF) announced the availability of a new <a href="https://training.linuxfoundation.org/training/cloudnativedev-bootcamp/?utm_source=lftraining&amp;utm_medium=pr&amp;utm_campaign=clouddevbc0322">Cloud Native Developer Bootcamp</a> to provide participants with the knowledge and skills to design, build, and deploy cloud native applications. Check out the <a href="https://www.cncf.io/announcements/2022/03/15/new-cloud-native-developer-bootcamp-provides-a-clear-path-to-cloud-native-careers/">announcement</a> to learn more.</li> </ul> <h3 id="project-velocity">Project Velocity</h3> <p>The <a href="https://k8s.devstats.cncf.io/d/12/dashboards?orgId=1&amp;refresh=15m">CNCF K8s DevStats</a> project aggregates a number of interesting data points related to the velocity of Kubernetes and various sub-projects. This includes everything from individual contributions to the number of companies that are contributing, and is an illustration of the depth and breadth of effort that goes into evolving this ecosystem.</p> <p>In the v1.24 release cycle, which <a href="https://github.com/kubernetes/sig-release/tree/master/releases/release-1.24">ran for 17 weeks</a> (January 10 to May 3), we saw contributions from <a href="https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;var-period_name=v1.23.0%20-%20v1.24.0&amp;var-metric=contributions">1029 companies</a> and <a href="https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;var-period_name=v1.23.0%20-%20v1.24.0&amp;var-metric=contributions&amp;var-repogroup_name=Kubernetes&amp;var-country_name=All&amp;var-companies=All&amp;var-repo_name=kubernetes%2Fkubernetes">1179 individuals</a>.</p> <h2 id="upcoming-release-webinar">Upcoming Release Webinar</h2> <p>Join members of the Kubernetes 1.24 release team on Tue May 24, 2022 9:45am – 11am PT to learn about the major features of this release, as well as deprecations and removals to help plan for upgrades. For more information and registration, visit the <a href="https://community.cncf.io/e/mck3kd/">event page</a> on the CNCF Online Programs site.</p> <h2 id="get-involved">Get Involved</h2> <p>The simplest way to get involved with Kubernetes is by joining one of the many <a href="https://github.com/kubernetes/community/blob/master/sig-list.md">Special Interest Groups</a> (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly <a href="https://github.com/kubernetes/community/tree/master/communication">community meeting</a>, and through the channels below:</p> <ul> <li>Find out more about contributing to Kubernetes at the <a href="https://www.kubernetes.dev/">Kubernetes Contributors</a> website</li> <li>Follow us on Twitter <a href="https://twitter.com/kubernetesio">@Kubernetesio</a> for the latest updates</li> <li>Join the community discussion on <a href="https://discuss.kubernetes.io/">Discuss</a></li> <li>Join the community on <a href="http://slack.k8s.io/">Slack</a></li> <li>Post questions (or answer questions) on <a href="https://serverfault.com/questions/tagged/kubernetes">Server Fault</a>.</li> <li>Share your Kubernetes <a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">story</a></li> <li>Read more about what’s happening with Kubernetes on the <a href="https://kubernetes.io/blog/">blog</a></li> <li>Learn more about the <a href="https://github.com/kubernetes/sig-release/tree/master/release-team">Kubernetes Release Team</a></li> </ul></description></item><item><title>Blog: Frontiers, fsGroups and frogs: the Kubernetes 1.23 release interview</title><link>https://kubernetes.io/blog/2022/04/29/frontiers-fsgroups-and-frogs-the-kubernetes-1.23-release-interview/</link><pubDate>Fri, 29 Apr 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/04/29/frontiers-fsgroups-and-frogs-the-kubernetes-1.23-release-interview/</guid><description> <p><strong>Author</strong>: Craig Box (Google)</p> <p>One of the highlights of hosting the weekly <a href="https://kubernetespodcast.com/">Kubernetes Podcast from Google</a> is talking to the release managers for each new Kubernetes version. The release team is constantly refreshing. Many working their way from small documentation fixes, step up to shadow roles, and then eventually lead a release.</p> <p>As we prepare for the 1.24 release next week, <a href="https://www.google.com/search?q=%22release+interview%22+site%3Akubernetes.io%2Fblog">in accordance with long-standing tradition</a>, I'm pleased to bring you a look back at the story of 1.23. The release was led by <a href="https://twitter.com/reylejano">Rey Lejano</a>, a Field Engineer at SUSE. <a href="https://kubernetespodcast.com/episode/167-kubernetes-1.23/">I spoke to Rey</a> in December, as he was awaiting the birth of his first child.</p> <p>Make sure you <a href="https://kubernetespodcast.com/subscribe/">subscribe, wherever you get your podcasts</a>, so you hear all our stories from the Cloud Native community, including the story of 1.24 next week.</p> <p><em>This transcript has been lightly edited and condensed for clarity.</em></p> <hr> <p><strong>CRAIG BOX: I'd like to start with what is, of course, on top of everyone's mind at the moment. Let's talk African clawed frogs!</strong></p> <p>REY LEJANO: [CHUCKLES] Oh, you mean <a href="https://en.wikipedia.org/wiki/African_clawed_frog">Xenopus lavis</a>, the scientific name for the African clawed frog?</p> <p><strong>CRAIG BOX: Of course.</strong></p> <p>REY LEJANO: Not many people know, but my background and my degree is actually in microbiology, from the University of California Davis. I did some research for about four years in biochemistry, in a biochemistry lab, and I <a href="https://www.sciencedirect.com/science/article/pii/">do have a research paper published</a>. It's actually on glycoproteins, particularly something called &quot;cortical granule lectin&quot;. We used frogs, because they generate lots and lots of eggs, from which we can extract the protein. That protein prevents polyspermy. When the sperm goes into the egg, the egg releases a glycoprotein, cortical granule lectin, to the membrane, and prevents any other sperm from going inside the egg.</p> <p><strong>CRAIG BOX: Were you able to take anything from the testing that we did on frogs and generalize that to higher-order mammals, perhaps?</strong></p> <p>REY LEJANO: Yes. Since mammals also have cortical granule lectin, we were able to analyze both the convergence and the evolutionary pattern, not just from multiple species of frogs, but also into mammals as well.</p> <p><strong>CRAIG BOX: Now, there's a couple of different threads to unravel here. When you were young, what led you into the fields of biology, and perhaps more the technical side of it?</strong></p> <p>REY LEJANO: I think it was mostly from family, since I do have a family history in the medical field that goes back generations. So I kind of felt like that was the natural path going into college.</p> <p><strong>CRAIG BOX: Now, of course, you're working in a more abstract tech field. What led you out of microbiology?</strong></p> <p>REY LEJANO: [CHUCKLES] Well, I've always been interested in tech. Taught myself a little programming when I was younger, before high school, did some web dev stuff. Just kind of got burnt out being in a lab. I was literally in the basement. I had a great opportunity to join a consultancy that specialized in <a href="https://www.axelos.com/certifications/itil-service-management/what-is-itil">ITIL</a>. I actually started off with application performance management, went into monitoring, went into operation management and also ITIL, which is aligning your IT asset management and service managements with business services. Did that for a good number of years, actually.</p> <p><strong>CRAIG BOX: It's very interesting, as people describe the things that they went through and perhaps the technologies that they worked on, you can pretty much pinpoint how old they might be. There's a lot of people who come into tech these days that have never heard of ITIL. They have no idea what it is. It's basically just SRE with more process.</strong></p> <p>REY LEJANO: Yes, absolutely. It's not very cloud native. [CHUCKLES]</p> <p><strong>CRAIG BOX: Not at all.</strong></p> <p>REY LEJANO: You don't really hear about it in the cloud native landscape. Definitely, you can tell someone's been in the field for a little bit, if they specialize or have worked with ITIL before.</p> <p><strong>CRAIG BOX: You mentioned that you wanted to get out of the basement. That is quite often where people put the programmers. Did they just give you a bit of light in the new basement?</strong></p> <p>REY LEJANO: [LAUGHS] They did give us much better lighting. Able to get some vitamin D sometimes, as well.</p> <p><strong>CRAIG BOX: To wrap up the discussion about your previous career — over the course of the last year, with all of the things that have happened in the world, I could imagine that microbiology skills may be more in demand than perhaps they were when you studied them?</strong></p> <p>REY LEJANO: Oh, absolutely. I could definitely see a big increase of numbers of people going into the field. Also, reading what's going on with the world currently kind of brings back all the education I've learned in the past, as well.</p> <p><strong>CRAIG BOX: Do you keep in touch with people you went through school with?</strong></p> <p>REY LEJANO: Just some close friends, but not in the microbiology field.</p> <p><strong>CRAIG BOX: One thing that I think will probably happen as a result of the pandemic is a renewed interest in some of these STEM fields. It will be interesting to see what impact that has on society at large.</strong></p> <p>REY LEJANO: Yeah. I think that'll be great.</p> <p><strong>CRAIG BOX: You mentioned working at a consultancy doing IT management, application performance monitoring, and so on. When did Kubernetes come into your professional life?</strong></p> <p>REY LEJANO: One of my good friends at the company I worked at, left in mid-2015. He went on to a company that was pretty heavily into Docker. He taught me a little bit. I did my first &quot;docker run&quot; around 2015, maybe 2016. Then, one of the applications we were using for the ITIL framework was containerized around 2018 or so, also in Kubernetes. At that time, it was pretty buggy. That was my initial introduction to Kubernetes and containerised applications.</p> <p>Then I left that company, and I actually joined my friend over at <a href="https://rx-m.com/">RX-M</a>, which is a cloud native consultancy and training firm. They specialize in Docker and Kubernetes. I was able to get my feet wet. I got my CKD, got my CKA as well. And they were really, really great at encouraging us to learn more about Kubernetes and also to be involved in the community.</p> <p><strong>CRAIG BOX: You will have seen, then, the life cycle of people adopting Kubernetes and containerization at large, through your own initial journey and then through helping customers. How would you characterize how that journey has changed from the early days to perhaps today?</strong></p> <p>REY LEJANO: I think the early days, there was a lot of questions of, why do I have to containerize? Why can't I just stay with virtual machines?</p> <p><strong>CRAIG BOX: It's a line item on your CV.</strong></p> <p>REY LEJANO: [CHUCKLES] It is. And nowadays, I think people know the value of using containers, of orchestrating containers with Kubernetes. I don't want to say &quot;jumping on the bandwagon&quot;, but it's become the de-facto standard to orchestrate containers.</p> <p><strong>CRAIG BOX: It's not something that a consultancy needs to go out and pitch to customers that they should be doing. They're just taking it as, that will happen, and starting a bit further down the path, perhaps.</strong></p> <p>REY LEJANO: Absolutely.</p> <p><strong>CRAIG BOX: Working at a consultancy like that, how much time do you get to work on improving process, perhaps for multiple customers, and then looking at how you can upstream that work, versus paid work that you do for just an individual customer at a time?</strong></p> <p>REY LEJANO: Back then, it would vary. They helped me introduce myself, and I learned a lot about the cloud native landscape and Kubernetes itself. They helped educate me as to how the cloud native landscape, and the tools around it, can be used together. My boss at that company, Randy, he actually encouraged us to start contributing upstream, and encouraged me to join the release team. He just said, this is a great opportunity. Definitely helped me with starting with the contributions early on.</p> <p><strong>CRAIG BOX: Was the release team the way that you got involved with upstream Kubernetes contribution?</strong></p> <p>REY LEJANO: Actually, no. My first contribution was with SIG Docs. I met Taylor Dolezal — he was the release team lead for 1.19, but he is involved with SIG Docs as well. I met him at KubeCon 2019, I sat at his table during a luncheon. I remember Paris Pittman was hosting this luncheon at the Marriott. Taylor says he was involved with SIG Docs. He encouraged me to join. I started joining into meetings, started doing a few drive-by PRs. That's what we call them — drive-by — little typo fixes. Then did a little bit more, started to send better or higher quality pull requests, and also reviewing PRs.</p> <p><strong>CRAIG BOX: When did you first formally take your release team role?</strong></p> <p>REY LEJANO: That was in <a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.18/release_team.md">1.18</a>, in December. My boss at the time encouraged me to apply. I did, was lucky enough to get accepted for the release notes shadow. Then from there, stayed in with release notes for a few cycles, then went into Docs, naturally then led Docs, then went to Enhancements, and now I'm the release lead for 1.23.</p> <p><strong>CRAIG BOX: I don't know that a lot of people think about what goes into a good release note. What would you say does?</strong></p> <p>REY LEJANO: [CHUCKLES] You have to tell the end user what has changed or what effect that they might see in the release notes. It doesn't have to be highly technical. It could just be a few lines, and just saying what has changed, what they have to do if they have to do anything as well.</p> <p><strong>CRAIG BOX: As you moved through the process of shadowing, how did you learn from the people who were leading those roles?</strong></p> <p>REY LEJANO: I said this a few times when I was the release lead for this cycle. You get out of the release team as much as you put in, or it directly aligns to how much you put in. I learned a lot. I went into the release team having that mindset of learning from the role leads, learning from the other shadows, as well. That's actually a saying that my first role lead told me. I still carry it to heart, and that was back in 1.18. That was Eddie, in the very first meeting we had, and I still carry it to heart.</p> <p><strong>CRAIG BOX: You, of course, were <a href="https://github.com/kubernetes/sig-release/tree/master/releases/release-1.23">the release lead for 1.23</a>. First of all, congratulations on the release.</strong></p> <p>REY LEJANO: Thank you very much.</p> <p><strong>CRAIG BOX: The theme for this release is <a href="https://kubernetes.io/blog/2021/12/07/kubernetes-1-23-release-announcement/">The Next Frontier</a>. Tell me the story of how we came to the theme and then the logo.</strong></p> <p>REY LEJANO: The Next Frontier represents a few things. It not only represents the next enhancements in this release, but Kubernetes itself also has a history of Star Trek references. The original codename for Kubernetes was Project Seven, a reference to Seven of Nine, originally from Star Trek Voyager. Also the seven spokes in the helm in the logo of Kubernetes as well. And, of course, Borg, the predecessor to Kubernetes.</p> <p>The Next Frontier continues that Star Trek reference. It's a fusion of two titles in the Star Trek universe. One is <a href="https://en.wikipedia.org/wiki/Star_Trek_V:_The_Final_Frontier">Star Trek V, the Final Frontier</a>, and the Star Trek: The Next Generation.</p> <p><strong>CRAIG BOX: Do you have any opinion on the fact that Star Trek V was an odd-numbered movie, and they are <a href="https://screenrant.com/star-trek-movies-odd-number-curse-explained/">canonically referred to as being lesser than the even-numbered ones</a>?</strong></p> <p>REY LEJANO: I can't say, because I am such a sci-fi nerd that I love all of them even though they're bad. Even the post-Next Generation movies, after the series, I still liked all of them, even though I know some weren't that great.</p> <p><strong>CRAIG BOX: Am I right in remembering that Star Trek V was the one directed by William Shatner?</strong></p> <p>REY LEJANO: Yes, that is correct.</p> <p><strong>CRAIG BOX: I think that says it all.</strong></p> <p>REY LEJANO: [CHUCKLES] Yes.</p> <p><strong>CRAIG BOX: Now, I understand that the theme comes from a part of the <a href="https://github.com/kubernetes/community/blob/master/sig-release/charter.md">SIG Release charter</a>?</strong></p> <p>REY LEJANO: Yes. There's a line in the SIG Release charter, &quot;ensure there is a consistent group of community members in place to support the release process across time.&quot; With the release team, we have new shadows that join every single release cycle. With this, we're growing with this community. We're growing the release team members. We're growing SIG Release. We're growing the Kubernetes community itself. For a lot of people, this is their first time contributing to open source, so that's why I say it's their new open source frontier.</p> <p><strong>CRAIG BOX: And the logo is obviously very Star Trek-inspired. It sort of surprised me that it took that long for someone to go this route.</strong></p> <p>REY LEJANO: I was very surprised as well. I had to relearn Adobe Illustrator to create the logo.</p> <p><strong>CRAIG BOX: This your own work, is it?</strong></p> <p>REY LEJANO: This is my own work.</p> <p><strong>CRAIG BOX: It's very nice.</strong></p> <p>REY LEJANO: Thank you very much. Funny, the galaxy actually took me the longest time versus the ship. Took me a few days to get that correct. I'm always fine-tuning it, so there might be a final change when this is actually released.</p> <p><strong>CRAIG BOX: No frontier is ever truly final.</strong></p> <p>REY LEJANO: True, very true.</p> <p><strong>CRAIG BOX: Moving now from the theme of the release to the substance, perhaps, what is new in 1.23?</strong></p> <p>REY LEJANO: We have 47 enhancements. I'm going to run through most of the stable ones, if not all of them, some of the key Beta ones, and a few of the Alpha enhancements for 1.23.</p> <p>One of the key enhancements is <a href="https://github.com/kubernetes/enhancements/issues/563">dual-stack IPv4/IPv6</a>, which went GA in 1.23.</p> <p>Some background info: dual-stack was introduced as Alpha in 1.15. You probably saw a keynote at KubeCon 2019. Back then, the way dual-stack worked was that you needed two services — you needed a service per IP family. You would need a service for IPv4 and a service for IPv6. It was refactored in 1.20. In 1.21, it was in Beta; clusters were enabled to be dual-stack by default.</p> <p>And then in 1.23 we did remove the IPv6 dual-stack feature flag. It's not mandatory to use dual-stack. It's actually not &quot;default&quot; still. The pods, the services still default to single-stack. There are some requirements to be able to use dual-stack. The nodes have to be routable on IPv4 and IPv6 network interfaces. You need a CNI plugin that supports dual-stack. The pods themselves have to be configured to be dual-stack. And the services need the ipFamilyPolicy field to specify prefer dual-stack, or require dual-stack.</p> <p><strong>CRAIG BOX: This sounds like there's an implication in this that v4 is still required. Do you see a world where we can actually move to v6-only clusters?</strong></p> <p>REY LEJANO: I think we'll be talking about IPv4 and IPv6 for many, many years to come. I remember a long time ago, they kept saying &quot;it's going to be all IPv6&quot;, and that was decades ago.</p> <p><strong>CRAIG BOX: I think I may have mentioned on the show before, but there was <a href="https://www.youtube.com/watch?v=AEaJtZVimqs">a meeting in London that Vint Cerf attended</a>, and he gave a public presentation at the time to say, now is the time of v6. And that was 10 years ago at least. It's still not the time of v6, and my desktop still doesn't have Linux on it. One day.</strong></p> <p>REY LEJANO: [LAUGHS] In my opinion, that's one of the big key features that went stable for 1.23.</p> <p>One of the other highlights of 1.23 is <a href="https://kubernetes.io/blog/2021/12/09/pod-security-admission-beta/">pod security admission going to Beta</a>. I know this feature is going to Beta, but I highlight this because as some people might know, PodSecurityPolicy, which was deprecated in 1.21, is targeted to be removed in 1.25. Pod security admission replaces pod security policy. It's an admission controller. It evaluates the pods against a predefined set of pod security standards to either admit or deny the pod for running.</p> <p>There's three levels of pod security standards. Privileged, that's totally open. Baseline, known privileges escalations are minimized. Or Restricted, which is hardened. And you could set pod security standards either to run in three modes, which is enforce: reject any pods that are in violation; to audit: pods are allowed to be created, but the violations are recorded; or warn: it will send a warning message to the user, and the pod is allowed.</p> <p><strong>CRAIG BOX: You mentioned there that PodSecurityPolicy is due to be deprecated in two releases' time. Are we lining up these features so that pod security admission will be GA at that time?</strong></p> <p>REY LEJANO: Yes. Absolutely. I'll talk about that for another feature in a little bit as well. There's also another feature that went to GA. It was an API that went to GA, and therefore the Beta API is now deprecated. I'll talk about that a little bit.</p> <p><strong>CRAIG BOX: All right. Let's talk about what's next on the list.</strong></p> <p>REY LEJANO: Let's move on to more stable enhancements. One is the <a href="https://github.com/kubernetes/enhancements/issues/592">TTL controller</a>. This cleans up jobs and pods after the jobs are finished. There is a TTL timer that starts when the job or pod is finished. This TTL controller watches all the jobs, and ttlSecondsAfterFinished needs to be set. The controller will see if the ttlSecondsAfterFinished, combined with the last transition time, if it's greater than now. If it is, then it will delete the job and the pods of that job.</p> <p><strong>CRAIG BOX: Loosely, it could be called a garbage collector?</strong></p> <p>REY LEJANO: Yes. Garbage collector for pods and jobs, or jobs and pods.</p> <p><strong>CRAIG BOX: If Kubernetes is truly becoming a programming language, it of course has to have a garbage collector implemented.</strong></p> <p>REY LEJANO: Yeah. There's another one, too, coming in Alpha. [CHUCKLES]</p> <p><strong>CRAIG BOX: Tell me about that.</strong></p> <p>REY LEJANO: That one is coming in in Alpha. It's actually one of my favorite features, because there's only a few that I'm going to highlight today. <a href="https://github.com/kubernetes/enhancements/issues/1847">PVCs for StafeulSet will be cleaned up</a>. It will auto-delete PVCs created by StatefulSets, when you delete that StatefulSet.</p> <p><strong>CRAIG BOX: What's next on our tour of stable features?</strong></p> <p>REY LEJANO: Next one is, <a href="https://github.com/kubernetes/enhancements/issues/695">skip volume ownership change goes to stable</a>. This is from SIG Storage. There are times when you're running a stateful application, like many databases, they're sensitive to permission bits changing underneath. Currently, when a volume is bind mounted inside the container, the permissions of that volume will change recursively. It might take a really long time.</p> <p>Now, there's a field, the fsGroupChangePolicy, which allows you, as a user, to tell Kubernetes how you want the permission and ownership change for that volume to happen. You can set it to always, to always change permissions, or just on mismatch, to only do it when the permission ownership changes at the top level is different from what is expected.</p> <p><strong>CRAIG BOX: It does feel like a lot of these enhancements came from a very particular use case where someone said, &quot;hey, this didn't work for me and I've plumbed in a feature that works with exactly the thing I need to have&quot;.</strong></p> <p>REY LEJANO: Absolutely. People create issues for these, then create Kubernetes enhancement proposals, and then get targeted for releases.</p> <p><strong>CRAIG BOX: Another GA feature in this release — ephemeral volumes.</strong></p> <p>REY LEJANO: We've always been able to use empty dir for ephemeral volumes, but now we could actually have <a href="https://github.com/kubernetes/enhancements/issues/1698">ephemeral inline volumes</a>, meaning that you could take your standard CSI driver and be able to use ephemeral volumes with it.</p> <p><strong>CRAIG BOX: And, a long time coming, <a href="https://github.com/kubernetes/enhancements/issues/19">CronJobs</a>.</strong></p> <p>REY LEJANO: CronJobs is a funny one, because it was stable before 1.23. For 1.23, it was still tracked,but it was just cleaning up some of the old controller. With CronJobs, there's a v2 controller. What was cleaned up in 1.23 is just the old v1 controller.</p> <p><strong>CRAIG BOX: Were there any other duplications or major cleanups of note in this release?</strong></p> <p>REY LEJANO: Yeah. There were a few you might see in the major themes. One's a little tricky, around FlexVolumes. This is one of the efforts from SIG Storage. They have an effort to migrate in-tree plugins to CSI drivers. This is a little tricky, because FlexVolumes were actually deprecated in November 2020. We're <a href="https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md#kubernetes-volume-plugin-faq-for-storage-vendors">formally announcing it in 1.23</a>.</p> <p><strong>CRAIG BOX: FlexVolumes, in my mind, predate CSI as a concept. So it's about time to get rid of them.</strong></p> <p>REY LEJANO: Yes, it is. There's another deprecation, just some <a href="https://kubernetes.io/docs/concepts/cluster-administration/system-logs/#klog">klog specific flags</a>, but other than that, there are no other big deprecations in 1.23.</p> <p><strong>CRAIG BOX: The buzzword of the last KubeCon, and in some ways the theme of the last 12 months, has been secure software supply chain. What work is Kubernetes doing to improve in this area?</strong></p> <p>REY LEJANO: For 1.23, Kubernetes is now SLSA compliant at Level 1, which means that provenance attestation files that describe the staging and release phases of the release process are satisfactory for the SLSA framework.</p> <p><strong>CRAIG BOX: What needs to happen to step up to further levels?</strong></p> <p>REY LEJANO: Level 1 means a few things — that the build is scripted; that the provenance is available, meaning that the artifacts are verified and they're handed over from one phase to the next; and describes how the artifact is produced. Level 2 means that the source is version-controlled, which it is, provenance is authenticated, provenance is service-generated, and there is a build service. There are four levels of SLSA compliance.</p> <p><strong>CRAIG BOX: It does seem like the levels were largely influenced by what it takes to build a big, secure project like this. It doesn't seem like it will take a lot of extra work to move up to verifiable provenance, for example. There's probably just a few lines of script required to meet many of those requirements.</strong></p> <p>REY LEJANO: Absolutely. I feel like we're almost there; we'll see what will come out of 1.24. And I do want to give a big shout-out to SIG Release and Release Engineering, primarily to Adolfo García Veytia, who is aka Puerco on GitHub and on Slack. He's been driving this forward.</p> <p><strong>CRAIG BOX: You've mentioned some APIs that are being graduated in time to replace their deprecated version. Tell me about the new HPA API.</strong></p> <p>REY LEJANO: The <a href="https://github.com/kubernetes/enhancements/issues/2702">horizontal pod autoscaler v2 API</a>, is now stable, which means that the v2beta2 API is deprecated. Just for everyone's knowledge, the v1 API is not being deprecated. The difference is that v2 adds support for multiple and custom metrics to be used for HPA.</p> <p><strong>CRAIG BOX: There's also now a facility to validate my CRDs with an expression language.</strong></p> <p>REY LEJANO: Yeah. You can use the <a href="https://github.com/google/cel-spec">Common Expression Language, or CEL</a>, to validate your CRDs, so you no longer need to use webhooks. This also makes the CRDs more self-contained and declarative, because the rules are now kept within the CRD object definition.</p> <p><strong>CRAIG BOX: What new features, perhaps coming in Alpha or Beta, have taken your interest?</strong></p> <p>REY LEJANO: Aside from pod security policies, I really love <a href="https://github.com/kubernetes/enhancements/issues/277">ephemeral containers</a> supporting kubectl debug. It launches an ephemeral container and a running pod, shares those pod namespaces, and you can do all your troubleshooting with just running kubectl debug.</p> <p><strong>CRAIG BOX: There's also been some interesting changes in the way that events are handled with kubectl.</strong></p> <p>REY LEJANO: Yeah. kubectl events has always had some issues, like how things weren't sorted. <a href="https://github.com/kubernetes/enhancements/issues/1440">kubectl events improved</a> that so now you can do <code>--watch</code>, and it will also sort with the <code>--watch</code> option as well. That is something new. You can actually combine fields and custom columns. And also, you can list events in the timeline with doing the last N number of minutes. And you can also sort events using other criteria as well.</p> <p><strong>CRAIG BOX: You are a field engineer at SUSE. Are there any things that are coming in that your individual customers that you deal with are looking out for?</strong></p> <p>REY LEJANO: More of what I look out for to help the customers.</p> <p><strong>CRAIG BOX: Right.</strong></p> <p>REY LEJANO: I really love kubectl events. Really love the PVCs being cleaned up with StatefulSets. Most of it's for selfish reasons that it will improve troubleshooting efforts. [CHUCKLES]</p> <p><strong>CRAIG BOX: I have always hoped that a release team lead would say to me, &quot;yes, I have selfish reasons. And I finally got something I wanted in.&quot;</strong></p> <p>REY LEJANO: [LAUGHS]</p> <p><strong>CRAIG BOX: Perhaps I should run to be release team lead, just so I can finally get init containers fixed once and for all.</strong></p> <p>REY LEJANO: Oh, init containers, I've been looking for that for a while. I've actually created animated GIFs on how init containers will be run with that Kubernetes enhancement proposal, but it's halted currently.</p> <p><strong>CRAIG BOX: One day.</strong></p> <p>REY LEJANO: One day. Maybe I shouldn't stay halted.</p> <p><strong>CRAIG BOX: You mentioned there are obviously the things you look out for. Are there any things that are coming down the line, perhaps Alpha features or maybe even just proposals you've seen lately, that you're personally really looking forward to seeing which way they go?</strong></p> <p>REY LEJANO: Yeah. Oone is a very interesting one, it affects the whole community, so it's not just for personal reasons. As you may have known, Dockershim is deprecated. And we did release a blog that it will be removed in 1.24.</p> <p><strong>CRAIG BOX: Scared a bunch of people.</strong></p> <p>REY LEJANO: Scared a bunch of people. From a survey, we saw that a lot of people are still using Docker and Dockershim. One of the enhancements for 1.23 is, <a href="https://github.com/kubernetes/enhancements/issues/2040">kubelet CRI goes to Beta</a>. This promotes the CRI API, which is required. This had to be in Beta for Dockershim to be removed in 1.24.</p> <p><strong>CRAIG BOX: Now, in the last release team lead interview, <a href="https://kubernetespodcast.com/episode/157-kubernetes-1.22/">we spoke with Savitha Raghunathan</a>, and she talked about what she would advise you as her successor. It was to look out for the mental health of the team members. How were you able to take that advice on board?</strong></p> <p>REY LEJANO: That was great advice from Savitha. A few things I've made note of with each release team meeting. After each release team meeting, I stop the recording, because we do record all the meetings and post them on YouTube. And I open up the floor to anyone who wants to say anything that's not recorded, that's not going to be on the agenda. Also, I tell people not to work on weekends. I broke this rule once, but other than that, I told people it could wait. Just be mindful of your mental health.</p> <p><strong>CRAIG BOX: It's just been announced that <a href="https://twitter.com/JamesLaverack/status/1466834312993644551">James Laverack from Jetstack</a> will be the release team lead for 1.24. James and I shared an interesting Mexican dinner at the last KubeCon in San Diego.</strong></p> <p>REY LEJANO: Oh, nice. I didn't know you knew James.</p> <p><strong>CRAIG BOX: The British tech scene. We're a very small world. What will your advice to James be?</strong></p> <p>REY LEJANO: What I would tell James for 1.24 is use teachable moments in the release team meetings. When you're a shadow for the first time, it's very daunting. It's very difficult, because you don't know the repos. You don't know the release process. Everyone around you seems like they know the release process, and very familiar with what the release process is. But as a first-time shadow, you don't know all the vernacular for the community. I just advise to use teachable moments. Take a few minutes in the release team meetings to make it a little easier for new shadows to ramp up and to be familiar with the release process.</p> <p><strong>CRAIG BOX: Has there been major evolution in the process in the time that you've been involved? Or do you think that it's effectively doing what it needs to do?</strong></p> <p>REY LEJANO: It's always evolving. I remember my first time in release notes, 1.18, we said that our goal was to automate and program our way out so that we don't have a release notes team anymore. That's changed [CHUCKLES] quite a bit. Although there's been significant advancements in the release notes process by Adolfo and also James, they've created a subcommand in krel to generate release notes.</p> <p>But nowadays, all their release notes are richer. Still not there at the automation process yet. Every release cycle, there is something a little bit different. For this release cycle, we had a production readiness review deadline. It was a soft deadline. A production readiness review is a review by several people in the community. It's actually been required since 1.21, and it ensures that the enhancements are observable, scalable, supportable, and it's safe to operate in production, and could also be disabled or rolled back. In 1.23, we had a deadline to have the production readiness review completed by a specific date.</p> <p><strong>CRAIG BOX: How have you found the change of schedule to three releases per year rather than four?</strong></p> <p>REY LEJANO: Moving to three releases a year from four, in my opinion, has been an improvement, because we support the last three releases, and now we can actually support the last releases in a calendar year instead of having 9 months out of 12 months of the year.</p> <p><strong>CRAIG BOX: The next event on the calendar is a <a href="https://www.kubernetes.dev/events/kcc2021/">Kubernetes contributor celebration</a> starting next Monday. What can we expect from that event?</strong></p> <p>REY LEJANO: This is our second time running this virtual event. It's a virtual celebration to recognize the whole community and all of our accomplishments of the year, and also contributors. There's a number of events during this week of celebration. It starts the week of December 13.</p> <p>There's events like the Kubernetes Contributor Awards, where SIGs honor and recognize the hard work of the community and contributors. There's also a DevOps party game as well. There is a cloud native bake-off. I do highly suggest people to go to <a href="https://www.kubernetes.dev/events/past-events/2021/kcc2021/">kubernetes.dev/celebration</a> to learn more.</p> <p><strong>CRAIG BOX: How exactly does one judge a virtual bake-off?</strong></p> <p>REY LEJANO: That I don't know. [CHUCKLES]</p> <p><strong>CRAIG BOX: I tasted my scones. I think they're the best. I rate them 10 out of 10.</strong></p> <p>REY LEJANO: Yeah. That is very difficult to do virtually. I would have to say, probably what the dish is, how closely it is tied with Kubernetes or open source or to CNCF. There's a few judges. I know Josh Berkus and Rin Oliver are a few of the judges running the bake-off.</p> <p><strong>CRAIG BOX: Yes. We spoke with Josh about his love of the kitchen, and so he seems like a perfect fit for that role.</strong></p> <p>REY LEJANO: He is.</p> <p><strong>CRAIG BOX: Finally, your wife and yourself are expecting your first child in January. Have you had a production readiness review for that?</strong></p> <p>REY LEJANO: I think we failed that review. [CHUCKLES]</p> <p><strong>CRAIG BOX: There's still time.</strong></p> <p>REY LEJANO: We are working on refactoring. We're going to refactor a little bit in December, and <code>--apply</code> again.</p> <hr> <p><em><a href="https://twitter.com/reylejano">Rey Lejano</a> is a field engineer at SUSE, by way of Rancher Labs, and was the release team lead for Kubernetes 1.23. He is now also a co-chair for SIG Docs. His son Liam is now 3 and a half months old.</em></p> <p><em>You can find the <a href="http://www.kubernetespodcast.com/">Kubernetes Podcast from Google</a> at <a href="https://twitter.com/KubernetesPod">@KubernetesPod</a> on Twitter, and you can <a href="https://kubernetespodcast.com/subscribe/">subscribe</a> so you never miss an episode.</em></p></description></item><item><title>Blog: Increasing the security bar in Ingress-NGINX v1.2.0</title><link>https://kubernetes.io/blog/2022/04/28/ingress-nginx-1-2-0/</link><pubDate>Thu, 28 Apr 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/04/28/ingress-nginx-1-2-0/</guid><description> <p><strong>Authors:</strong> Ricardo Katz (VMware), James Strong (Chainguard)</p> <p>The <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Ingress</a> may be one of the most targeted components of Kubernetes. An Ingress typically defines an HTTP reverse proxy, exposed to the Internet, containing multiple websites, and with some privileged access to Kubernetes API (such as to read Secrets relating to TLS certificates and their private keys).</p> <p>While it is a risky component in your architecture, it is still the most popular way to properly expose your services.</p> <p>Ingress-NGINX has been part of security assessments that figured out we have a big problem: we don't do all proper sanitization before turning the configuration into an <code>nginx.conf</code> file, which may lead to information disclosure risks.</p> <p>While we understand this risk and the real need to fix this, it's not an easy process to do, so we took another approach to reduce (but not remove!) this risk in the current (v1.2.0) release.</p> <h2 id="meet-ingress-nginx-v1-2-0-and-the-chrooted-nginx-process">Meet Ingress NGINX v1.2.0 and the chrooted NGINX process</h2> <p>One of the main challenges is that Ingress-NGINX runs the web proxy server (NGINX) alongside the Ingress controller (the component that has access to Kubernetes API that and that creates the <code>nginx.conf</code> file).</p> <p>So, NGINX does have the same access to the filesystem of the controller (and Kubernetes service account token, and other configurations from the container). While splitting those components is our end goal, the project needed a fast response; that lead us to the idea of using <code>chroot()</code>.</p> <p>Let's take a look into what an Ingress-NGINX container looked like before this change:</p> <p><img src="ingress-pre-chroot.png" alt="Ingress NGINX pre chroot"></p> <p>As we can see, the same container (not the Pod, the container!) that provides HTTP Proxy is the one that watches Ingress objects and writes the Container Volume</p> <p>Now, meet the new architecture:</p> <p><img src="ingress-post-chroot.png" alt="Ingress NGINX post chroot"></p> <p>What does all of this mean? A basic summary is: that we are isolating the NGINX service as a container inside the controller container.</p> <p>While this is not strictly true, to understand what was done here, it's good to understand how Linux containers (and underlying mechanisms such as kernel namespaces) work. You can read about cgroups in the Kubernetes glossary: <a href="https://kubernetes.io/docs/reference/glossary/?fundamental=true#term-cgroup"><code>cgroup</code></a> and learn more about cgroups interact with namespaces in the NGINX project article <a href="https://www.nginx.com/blog/what-are-namespaces-cgroups-how-do-they-work/">What Are Namespaces and cgroups, and How Do They Work?</a>. (As you read that, bear in mind that Linux kernel namespaces are a different thing from <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/">Kubernetes namespaces</a>).</p> <h2 id="skip-the-talk-what-do-i-need-to-use-this-new-approach">Skip the talk, what do I need to use this new approach?</h2> <p>While this increases the security, we made this feature an opt-in in this release so you can have time to make the right adjustments in your environment(s). This new feature is only available from release v1.2.0 of the Ingress-NGINX controller.</p> <p>There are two required changes in your deployments to use this feature:</p> <ul> <li>Append the suffix &quot;-chroot&quot; to the container image name. For example: <code>gcr.io/k8s-staging-ingress-nginx/controller-chroot:v1.2.0</code></li> <li>In your Pod template for the Ingress controller, find where you add the capability <code>NET_BIND_SERVICE</code> and add the capability <code>SYS_CHROOT</code>. After you edit the manifest, you'll see a snippet like:</li> </ul> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">capabilities</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">drop</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- ALL<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">add</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- NET_BIND_SERVICE<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span>- SYS_CHROOT<span style="color:#bbb"> </span></span></span></code></pre></div><p>If you deploy the controller using the official Helm chart then change the following setting in <code>values.yaml</code>:</p> <div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">controller</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">image</span>:<span style="color:#bbb"> </span></span></span><span style="display:flex;"><span><span style="color:#bbb"> </span><span style="color:#008000;font-weight:bold">chroot</span>:<span style="color:#bbb"> </span><span style="color:#a2f;font-weight:bold">true</span><span style="color:#bbb"> </span></span></span></code></pre></div><p>Ingress controllers are normally set up cluster-wide (the IngressClass API is cluster scoped). If you manage the Ingress-NGINX controller but you're not the overall cluster operator, then check with your cluster admin about whether you can use the <code>SYS_CHROOT</code> capability, <strong>before</strong> you enable it in your deployment.</p> <h2 id="ok-but-how-does-this-increase-the-security-of-my-ingress-controller">OK, but how does this increase the security of my Ingress controller?</h2> <p>Take the following configuration snippet and imagine, for some reason it was added to your <code>nginx.conf</code>:</p> <pre tabindex="0"><code>location /randomthing/ { alias /; autoindex on; } </code></pre><p>If you deploy this configuration, someone can call <code>http://website.example/randomthing</code> and get some listing (and access) to the whole filesystem of the Ingress controller.</p> <p>Now, can you spot the difference between chrooted and non chrooted Nginx on the listings below?</p> <table> <thead> <tr> <th>Without extra <code>chroot()</code></th> <th>With extra <code>chroot()</code></th> </tr> </thead> <tbody> <tr> <td><code>bin</code></td> <td><code>bin</code></td> </tr> <tr> <td><code>dev</code></td> <td><code>dev</code></td> </tr> <tr> <td><code>etc</code></td> <td><code>etc</code></td> </tr> <tr> <td><code>home</code></td> <td></td> </tr> <tr> <td><code>lib</code></td> <td><code>lib</code></td> </tr> <tr> <td><code>media</code></td> <td></td> </tr> <tr> <td><code>mnt</code></td> <td></td> </tr> <tr> <td><code>opt</code></td> <td><code>opt</code></td> </tr> <tr> <td><code>proc</code></td> <td><code>proc</code></td> </tr> <tr> <td><code>root</code></td> <td></td> </tr> <tr> <td><code>run</code></td> <td><code>run</code></td> </tr> <tr> <td><code>sbin</code></td> <td></td> </tr> <tr> <td><code>srv</code></td> <td></td> </tr> <tr> <td><code>sys</code></td> <td></td> </tr> <tr> <td><code>tmp</code></td> <td><code>tmp</code></td> </tr> <tr> <td><code>usr</code></td> <td><code>usr</code></td> </tr> <tr> <td><code>var</code></td> <td><code>var</code></td> </tr> <tr> <td><code>dbg</code></td> <td></td> </tr> <tr> <td><code>nginx-ingress-controller</code></td> <td></td> </tr> <tr> <td><code>wait-shutdown</code></td> <td></td> </tr> </tbody> </table> <p>The one in left side is not chrooted. So NGINX has full access to the filesystem. The one in right side is chrooted, so a new filesystem with only the required files to make NGINX work is created.</p> <h2 id="what-about-other-security-improvements-in-this-release">What about other security improvements in this release?</h2> <p>We know that the new <code>chroot()</code> mechanism helps address some portion of the risk, but still, someone can try to inject commands to read, for example, the <code>nginx.conf</code> file and extract sensitive information.</p> <p>So, another change in this release (this is opt-out!) is the <em>deep inspector</em>. We know that some directives or regular expressions may be dangerous to NGINX, so the deep inspector checks all fields from an Ingress object (during its reconciliation, and also with a <a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook">validating admission webhook</a>) to verify if any fields contains these dangerous directives.</p> <p>The ingress controller already does this for annotations, and our goal is to move this existing validation to happen inside deep inspection as part of a future release.</p> <p>You can take a look into the existing rules in <a href="https://github.com/kubernetes/ingress-nginx/blob/main/internal/ingress/inspector/rules.go">https://github.com/kubernetes/ingress-nginx/blob/main/internal/ingress/inspector/rules.go</a>.</p> <p>Due to the nature of inspecting and matching all strings within relevant Ingress objects, this new feature may consume a bit more CPU. You can disable it by running the ingress controller with the command line argument <code>--deep-inspect=false</code>.</p> <h2 id="what-s-next">What's next?</h2> <p>This is not our final goal. Our final goal is to split the control plane and the data plane processes. In fact, doing so will help us also achieve a <a href="https://gateway-api.sigs.k8s.io/">Gateway</a> API implementation, as we may have a different controller as soon as it &quot;knows&quot; what to provide to the data plane (we need some help here!!)</p> <p>Some other projects in Kubernetes already take this approach (like <a href="https://github.com/kubernetes-sigs/kpng">KPNG</a>, the proposed replacement for <code>kube-proxy</code>), and we plan to align with them and get the same experience for Ingress-NGINX.</p> <h2 id="further-reading">Further reading</h2> <p>If you want to take a look into how chrooting was done in Ingress NGINX, take a look into <a href="https://github.com/kubernetes/ingress-nginx/pull/8337">https://github.com/kubernetes/ingress-nginx/pull/8337</a> The release v1.2.0 containing all the changes can be found at <a href="https://github.com/kubernetes/ingress-nginx/releases/tag/controller-v1.2.0">https://github.com/kubernetes/ingress-nginx/releases/tag/controller-v1.2.0</a></p></description></item><item><title>Blog: Kubernetes Removals and Deprecations In 1.24</title><link>https://kubernetes.io/blog/2022/04/07/upcoming-changes-in-kubernetes-1-24/</link><pubDate>Thu, 07 Apr 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/04/07/upcoming-changes-in-kubernetes-1-24/</guid><description> <p><strong>Author</strong>: Mickey Boxell (Oracle)</p> <p>As Kubernetes evolves, features and APIs are regularly revisited and removed. New features may offer an alternative or improved approach to solving existing problems, motivating the team to remove the old approach.</p> <p>We want to make sure you are aware of the changes coming in the Kubernetes 1.24 release. The release will <strong>deprecate</strong> several (beta) APIs in favor of stable versions of the same APIs. The major change coming in the Kubernetes 1.24 release is the <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim">removal of Dockershim</a>. This is discussed below and will be explored in more depth at release time. For an early look at the changes coming in Kubernetes 1.24, take a look at the in-progress <a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md">CHANGELOG</a>.</p> <h2 id="a-note-about-dockershim">A note about Dockershim</h2> <p>It's safe to say that the removal receiving the most attention with the release of Kubernetes 1.24 is Dockershim. Dockershim was deprecated in v1.20. As noted in the <a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation">Kubernetes 1.20 changelog</a>: &quot;Docker support in the kubelet is now deprecated and will be removed in a future release. The kubelet uses a module called &quot;dockershim&quot; which implements CRI support for Docker and it has seen maintenance issues in the Kubernetes community.&quot; With the upcoming release of Kubernetes 1.24, the Dockershim will finally be removed.</p> <p>In the article <a href="https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/">Don't Panic: Kubernetes and Docker</a>, the authors succinctly captured the change's impact and encouraged users to remain calm:</p> <blockquote> <p>Docker as an underlying runtime is being deprecated in favor of runtimes that use the Container Runtime Interface (CRI) created for Kubernetes. Docker-produced images will continue to work in your cluster with all runtimes, as they always have.</p> </blockquote> <p>Several guides have been created with helpful information about migrating from dockershim to container runtimes that are directly compatible with Kubernetes. You can find them on the <a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/">Migrating from dockershim</a> page in the Kubernetes documentation.</p> <p>For more information about why Kubernetes is moving away from dockershim, check out the aptly named: <a href="https://kubernetes.io/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/">Kubernetes is Moving on From Dockershim</a> and the <a href="https://kubernetes.io/blog/2022/02/17/dockershim-faq/">updated dockershim removal FAQ</a>.</p> <p>Take a look at the <a href="https://kubernetes.io/blog/2022/03/31/ready-for-dockershim-removal/">Is Your Cluster Ready for v1.24?</a> post to learn about how to ensure your cluster continues to work after upgrading from v1.23 to v1.24.</p> <h2 id="the-kubernetes-api-removal-and-deprecation-process">The Kubernetes API removal and deprecation process</h2> <p>Kubernetes contains a large number of components that evolve over time. In some cases, this evolution results in APIs, flags, or entire features, being removed. To prevent users from facing breaking changes, Kubernetes contributors adopted a feature <a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/">deprecation policy</a>. This policy ensures that stable APIs may only be deprecated when a newer stable version of that same API is available and that APIs have a minimum lifetime as indicated by the following stability levels:</p> <ul> <li>Generally available (GA) or stable API versions may be marked as deprecated but must not be removed within a major version of Kubernetes.</li> <li>Beta or pre-release API versions must be supported for 3 releases after deprecation.</li> <li>Alpha or experimental API versions may be removed in any release without prior deprecation notice.</li> </ul> <p>Removals follow the same deprecation policy regardless of whether an API is removed due to a beta feature graduating to stable or because that API was not proven to be successful. Kubernetes will continue to make sure migration options are documented whenever APIs are removed.</p> <p><strong>Deprecated</strong> APIs are those that have been marked for removal in a future Kubernetes release. <strong>Removed</strong> APIs are those that are no longer available for use in current, supported Kubernetes versions after having been deprecated. These removals have been superseded by newer, stable/generally available (GA) APIs.</p> <h2 id="api-removals-deprecations-and-other-changes-for-kubernetes-1-24">API removals, deprecations, and other changes for Kubernetes 1.24</h2> <ul> <li><a href="https://github.com/kubernetes/enhancements/issues/281">Dynamic kubelet configuration</a>: <code>DynamicKubeletConfig</code> is used to enable the dynamic configuration of the kubelet. The <code>DynamicKubeletConfig</code> flag was deprecated in Kubernetes 1.22. In v1.24, this feature gate will be removed from the kubelet. See <a href="https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/">Reconfigure kubelet</a>. Refer to the <a href="https://github.com/kubernetes/enhancements/issues/281">&quot;Dynamic kubelet config is removed&quot; KEP</a> for more information.</li> <li><a href="https://github.com/kubernetes/kubernetes/pull/107207">Dynamic log sanitization</a>: The experimental dynamic log sanitization feature is deprecated and will be removed in v1.24. This feature introduced a logging filter that could be applied to all Kubernetes system components logs to prevent various types of sensitive information from leaking via logs. Refer to <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/1753-logs-sanitization#deprecation">KEP-1753: Kubernetes system components logs sanitization</a> for more information and an <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/1753-logs-sanitization#alternatives=">alternative approach</a>.</li> <li><a href="https://github.com/kubernetes/enhancements/issues/2221">Removing Dockershim from kubelet</a>: the Container Runtime Interface (CRI) for Docker (i.e. Dockershim) is currently a built-in container runtime in the kubelet code base. It was deprecated in v1.20. As of v1.24, the kubelet will no longer have dockershim. Check out this blog on <a href="https://kubernetes.io/blog/2022/03/31/ready-for-dockershim-removal/">what you need to do be ready for v1.24</a>.</li> <li><a href="https://github.com/kubernetes/enhancements/issues/1472">Storage capacity tracking for pod scheduling</a>: The CSIStorageCapacity API supports exposing currently available storage capacity via CSIStorageCapacity objects and enhances scheduling of pods that use CSI volumes with late binding. In v1.24, the CSIStorageCapacity API will be stable. The API graduating to stable initates the deprecation of the v1beta1 CSIStorageCapacity API. Refer to the <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1472-storage-capacity-tracking">Storage Capacity Constraints for Pod Scheduling KEP</a> for more information.</li> <li><a href="https://github.com/kubernetes/kubernetes/pull/107533">The <code>master</code> label is no longer present on kubeadm control plane nodes</a>. For new clusters, the label 'node-role.kubernetes.io/master' will no longer be added to control plane nodes, only the label 'node-role.kubernetes.io/control-plane' will be added. For more information, refer to <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-cluster-lifecycle/kubeadm/2067-rename-master-label-taint">KEP-2067: Rename the kubeadm &quot;master&quot; label and taint</a>.</li> <li><a href="https://github.com/kubernetes/enhancements/issues/177">VolumeSnapshot v1beta1 CRD will be removed</a>. Volume snapshot and restore functionality for Kubernetes and the <a href="https://github.com/container-storage-interface/spec/blob/master/spec.md">Container Storage Interface</a> (CSI), which provides standardized APIs design (CRDs) and adds PV snapshot/restore support for CSI volume drivers, moved to GA in v1.20. VolumeSnapshot v1beta1 was deprecated in v1.20 and will become unsupported with the v1.24 release. Refer to <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/177-volume-snapshot#kep-177-csi-snapshot">KEP-177: CSI Snapshot</a> and the <a href="https://kubernetes.io/blog/2020/12/10/kubernetes-1.20-volume-snapshot-moves-to-ga/">Volume Snapshot GA blog</a> blog article for more information.</li> </ul> <h2 id="what-to-do">What to do</h2> <h3 id="dockershim-removal">Dockershim removal</h3> <p>As stated earlier, there are several guides about <a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/">Migrating from dockershim</a>. You can start with <a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/find-out-runtime-you-use/">Finding what container runtime are on your nodes</a>. If your nodes are using dockershim, there are other possible Docker Engine dependencies such as Pods or third-party tools executing Docker commands or private registries in the Docker configuration file. You can follow the <a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-removal-affects-you/">Check whether Dockershim removal affects you</a> guide to review possible Docker Engine dependencies. Before upgrading to v1.24, you decide to either remain using Docker Engine and <a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/migrate-dockershim-dockerd/">Migrate Docker Engine nodes from dockershim to cri-dockerd</a> or migrate to a CRI-compatible runtime. Here's a guide to <a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/change-runtime-containerd/">change the container runtime on a node from Docker Engine to containerd</a>.</p> <h3 id="kubectl-convert"><code>kubectl convert</code></h3> <p>The <a href="https://kubernetes.io/docs/tasks/tools/included/kubectl-convert-overview/"><code>kubectl convert</code></a> plugin for <code>kubectl</code> can be helpful to address migrating off deprecated APIs. The plugin facilitates the conversion of manifests between different API versions, for example, from a deprecated to a non-deprecated API version. More general information about the API migration process can be found in the <a href="https://kubernetes.io/docs/reference/using-api/deprecation-guide/">Deprecated API Migration Guide</a>. Follow the <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/#install-kubectl-convert-plugin">install <code>kubectl convert</code> plugin</a> documentation to download and install the <code>kubectl-convert</code> binary.</p> <h3 id="looking-ahead">Looking ahead</h3> <p>The Kubernetes 1.25 and 1.26 releases planned for later this year will stop serving beta versions of several currently stable Kubernetes APIs. The v1.25 release will also remove PodSecurityPolicy, which was deprecated with Kubernetes 1.21 and will not graduate to stable. See <a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">PodSecurityPolicy Deprecation: Past, Present, and Future</a> for more information.</p> <p>The official <a href="https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-25">list of API removals planned for Kubernetes 1.25</a> is:</p> <ul> <li>The beta CronJob API (batch/v1beta1)</li> <li>The beta EndpointSlice API (discovery.k8s.io/v1beta1)</li> <li>The beta Event API (events.k8s.io/v1beta1)</li> <li>The beta HorizontalPodAutoscaler API (autoscaling/v2beta1)</li> <li>The beta PodDisruptionBudget API (policy/v1beta1)</li> <li>The beta PodSecurityPolicy API (policy/v1beta1)</li> <li>The beta RuntimeClass API (node.k8s.io/v1beta1)</li> </ul> <p>The official <a href="https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-26">list of API removals planned for Kubernetes 1.26</a> is:</p> <ul> <li>The beta FlowSchema and PriorityLevelConfiguration APIs (flowcontrol.apiserver.k8s.io/v1beta1)</li> <li>The beta HorizontalPodAutoscaler API (autoscaling/v2beta2)</li> </ul> <h3 id="want-to-know-more">Want to know more?</h3> <p>Deprecations are announced in the Kubernetes release notes. You can see the announcements of pending deprecations in the release notes for:</p> <ul> <li><a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.21.md#deprecation">Kubernetes 1.21</a></li> <li><a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.22.md#deprecation">Kubernetes 1.22</a></li> <li><a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.23.md#deprecation">Kubernetes 1.23</a></li> <li>We will formally announce the deprecations that come with <a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md#deprecation">Kubernetes 1.24</a> as part of the CHANGELOG for that release.</li> </ul> <p>For information on the process of deprecation and removal, check out the official Kubernetes <a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/#deprecating-parts-of-the-api">deprecation policy</a> document.</p></description></item><item><title>Blog: Is Your Cluster Ready for v1.24?</title><link>https://kubernetes.io/blog/2022/03/31/ready-for-dockershim-removal/</link><pubDate>Thu, 31 Mar 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/03/31/ready-for-dockershim-removal/</guid><description> <p><strong>Author:</strong> Kat Cosgrove</p> <p>Way back in December of 2020, Kubernetes announced the <a href="https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/">deprecation of Dockershim</a>. In Kubernetes, dockershim is a software shim that allows you to use the entire Docker engine as your container runtime within Kubernetes. In the upcoming v1.24 release, we are removing Dockershim - the delay between deprecation and removal in line with the <a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/">project’s policy</a> of supporting features for at least one year after deprecation. If you are a cluster operator, this guide includes the practical realities of what you need to know going into this release. Also, what do you need to do to ensure your cluster doesn’t fall over!</p> <h2 id="first-does-this-even-affect-you">First, does this even affect you?</h2> <p>If you are rolling your own cluster or are otherwise unsure whether or not this removal affects you, stay on the safe side and <a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-removal-affects-you/">check to see if you have any dependencies on Docker Engine</a>. Please note that using Docker Desktop to build your application containers is not a Docker dependency for your cluster. Container images created by Docker are compliant with the <a href="https://opencontainers.org/">Open Container Initiative (OCI)</a>, a Linux Foundation governance structure that defines industry standards around container formats and runtimes. They will work just fine on any container runtime supported by Kubernetes.</p> <p>If you are using a managed Kubernetes service from a cloud provider, and you haven’t explicitly changed the container runtime, there may be nothing else for you to do. Amazon EKS, Azure AKS, and Google GKE all default to containerd now, though you should make sure they do not need updating if you have any node customizations. To check the runtime of your nodes, follow <a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/find-out-runtime-you-use/">Find Out What Container Runtime is Used on a Node</a>.</p> <p>Regardless of whether you are rolling your own cluster or using a managed Kubernetes service from a cloud provider, you may need to <a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/migrating-telemetry-and-security-agents/">migrate telemetry or security agents that rely on Docker Engine</a>.</p> <h2 id="i-have-a-docker-dependency-what-now">I have a Docker dependency. What now?</h2> <p>If your Kubernetes cluster depends on Docker Engine and you intend to upgrade to Kubernetes v1.24 (which you should eventually do for security and similar reasons), you will need to change your container runtime from Docker Engine to something else or use <a href="https://github.com/Mirantis/cri-dockerd">cri-dockerd</a>. Since <a href="https://containerd.io/">containerd</a> is a graduated CNCF project and the runtime within Docker itself, it’s a safe bet as an alternative container runtime. Fortunately, the Kubernetes project has already documented the process of <a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/change-runtime-containerd/">changing a node’s container runtime</a>, using containerd as an example. Instructions are similar for switching to one of the other supported runtimes.</p> <h2 id="i-want-to-upgrade-kubernetes-and-i-need-to-maintain-compatibility-with-docker-as-a-runtime-what-are-my-options">I want to upgrade Kubernetes, and I need to maintain compatibility with Docker as a runtime. What are my options?</h2> <p>Fear not, you aren’t being left out in the cold and you don’t have to take the security risk of staying on an old version of Kubernetes. Mirantis and Docker have jointly released, and are maintaining, a replacement for dockershim. That replacement is called <a href="https://github.com/Mirantis/cri-dockerd">cri-dockerd</a>. If you do need to maintain compatibility with Docker as a runtime, install cri-dockerd following the instructions in the project’s documentation.</p> <h2 id="is-that-it">Is that it?</h2> <p>Yes. As long as you go into this release aware of the changes being made and the details of your own clusters, and you make sure to communicate clearly with your development teams, it will be minimally dramatic. You may have some changes to make to your cluster, application code, or scripts, but all of these requirements are documented. Switching from using Docker Engine as your runtime to using <a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/">one of the other supported container runtimes</a> effectively means removing the middleman, since the purpose of dockershim is to access the container runtime used by Docker itself. From a practical perspective, this removal is better both for you and for Kubernetes maintainers in the long-run.</p> <p>If you still have questions, please first check the <a href="https://kubernetes.io/blog/2022/02/17/dockershim-faq/">Dockershim Removal FAQ</a>.</p></description></item><item><title>Blog: Meet Our Contributors - APAC (Aus-NZ region)</title><link>https://kubernetes.io/blog/2022/03/16/meet-our-contributors-au-nz-ep-02/</link><pubDate>Wed, 16 Mar 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/03/16/meet-our-contributors-au-nz-ep-02/</guid><description> <p><strong>Authors &amp; Interviewers:</strong> <a href="https://github.com/anubha-v-ardhan">Anubhav Vardhan</a>, <a href="https://github.com/Atharva-Shinde">Atharva Shinde</a>, <a href="https://github.com/AvineshTripathi">Avinesh Tripathi</a>, <a href="https://github.com/bradmccoydev">Brad McCoy</a>, <a href="https://github.com/Debanitrkl">Debabrata Panigrahi</a>, <a href="https://github.com/jayesh-srivastava">Jayesh Srivastava</a>, <a href="https://github.com/verma-kunal">Kunal Verma</a>, <a href="https://github.com/PranshuSrivastava">Pranshu Srivastava</a>, <a href="github.com/Priyankasaggu11929/">Priyanka Saggu</a>, <a href="https://github.com/PurneswarPrasad">Purneswar Prasad</a>, <a href="https://github.com/vedant-kakde">Vedant Kakde</a></p> <hr> <p>Good day, everyone 👋</p> <p>Welcome back to the second episode of the &quot;Meet Our Contributors&quot; blog post series for APAC.</p> <p>This post will feature four outstanding contributors from the Australia and New Zealand regions, who have played diverse leadership and community roles in the Upstream Kubernetes project.</p> <p>So, without further ado, let's get straight to the blog.</p> <h2 id="caleb-woodbine-https-github-com-bobymcbobs"><a href="https://github.com/BobyMCbobs">Caleb Woodbine</a></h2> <p>Caleb Woodbine is currently a member of the ii.nz organisation.</p> <p>He began contributing to the Kubernetes project in 2018 as a member of the Kubernetes Conformance working group. His experience was positive, and he benefited from early guidance from <a href="https://github.com/hh">Hippie Hacker</a>, a fellow contributor from New Zealand.</p> <p>He has made major contributions to Kubernetes project since then through <code>SIG k8s-infra</code> and <code>k8s-conformance</code> working group.</p> <p>Caleb is also a co-organizer of the <a href="https://www.meetup.com/cloudnative-nz/">CloudNative NZ</a> community events, which aim to expand the reach of Kubernetes project throughout New Zealand in order to encourage technical education and improved employment opportunities.</p> <blockquote> <p><em>There need to be more outreach in APAC and the educators and universities must pick up Kubernetes, as they are very slow and about 8+ years out of date. NZ tends to rather pay overseas than educate locals on the latest cloud tech Locally.</em></p> </blockquote> <h2 id="dylan-graham-https-github-com-dylangraham"><a href="https://github.com/DylanGraham">Dylan Graham</a></h2> <p>Dylan Graham is a cloud engineer from Adeliade, Australia. He has been contributing to the upstream Kubernetes project since 2018.</p> <p>He stated that being a part of such a large-scale project was initially overwhelming, but that the community's friendliness and openness assisted him in getting through it.</p> <p>He began by contributing to the project documentation and is now mostly focused on the community support for the APAC region.</p> <p>He believes that consistent attendance at community/project meetings, taking on project tasks, and seeking community guidance as needed can help new aspiring developers become effective contributors.</p> <blockquote> <p><em>The feeling of being a part of a large community is really special. I've met some amazing people, even some before the pandemic in real life :)</em></p> </blockquote> <h2 id="hippie-hacker-https-github-com-hh"><a href="https://github.com/hh">Hippie Hacker</a></h2> <p>Hippie has worked for the CNCF.io as a Strategic Initiatives contractor from New Zealand for almost 5+ years. He is an active contributor to k8s-infra, API conformance testing, Cloud provider conformance submissions, and apisnoop.cncf.io domains of the upstream Kubernetes &amp; CNCF projects.</p> <p>He recounts their early involvement with the Kubernetes project, which began roughly 5 years ago when their firm, ii.nz, demonstrated <a href="https://ii.nz/post/bringing-the-cloud-to-your-community/">network booting from a Raspberry Pi using PXE and running Gitlab in-cluster to install Kubernetes on servers</a>.</p> <p>He describes their own contributing experience as someone who, at first, tried to do all of the hard lifting on their own, but eventually saw the benefit of group contributions which reduced burnout and task division which allowed folks to keep moving forward on their own momentum.</p> <p>He recommends that new contributors use pair programming.</p> <blockquote> <p><em>The cross pollination of approaches and two pairs of eyes on the same work can often yield a much more amplified effect than a PR comment / approval alone can afford.</em></p> </blockquote> <h2 id="nick-young-https-github-com-youngnick"><a href="https://github.com/youngnick">Nick Young</a></h2> <p>Nick Young works at VMware as a technical lead for Contour, a CNCF ingress controller. He was active with the upstream Kubernetes project from the beginning, and eventually became the chair of the LTS working group, where he advocated user concerns. He is currently the SIG Network Gateway API subproject's maintainer.</p> <p>His contribution path was notable in that he began working on major areas of the Kubernetes project early on, skewing his trajectory.</p> <p>He asserts the best thing a new contributor can do is to &quot;start contributing&quot;. Naturally, if it is relevant to their employment, that is excellent; however, investing non-work time in contributing can pay off in the long run in terms of work. He believes that new contributors, particularly those who are currently Kubernetes users, should be encouraged to participate in higher-level project discussions.</p> <blockquote> <p><em>Just being active and contributing will get you a long way. Once you've been active for a while, you'll find that you're able to answer questions, which will mean you're asked questions, and before you know it you are an expert.</em></p> </blockquote> <hr> <p>If you have any recommendations/suggestions for who we should interview next, please let us know in #sig-contribex. Your suggestions would be much appreciated. We're thrilled to have additional folks assisting us in reaching out to even more wonderful individuals of the community.</p> <p>We'll see you all in the next one. Everyone, till then, have a happy contributing! 👋</p></description></item><item><title>Blog: Updated: Dockershim Removal FAQ</title><link>https://kubernetes.io/blog/2022/02/17/dockershim-faq/</link><pubDate>Thu, 17 Feb 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/02/17/dockershim-faq/</guid><description> <p><strong>This supersedes the original <a href="https://kubernetes.io/blog/2020/12/02/dockershim-faq/">Dockershim Deprecation FAQ</a> article, published in late 2020. The article includes updates from the v1.24 release of Kubernetes.</strong></p> <hr> <p>This document goes over some frequently asked questions regarding the removal of <em>dockershim</em> from Kubernetes. The removal was originally <a href="https://kubernetes.io/blog/2020/12/08/kubernetes-1-20-release-announcement/">announced</a> as a part of the Kubernetes v1.20 release. The Kubernetes <a href="https://kubernetes.io/releases/#release-v1-24">v1.24 release</a> actually removed the dockershim from Kubernetes.</p> <p>For more on what that means, check out the blog post <a href="https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/">Don't Panic: Kubernetes and Docker</a>.</p> <p>To determine the impact that the removal of dockershim would have for you or your organization, you can read <a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-removal-affects-you/">Check whether dockershim removal affects you</a>.</p> <p>In the months and days leading up to the Kubernetes 1.24 release, Kubernetes contributors worked hard to try to make this a smooth transition.</p> <ul> <li>A blog post detailing our <a href="https://kubernetes.io/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/">commitment and next steps</a>.</li> <li>Checking if there were major blockers to migration to <a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/#container-runtimes">other container runtimes</a>.</li> <li>Adding a <a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/">migrating from dockershim</a> guide.</li> <li>Creating a list of <a href="https://kubernetes.io/docs/reference/node/topics-on-dockershim-and-cri-compatible-runtimes/">articles on dockershim removal and on using CRI-compatible runtimes</a>. That list includes some of the already mentioned docs, and also covers selected external sources (including vendor guides).</li> </ul> <h3 id="why-was-the-dockershim-removed-from-kubernetes">Why was the dockershim removed from Kubernetes?</h3> <p>Early versions of Kubernetes only worked with a specific container runtime: Docker Engine. Later, Kubernetes added support for working with other container runtimes. The CRI standard was <a href="https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/">created</a> to enable interoperability between orchestrators (like Kubernetes) and many different container runtimes. Docker Engine doesn't implement that interface (CRI), so the Kubernetes project created special code to help with the transition, and made that <em>dockershim</em> code part of Kubernetes itself.</p> <p>The dockershim code was always intended to be a temporary solution (hence the name: shim). You can read more about the community discussion and planning in the <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim">Dockershim Removal Kubernetes Enhancement Proposal</a>. In fact, maintaining dockershim had become a heavy burden on the Kubernetes maintainers.</p> <p>Additionally, features that were largely incompatible with the dockershim, such as cgroups v2 and user namespaces are being implemented in these newer CRI runtimes. Removing the dockershim from Kubernetes allows further development in those areas.</p> <h3 id="are-docker-and-containers-the-same-thing">Are Docker and containers the same thing?</h3> <p>Docker popularized the Linux containers pattern and has been instrumental in developing the underlying technology, however containers in Linux have existed for a long time. The container ecosystem has grown to be much broader than just Docker. Standards like OCI and CRI have helped many tools grow and thrive in our ecosystem, some replacing aspects of Docker while others enhance existing functionality.</p> <h3 id="will-my-existing-container-images-still-work">Will my existing container images still work?</h3> <p>Yes, the images produced from <code>docker build</code> will work with all CRI implementations. All your existing images will still work exactly the same.</p> <h4 id="what-about-private-images">What about private images?</h4> <p>Yes. All CRI runtimes support the same pull secrets configuration used in Kubernetes, either via the PodSpec or ServiceAccount.</p> <h3 id="can-i-still-use-docker-engine-in-kubernetes-1-23">Can I still use Docker Engine in Kubernetes 1.23?</h3> <p>Yes, the only thing changed in 1.20 is a single warning log printed at <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/">kubelet</a> startup if using Docker Engine as the runtime. You'll see this warning in all versions up to 1.23. The dockershim removal occurred in Kubernetes 1.24.</p> <p>If you're running Kubernetes v1.24 or later, see <a href="#can-i-still-use-docker-engine-as-my-container-runtime">Can I still use Docker Engine as my container runtime?</a>. (Remember, you can switch away from the dockershim if you're using any supported Kubernetes release; from release v1.24, you <strong>must</strong> switch as Kubernetes no longer includes the dockershim).</p> <h3 id="which-cri-implementation-should-i-use">Which CRI implementation should I use?</h3> <p>That’s a complex question and it depends on a lot of factors. If Docker Engine is working for you, moving to containerd should be a relatively easy swap and will have strictly better performance and less overhead. However, we encourage you to explore all the options from the <a href="https://landscape.cncf.io/card-mode?category=container-runtime&amp;grouping=category">CNCF landscape</a> in case another would be an even better fit for your environment.</p> <h4 id="can-i-still-use-docker-engine-as-my-container-runtime">Can I still use Docker Engine as my container runtime?</h4> <p>First off, if you use Docker on your own PC to develop or test containers: nothing changes. You can still use Docker locally no matter what container runtime(s) you use for your Kubernetes clusters. Containers make this kind of interoperability possible.</p> <p>Mirantis and Docker have <a href="https://www.mirantis.com/blog/mirantis-to-take-over-support-of-kubernetes-dockershim-2/">committed</a> to maintaining a replacement adapter for Docker Engine, and to maintain that adapter even after the in-tree dockershim is removed from Kubernetes. The replacement adapter is named <a href="https://github.com/Mirantis/cri-dockerd"><code>cri-dockerd</code></a>.</p> <p>You can install <code>cri-dockerd</code> and use it to connect the kubelet to Docker Engine. Read <a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/migrate-dockershim-dockerd/">Migrate Docker Engine nodes from dockershim to cri-dockerd</a> to learn more.</p> <h3 id="are-there-examples-of-folks-using-other-runtimes-in-production-today">Are there examples of folks using other runtimes in production today?</h3> <p>All Kubernetes project produced artifacts (Kubernetes binaries) are validated with each release.</p> <p>Additionally, the <a href="https://kind.sigs.k8s.io/">kind</a> project has been using containerd for some time and has seen an improvement in stability for its use case. Kind and containerd are leveraged multiple times every day to validate any changes to the Kubernetes codebase. Other related projects follow a similar pattern as well, demonstrating the stability and usability of other container runtimes. As an example, OpenShift 4.x has been using the <a href="https://cri-o.io/">CRI-O</a> runtime in production since June 2019.</p> <p>For other examples and references you can look at the adopters of containerd and CRI-O, two container runtimes under the Cloud Native Computing Foundation (<a href="https://cncf.io">CNCF</a>).</p> <ul> <li><a href="https://github.com/containerd/containerd/blob/master/ADOPTERS.md">containerd</a></li> <li><a href="https://github.com/cri-o/cri-o/blob/master/ADOPTERS.md">CRI-O</a></li> </ul> <h3 id="people-keep-referencing-oci-what-is-that">People keep referencing OCI, what is that?</h3> <p>OCI stands for the <a href="https://opencontainers.org/about/overview/">Open Container Initiative</a>, which standardized many of the interfaces between container tools and technologies. They maintain a standard specification for packaging container images (OCI image-spec) and running containers (OCI runtime-spec). They also maintain an actual implementation of the runtime-spec in the form of <a href="https://github.com/opencontainers/runc">runc</a>, which is the underlying default runtime for both <a href="https://containerd.io/">containerd</a> and <a href="https://cri-o.io/">CRI-O</a>. The CRI builds on these low-level specifications to provide an end-to-end standard for managing containers.</p> <h3 id="what-should-i-look-out-for-when-changing-cri-implementations">What should I look out for when changing CRI implementations?</h3> <p>While the underlying containerization code is the same between Docker and most CRIs (including containerd), there are a few differences around the edges. Some common things to consider when migrating are:</p> <ul> <li>Logging configuration</li> <li>Runtime resource limitations</li> <li>Node provisioning scripts that call docker or use Docker Engine via its control socket</li> <li>Plugins for <code>kubectl</code> that require the <code>docker</code> CLI or the Docker Engine control socket</li> <li>Tools from the Kubernetes project that require direct access to Docker Engine (for example: the deprecated <code>kube-imagepuller</code> tool)</li> <li>Configuration of functionality like <code>registry-mirrors</code> and insecure registries</li> <li>Other support scripts or daemons that expect Docker Engine to be available and are run outside of Kubernetes (for example, monitoring or security agents)</li> <li>GPUs or special hardware and how they integrate with your runtime and Kubernetes</li> </ul> <p>If you use Kubernetes resource requests/limits or file-based log collection DaemonSets then they will continue to work the same, but if you've customized your <code>dockerd</code> configuration, you’ll need to adapt that for your new container runtime where possible.</p> <p>Another thing to look out for is anything expecting to run for system maintenance or nested inside a container when building images will no longer work. For the former, you can use the <a href="https://github.com/kubernetes-sigs/cri-tools"><code>crictl</code></a> tool as a drop-in replacement (see <a href="https://kubernetes.io/docs/tasks/debug/debug-cluster/crictl/#mapping-from-docker-cli-to-crictl">mapping from docker cli to crictl</a>) and for the latter you can use newer container build options like <a href="https://github.com/genuinetools/img">img</a>, <a href="https://github.com/containers/buildah">buildah</a>, <a href="https://github.com/GoogleContainerTools/kaniko">kaniko</a>, or <a href="https://github.com/vmware-tanzu/buildkit-cli-for-kubectl">buildkit-cli-for-kubectl</a> that don’t require Docker.</p> <p>For containerd, you can start with their <a href="https://github.com/containerd/cri/blob/master/docs/registry.md">documentation</a> to see what configuration options are available as you migrate things over.</p> <p>For instructions on how to use containerd and CRI-O with Kubernetes, see the Kubernetes documentation on <a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/">Container Runtimes</a>.</p> <h3 id="what-if-i-have-more-questions">What if I have more questions?</h3> <p>If you use a vendor-supported Kubernetes distribution, you can ask them about upgrade plans for their products. For end-user questions, please post them to our end user community forum: <a href="https://discuss.kubernetes.io/">https://discuss.kubernetes.io/</a>.</p> <p>You can discuss the decision to remove dockershim via a dedicated <a href="https://github.com/kubernetes/kubernetes/issues/106917">GitHub issue</a>.</p> <p>You can also check out the excellent blog post <a href="https://dev.to/inductor/wait-docker-is-deprecated-in-kubernetes-now-what-do-i-do-e4m">Wait, Docker is deprecated in Kubernetes now?</a> a more in-depth technical discussion of the changes.</p> <h3 id="is-there-any-tooling-that-can-help-me-find-dockershim-in-use">Is there any tooling that can help me find dockershim in use?</h3> <p>Yes! The <a href="https://github.com/aws-containers/kubectl-detector-for-docker-socket">Detector for Docker Socket (DDS)</a> is a kubectl plugin that you can install and then use to check your cluster. DDS can detect if active Kubernetes workloads are mounting the Docker Engine socket (<code>docker.sock</code>) as a volume. Find more details and usage patterns in the DDS project's <a href="https://github.com/aws-containers/kubectl-detector-for-docker-socket">README</a>.</p> <h3 id="can-i-have-a-hug">Can I have a hug?</h3> <p>Yes, we're still giving hugs as requested. 🤗🤗🤗</p></description></item><item><title>Blog: SIG Node CI Subproject Celebrates Two Years of Test Improvements</title><link>https://kubernetes.io/blog/2022/02/16/sig-node-ci-subproject-celebrates/</link><pubDate>Wed, 16 Feb 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/02/16/sig-node-ci-subproject-celebrates/</guid><description> <p><strong>Authors:</strong> Sergey Kanzhelev (Google), Elana Hashman (Red Hat)</p> <p>Ensuring the reliability of SIG Node upstream code is a continuous effort that takes a lot of behind-the-scenes effort from many contributors. There are frequent releases of Kubernetes, base operating systems, container runtimes, and test infrastructure that result in a complex matrix that requires attention and steady investment to &quot;keep the lights on.&quot; In May 2020, the Kubernetes node special interest group (&quot;SIG Node&quot;) organized a new subproject for continuous integration (CI) for node-related code and tests. Since its inauguration, the SIG Node CI subproject has run a weekly meeting, and even the full hour is often not enough to complete triage of all bugs, test-related PRs and issues, and discuss all related ongoing work within the subgroup.</p> <p>Over the past two years, we've fixed merge-blocking and release-blocking tests, reducing time to merge Kubernetes contributors' pull requests thanks to reduced test flakes. When we started, Node test jobs only passed 42% of the time, and through our efforts, we now ensure a consistent &gt;90% job pass rate. We've closed 144 test failure issues and merged 176 pull requests just in kubernetes/kubernetes. And we've helped subproject participants ascend the Kubernetes contributor ladder, with 3 new org members, 6 new reviewers, and 2 new approvers.</p> <p>The Node CI subproject is an approachable first stop to help new contributors get started with SIG Node. There is a low barrier to entry for new contributors to address high-impact bugs and test fixes, although there is a long road before contributors can climb the entire contributor ladder: it took over a year to establish two new approvers for the group. The complexity of all the different components that power Kubernetes nodes and its test infrastructure requires a sustained investment over a long period for developers to deeply understand the entire system, both at high and low levels of detail.</p> <p>We have several regular contributors at our meetings, however; our reviewers and approvers pool is still small. It is our goal to continue to grow contributors to ensure a sustainable distribution of work that does not just fall to a few key approvers.</p> <p>It's not always obvious how subprojects within SIGs are formed, operate, and work. Each is unique to its sponsoring SIG and tailored to the projects that the group is intended to support. As a group that has welcomed many first-time SIG Node contributors, we'd like to share some of the details and accomplishments over the past two years, helping to demystify our inner workings and celebrate the hard work of all our dedicated contributors!</p> <h2 id="timeline">Timeline</h2> <p><em><strong>May 2020.</strong></em> SIG Node CI group was formed on May 11, 2020, with more than <a href="https://docs.google.com/document/d/1fb-ugvgdSVIkkuJ388_nhp2pBTy_4HEVg5848Xy7n5U/edit#bookmark=id.vsb8pqnf4gib">30 volunteers</a> signed up, to improve SIG Node CI signal and overall observability. Victor Pickard focused on getting <a href="https://testgrid.k8s.io/sig-node">testgrid jobs</a> passing when Ning Liao suggested forming a group around this effort and came up with the <a href="https://docs.google.com/document/d/1yS-XoUl6GjZdjrwxInEZVHhxxLXlTIX2CeWOARmD8tY/edit#heading=h.te6sgum6s8uf">original group charter document</a>. The SIG Node chairs sponsored group creation with Victor as a subproject lead. Sergey Kanzhelev joined Victor shortly after as a co-lead.</p> <p>At the kick-off meeting, we discussed which tests to concentrate on fixing first and discussed merge-blocking and release-blocking tests, many of which were failing due to infrastructure issues or buggy test code.</p> <p>The subproject launched weekly hour-long meetings to discuss ongoing work discussion and triage.</p> <p><em><strong>June 2020.</strong></em> Morgan Bauer, Karan Goel, and Jorge Alarcon Ochoa were recognized as reviewers for the SIG Node CI group for their contributions, helping significantly with the early stages of the subproject. David Porter and Roy Yang also joined the SIG test failures GitHub team.</p> <p><em><strong>August 2020.</strong></em> All merge-blocking and release-blocking tests were passing, with some flakes. However, only 42% of all SIG Node test jobs were green, as there were many flakes and failing tests.</p> <p><em><strong>October 2020.</strong></em> Amim Knabben becomes a Kubernetes org member for his contributions to the subproject.</p> <p><em><strong>January 2021.</strong></em> With healthy presubmit and critical periodic jobs passing, the subproject discussed its goal for cleaning up the rest of periodic tests and ensuring they passed without flakes.</p> <p>Elana Hashman joined the subproject, stepping up to help lead it after Victor's departure.</p> <p><em><strong>February 2021.</strong></em> Artyom Lukianov becomes a Kubernetes org member for his contributions to the subproject.</p> <p><em><strong>August 2021.</strong></em> After SIG Node successfully ran a <a href="https://groups.google.com/g/kubernetes-dev/c/w2ghO4ihje0/m/VeEql1LJBAAJ">bug scrub</a> to clean up its bug backlog, the scope of the meeting was extended to include bug triage to increase overall reliability, anticipating issues before they affect the CI signal.</p> <p>Subproject leads Elana Hashman and Sergey Kanzhelev are both recognized as approvers on all node test code, supported by SIG Node and SIG Testing.</p> <p><em><strong>September 2021.</strong></em> After significant deflaking progress with serial tests in the 1.22 release spearheaded by Francesco Romani, the subproject set a goal for getting the serial job fully passing by the 1.23 release date.</p> <p>Mike Miranda becomes a Kubernetes org member for his contributions to the subproject.</p> <p><em><strong>November 2021.</strong></em> Throughout 2021, SIG Node had no merge or release-blocking test failures. Many flaky tests from past releases are removed from release-blocking dashboards as they had been fully cleaned up.</p> <p>Danielle Lancashire was recognized as a reviewer for SIG Node's subgroup, test code.</p> <p>The final node serial tests were completely fixed. The serial tests consist of many disruptive and slow tests which tend to be flakey and are hard to troubleshoot. By the 1.23 release freeze, the last serial tests were fixed and the job was passing without flakes.</p> <p><a href="https://kubernetes.slack.com/archives/C0BP8PW9G/p1638211041322900"><img src="serial-tests-green.png" alt="Slack announcement that Serial tests are green"></a></p> <p>The 1.23 release got a special shout out for the tests quality and CI signal. The SIG Node CI subproject was proud to have helped contribute to such a high-quality release, in part due to our efforts in identifying and fixing flakes in Node and beyond.</p> <p><a href="https://kubernetes.slack.com/archives/C92G08FGD/p1637175755023200"><img src="release-mostly-green.png" alt="Slack shoutout that release was mostly green"></a></p> <p><em><strong>December 2021.</strong></em> An estimated 90% of test jobs were passing at the time of the 1.23 release (up from 42% in August 2020).</p> <p>Dockershim code was removed from Kubernetes. This affected nearly half of SIG Node's test jobs and the SIG Node CI subproject reacted quickly and retargeted all the tests. SIG Node was the first SIG to complete test migrations off dockershim, providing examples for other affected SIGs. The vast majority of new jobs passed at the time of introduction without further fixes required. The <a href="https://k8s.io/dockershim">effort of removing dockershim</a>) from Kubernetes is ongoing. There are still some wrinkles from the dockershim removal as we uncover more dependencies on dockershim, but we plan to stabilize all test jobs by the 1.24 release.</p> <h2 id="statistics">Statistics</h2> <p>Our regular meeting attendees and subproject participants for the past few months:</p> <ul> <li>Aditi Sharma</li> <li>Artyom Lukianov</li> <li>Arnaud Meukam</li> <li>Danielle Lancashire</li> <li>David Porter</li> <li>Davanum Srinivas</li> <li>Elana Hashman</li> <li>Francesco Romani</li> <li>Matthias Bertschy</li> <li>Mike Miranda</li> <li>Paco Xu</li> <li>Peter Hunt</li> <li>Ruiwen Zhao</li> <li>Ryan Phillips</li> <li>Sergey Kanzhelev</li> <li>Skyler Clark</li> <li>Swati Sehgal</li> <li>Wenjun Wu</li> </ul> <p>The <a href="https://github.com/kubernetes/test-infra/">kubernetes/test-infra</a> source code repository contains test definitions. The number of Node PRs just in that repository:</p> <ul> <li>2020 PRs (since May): <a href="https://github.com/kubernetes/test-infra/pulls?q=is%3Apr+is%3Aclosed+label%3Asig%2Fnode+created%3A2020-05-01..2020-12-31+-author%3Ak8s-infra-ci-robot+">183</a></li> <li>2021 PRs: <a href="https://github.com/kubernetes/test-infra/pulls?q=is%3Apr+is%3Aclosed+label%3Asig%2Fnode+created%3A2021-01-01..2021-12-31+-author%3Ak8s-infra-ci-robot+">264</a></li> </ul> <p>Triaged issues and PRs on CI board (including triaging away from the subgroup scope):</p> <ul> <li>2020 (since May): <a href="https://github.com/issues?q=project%3Akubernetes%2F43+created%3A2020-05-01..2020-12-31">132</a></li> <li>2021: <a href="https://github.com/issues?q=project%3Akubernetes%2F43+created%3A2021-01-01..2021-12-31+">532</a></li> </ul> <h2 id="future">Future</h2> <p>Just &quot;keeping the lights on&quot; is a bold task and we are committed to improving this experience. We are working to simplify the triage and review processes for SIG Node.</p> <p>Specifically, we are working on better test organization, naming, and tracking:</p> <ul> <li><a href="https://github.com/kubernetes/enhancements/pull/3042">https://github.com/kubernetes/enhancements/pull/3042</a></li> <li><a href="https://github.com/kubernetes/test-infra/issues/24641">https://github.com/kubernetes/test-infra/issues/24641</a></li> <li><a href="https://docs.google.com/spreadsheets/d/1IwONkeXSc2SG_EQMYGRSkfiSWNk8yWLpVhPm-LOTbGM/edit#gid=0">Kubernetes SIG-Node CI Testgrid Tracker</a></li> </ul> <p>We are also constantly making progress on improved tests debuggability and de-flaking.</p> <p>If any of this interests you, we'd love for you to join us! There's plenty to learn in debugging test failures, and it will help you gain familiarity with the code that SIG Node maintains.</p> <p>You can always find information about the group on the <a href="https://github.com/kubernetes/community/tree/master/sig-node">SIG Node</a> page. We give group updates at our maintainer track sessions, such as <a href="https://kccnceu2021.sched.com/event/iE8E/kubernetes-sig-node-intro-and-deep-dive-elana-hashman-red-hat-sergey-kanzhelev-google">KubeCon + CloudNativeCon Europe 2021</a> and <a href="https://kccncna2021.sched.com/event/lV9D/kubenetes-sig-node-intro-and-deep-dive-elana-hashman-derek-carr-red-hat-sergey-kanzhelev-dawn-chen-google?iframe=no&amp;w=100%25&amp;sidebar=yes&amp;bg=no">KubeCon + CloudNative North America 2021</a>. Join us in our mission to keep the kubelet and other SIG Node components reliable and ensure smooth and uneventful releases!</p></description></item><item><title>Blog: Spotlight on SIG Multicluster</title><link>https://kubernetes.io/blog/2022/02/07/sig-multicluster-spotlight-2022/</link><pubDate>Mon, 07 Feb 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/02/07/sig-multicluster-spotlight-2022/</guid><description> <p><strong>Authors:</strong> Dewan Ahmed (Aiven) and Chris Short (AWS)</p> <h2 id="introduction">Introduction</h2> <p><a href="https://github.com/kubernetes/community/tree/master/sig-multicluster">SIG Multicluster</a> is the SIG focused on how Kubernetes concepts are expanded and used beyond the cluster boundary. Historically, Kubernetes resources only interacted within that boundary - KRU or Kubernetes Resource Universe (not an actual Kubernetes concept). Kubernetes clusters, even now, don't really know anything about themselves or, about other clusters. Absence of cluster identifiers is a case in point. With the growing adoption of multicloud and multicluster deployments, the work SIG Multicluster doing is gaining a lot of attention. In this blog, <a href="https://twitter.com/jeremyot">Jeremy Olmsted-Thompson, Google</a> and <a href="https://twitter.com/ChrisShort">Chris Short, AWS</a> discuss the interesting problems SIG Multicluster is solving and how you can get involved. Their initials <strong>JOT</strong> and <strong>CS</strong> will be used for brevity.</p> <h2 id="a-summary-of-their-conversation">A summary of their conversation</h2> <p><strong>CS</strong>: How long has the SIG Multicluster existed and how was the SIG in its infancy? How long have you been with this SIG?</p> <p><strong>JOT</strong>: I've been around for almost two years in the SIG Multicluster. All I know about the infancy years is from the lore but even in the early days, it was always about solving this same problem. Early efforts have been things like <a href="https://github.com/kubernetes-sigs/kubefed">KubeFed</a>. I think there are still folks using KubeFed but it's a smaller slice. Back then, I think people out there deploying large numbers of Kubernetes clusters were really not at a point where we had a ton of real concrete use cases. Projects like KubeFed and <a href="https://github.com/kubernetes-retired/cluster-registry">Cluster Registry</a> were developed around that time and the need back then can be associated to these projects. The motivation for these projects were how do we solve the problems that we think people are <strong>going to have</strong>, when they start expanding to multiple clusters. Honestly, in some ways, it was trying to do too much at that time.</p> <p><strong>CS</strong>: How does KubeFed differ from the current state of SIG Multicluster? How does the <strong>lore</strong> differ from the <strong>now</strong>?</p> <p><strong>JOT</strong>: Yeah, it was like trying to get ahead of potential problems instead of addressing specific problems. I think towards the end of 2019, there was a slow down in SIG multicluster work and we kind of picked it back up with one of the most active recent projects that is the <a href="https://github.com/kubernetes-sigs/mcs-api">SIG Multicluster services (MCS)</a>.</p> <p>Now this is the shift to solving real specific problems. For example,</p> <blockquote> <p>I've got workloads that are spread across multiple clusters and I need them to talk to each other.</p> </blockquote> <p>Okay, that's very straightforward and we know that we need to solve that. To get started, let's make sure that these projects can work together on a common API so you get the same kind of portability that you get with Kubernetes.</p> <p>There's a few implementations of the MCS API out there and more are being developed. But, we didn't build an implementation because depending on how you're deploying things there could be hundreds of implementations. As long as you only need the basic Multicluster service functionality, it'll just work on whatever background you want, whether it's Submariner, GKE, or a service mesh.</p> <p>My favorite example of &quot;then vs. now&quot; is cluster ID. A few years ago, there was an effort to define a cluster ID. A lot of really good thought went into this concept, for example, how do we make a cluster ID is unique across multiple clusters. How do we make this ID globally unique so it'll work in every contact? Let's say, there's an acquisition or merger of teams - does the cluster IDs still remain unique for those teams?</p> <p>With Multicluster services, we found the need for an actual cluster ID, and it has a very specific need. To address this specific need, we're no longer considering every single Kubernetes cluster out there rather the ClusterSets - a grouping of clusters that work together in some kind of bounds. That's a much narrower scope than considering clusters everywhere in time and space. It also leaves flexibility for an implementer to define the boundary (a ClusterSet) beyond which this cluster ID will no longer be unique.</p> <p><strong>CS</strong>: How do you feel about the current state of SIG Multicluster versus where you're hoping to be in future?</p> <p><strong>JOT</strong>: There's a few projects that are kind of getting started, for example, Work API. In the future, I think that some common practices around how do we deploy things across clusters are going to develop.</p> <blockquote> <p>If I have clusters deployed in a bunch of different regions; what's the best way to actually do that?</p> </blockquote> <p>The answer is, almost always, &quot;it depends&quot;. Why are you doing this? Is it because there's some kind of compliance that makes you care about locality? Is it performance? Is it availability?</p> <p>I think revisiting registry patterns will probably be a natural step after we have cluster IDs, that is, how do you actually associate these clusters together? Maybe you've got a distributed deployment that you run in your own data centers all over the world. I imagine that expanding the API in that space is going to be important as more multi cluster features develop. It really depends on what the community starts doing with these tools.</p> <p><strong>CS</strong>: In the early days of Kubernetes, we used to have a few large Kubernetes clusters and now we're dealing with many small Kubernetes clusters - even multiple clusters for our own dev environments. How has this shift from a few large clusters to many small clusters affected the SIG? Has it accelerated the work or make it challenging in any way?</p> <p><strong>JOT</strong>: I think that it has created a lot of ambiguity that needs solving. Originally, you'd have a dev cluster, a staging cluster, and a prod cluster. When the multi region thing came in, we started needing dev/staging/prod clusters, per region. And then, sometimes clusters really need more isolation due to compliance or some regulations issues. Thus, we're ending up with a lot of clusters. I think figuring out the right balance on how many clusters should you actually have is important. The power of Kubernetes is being able to deploy a lot of things managed by a single control plane. So, it's not like every single workload that gets deployed should be in its own cluster. But I think it's pretty clear that we can't put every single workload in a single cluster.</p> <p><strong>CS</strong>: What are some of your favorite things about this SIG?</p> <p><strong>JOT</strong>: The complexity of the problems, the people and the newness of the space. We don't have right answers and we have to figure this out. At the beginning, we couldn't even think about multi clusters because there was no way to connect services across clusters. Now there is and we're starting to go tackle those problems, I think that this is a really fun place to be in because I expect that the SIG is going to get a lot busier the next couple of years. It's a very collaborative group and we definitely would like more people to come join us, get involved, raise their problems and bring their ideas.</p> <p><strong>CS</strong>: What do you think keeps people in this group? How has the pandemic affected you?</p> <p><strong>JOT</strong>: I think it definitely got a little bit quieter during the pandemic. But for the most part; it's a very distributed group so whether you're calling in to our weekly meetings from a conference room or from your home, it doesn't make that huge of a difference. During the pandemic, a lot of people had time to focus on what's next for their scale and growth. I think that's what keeps people in the group - we have real problems that need to be solved which are very new in this space. And it's fun :)</p> <h2 id="wrap-up">Wrap up</h2> <p><strong>CS</strong>: That's all we have for today. Thanks Jeremy for your time.</p> <p><strong>JOT</strong>: Thanks Chris. Everybody is welcome at our <a href="https://github.com/kubernetes/community/tree/master/sig-multicluster#meetings">bi-weekly meetings</a>. We love as many people to come as possible and welcome all questions and all ideas. It's a new space and it'd be great to grow the community.</p></description></item><item><title>Blog: Securing Admission Controllers</title><link>https://kubernetes.io/blog/2022/01/19/secure-your-admission-controllers-and-webhooks/</link><pubDate>Wed, 19 Jan 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/01/19/secure-your-admission-controllers-and-webhooks/</guid><description> <p><strong>Author:</strong> Rory McCune (Aqua Security)</p> <p><a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/">Admission control</a> is a key part of Kubernetes security, alongside authentication and authorization. Webhook admission controllers are extensively used to help improve the security of Kubernetes clusters in a variety of ways including restricting the privileges of workloads and ensuring that images deployed to the cluster meet organization’s security requirements.</p> <p>However, as with any additional component added to a cluster, security risks can present themselves. A security risk example is if the deployment and management of the admission controller are not handled correctly. To help admission controller users and designers manage these risks appropriately, the <a href="https://github.com/kubernetes/community/tree/master/sig-security#security-docs">security documentation</a> subgroup of SIG Security has spent some time developing a <a href="https://github.com/kubernetes/sig-security/tree/main/sig-security-docs/papers/admission-control">threat model for admission controllers</a>. This threat model looks at likely risks which may arise from the incorrect use of admission controllers, which could allow security policies to be bypassed, or even allow an attacker to get unauthorised access to the cluster.</p> <p>From the threat model, we developed a set of security best practices that should be adopted to ensure that cluster operators can get the security benefits of admission controllers whilst avoiding any risks from using them.</p> <h2 id="admission-controllers-and-good-practices-for-security">Admission controllers and good practices for security</h2> <p>From the threat model, a couple of themes emerged around how to ensure the security of admission controllers.</p> <h3 id="secure-webhook-configuration">Secure webhook configuration</h3> <p>It’s important to ensure that any security component in a cluster is well configured and admission controllers are no different here. There are a couple of security best practices to consider when using admission controllers</p> <ul> <li><strong>Correctly configured TLS for all webhook traffic</strong>. Communications between the API server and the admission controller webhook should be authenticated and encrypted to ensure that attackers who may be in a network position to view or modify this traffic cannot do so. To achieve this access the API server and webhook must be using certificates from a trusted certificate authority so that they can validate their mutual identities</li> <li><strong>Only authenticated access allowed</strong>. If an attacker can send an admission controller large numbers of requests, they may be able to overwhelm the service causing it to fail. Ensuring all access requires strong authentication should mitigate that risk.</li> <li><strong>Admission controller fails closed</strong>. This is a security practice that has a tradeoff, so whether a cluster operator wants to configure it will depend on the cluster’s threat model. If an admission controller fails closed, when the API server can’t get a response from it, all deployments will fail. This stops attackers bypassing the admission controller by disabling it, but, can disrupt the cluster’s operation. As clusters can have multiple webhooks, one approach to hit a middle ground might be to have critical controls on a fail closed setups and less critical controls allowed to fail open.</li> <li><strong>Regular reviews of webhook configuration</strong>. Configuration mistakes can lead to security issues, so it’s important that the admission controller webhook configuration is checked to make sure the settings are correct. This kind of review could be done automatically by an Infrastructure As Code scanner or manually by an administrator.</li> </ul> <h3 id="secure-cluster-configuration-for-admission-control">Secure cluster configuration for admission control</h3> <p>In most cases, the admission controller webhook used by a cluster will be installed as a workload in the cluster. As a result, it’s important to ensure that Kubernetes' security features that could impact its operation are well configured.</p> <ul> <li><strong>Restrict <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/">RBAC</a> rights</strong>. Any user who has rights which would allow them to modify the configuration of the webhook objects or the workload that the admission controller uses could disrupt its operation. So it’s important to make sure that only cluster administrators have those rights.</li> <li><strong>Prevent privileged workloads</strong>. One of the realities of container systems is that if a workload is given certain privileges, it will be possible to break out to the underlying cluster node and impact other containers on that node. Where admission controller services run in the cluster they’re protecting, it’s important to ensure that any requirement for privileged workloads is carefully reviewed and restricted as much as possible.</li> <li><strong>Strictly control external system access</strong>. As a security service in a cluster admission controller systems will have access to sensitive information like credentials. To reduce the risk of this information being sent outside the cluster, <a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">network policies</a> should be used to restrict the admission controller services access to external networks.</li> <li><strong>Each cluster has a dedicated webhook</strong>. Whilst it may be possible to have admission controller webhooks that serve multiple clusters, there is a risk when using that model that an attack on the webhook service would have a larger impact where it’s shared. Also where multiple clusters use an admission controller there will be increased complexity and access requirements, making it harder to secure.</li> </ul> <h3 id="admission-controller-rules">Admission controller rules</h3> <p>A key element of any admission controller used for Kubernetes security is the rulebase it uses. The rules need to be able to accurately meet their goals avoiding false positive and false negative results.</p> <ul> <li><strong>Regularly test and review rules</strong>. Admission controller rules need to be tested to ensure their accuracy. They also need to be regularly reviewed as the Kubernetes API will change with each new version, and rules need to be assessed with each Kubernetes release to understand any changes that may be required to keep them up to date.</li> </ul></description></item><item><title>Blog: Meet Our Contributors - APAC (India region)</title><link>https://kubernetes.io/blog/2022/01/10/meet-our-contributors-india-ep-01/</link><pubDate>Mon, 10 Jan 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/01/10/meet-our-contributors-india-ep-01/</guid><description> <p><strong>Authors &amp; Interviewers:</strong> <a href="https://github.com/anubha-v-ardhan">Anubhav Vardhan</a>, <a href="https://github.com/Atharva-Shinde">Atharva Shinde</a>, <a href="https://github.com/AvineshTripathi">Avinesh Tripathi</a>, <a href="https://github.com/Debanitrkl">Debabrata Panigrahi</a>, <a href="https://github.com/verma-kunal">Kunal Verma</a>, <a href="https://github.com/PranshuSrivastava">Pranshu Srivastava</a>, <a href="https://github.com/CIPHERTron">Pritish Samal</a>, <a href="https://github.com/PurneswarPrasad">Purneswar Prasad</a>, <a href="https://github.com/vedant-kakde">Vedant Kakde</a></p> <p><strong>Editor:</strong> <a href="https://psaggu.com">Priyanka Saggu</a></p> <hr> <p>Good day, everyone 👋</p> <p>Welcome to the first episode of the APAC edition of the &quot;Meet Our Contributors&quot; blog post series.</p> <p>In this post, we'll introduce you to five amazing folks from the India region who have been actively contributing to the upstream Kubernetes projects in a variety of ways, as well as being the leaders or maintainers of numerous community initiatives.</p> <p>💫 <em>Let's get started, so without further ado…</em></p> <h2 id="arsh-sharma-https-github-com-rinkiyakedad"><a href="https://github.com/RinkiyaKeDad">Arsh Sharma</a></h2> <p>Arsh is currently employed with Okteto as a Developer Experience engineer. As a new contributor, he realised that 1:1 mentorship opportunities were quite beneficial in getting him started with the upstream project.</p> <p>He is presently a CI Signal shadow on the Kubernetes 1.23 release team. He is also contributing to the SIG Testing and SIG Docs projects, as well as to the <a href="https://github.com/cert-manager/infrastructure">cert-manager</a> tools development work that is being done under the aegis of SIG Architecture.</p> <p>To the newcomers, Arsh helps plan their early contributions sustainably.</p> <blockquote> <p><em>I would encourage folks to contribute in a way that's sustainable. What I mean by that is that it's easy to be very enthusiastic early on and take up more stuff than one can actually handle. This can often lead to burnout in later stages. It's much more sustainable to work on things iteratively.</em></p> </blockquote> <h2 id="kunal-kushwaha-https-github-com-kunal-kushwaha"><a href="https://github.com/kunal-kushwaha">Kunal Kushwaha</a></h2> <p>Kunal Kushwaha is a core member of the Kubernetes marketing council. He is also a CNCF ambassador and one of the founders of the <a href="https://community.cncf.io/cloud-native-students/">CNCF Students Program</a>.. He also served as a Communications role shadow during the 1.22 release cycle.</p> <p>At the end of his first year, Kunal began contributing to the <a href="https://github.com/fabric8io/kubernetes-client">fabric8io kubernetes-client</a> project. He was then selected to work on the same project as part of Google Summer of Code. Kunal mentored people on the same project, first through Google Summer of Code then through Google Code-in.</p> <p>As an open-source enthusiast, he believes that diverse participation in the community is beneficial since it introduces new perspectives and opinions and respect for one's peers. He has worked on various open-source projects, and his participation in communities has considerably assisted his development as a developer.</p> <blockquote> <p><em>I believe if you find yourself in a place where you do not know much about the project, that's a good thing because now you can learn while contributing and the community is there to help you. It has helped me a lot in gaining skills, meeting people from around the world and also helping them. You can learn on the go, you don't have to be an expert. Make sure to also check out no code contributions because being a beginner is a skill and you can bring new perspectives to the organisation.</em></p> </blockquote> <h2 id="madhav-jivarajani-https-github-com-madhavjivrajani"><a href="https://github.com/MadhavJivrajani">Madhav Jivarajani</a></h2> <p>Madhav Jivarajani works on the VMware Upstream Kubernetes stability team. He began contributing to the Kubernetes project in January 2021 and has since made significant contributions to several areas of work under SIG Architecture, SIG API Machinery, and SIG ContribEx (contributor experience).</p> <p>Among several significant contributions are his recent efforts toward the Archival of <a href="https://github.com/kubernetes/community/issues/6055">design proposals</a>, refactoring the <a href="https://github.com/kubernetes/k8s.io/pull/2713">&quot;groups&quot; codebase</a> under k8s-infra repository to make it mockable and testable, and improving the functionality of the <a href="https://github.com/kubernetes/test-infra/issues/23129">GitHub k8s bot</a>.</p> <p>In addition to his technical efforts, Madhav oversees many projects aimed at assisting new contributors. He organises bi-weekly &quot;KEP reading club&quot; sessions to help newcomers understand the process of adding new features, deprecating old ones, and making other key changes to the upstream project. He has also worked on developing <a href="https://github.com/kubernetes-sigs/contributor-katacoda">Katacoda scenarios</a> to assist new contributors to become acquainted with the process of contributing to k/k. In addition to his current efforts to meet with community members every week, he has organised several <a href="https://www.youtube.com/watch?v=FgsXbHBRYIc">new contributors workshops (NCW)</a>.</p> <blockquote> <p><em>I initially did not know much about Kubernetes. I joined because the community was super friendly. But what made me stay was not just the people, but the project itself. My solution to not feeling overwhelmed in the community was to gain as much context and knowledge into the topics that I was interested in and were being discussed. And as a result I continued to dig deeper into Kubernetes and the design of it. I am a systems nut &amp; thus Kubernetes was an absolute goldmine for me.</em></p> </blockquote> <h2 id="rajas-kakodkar-https-github-com-rajaskakodkar"><a href="https://github.com/rajaskakodkar">Rajas Kakodkar</a></h2> <p>Rajas Kakodkar currently works at VMware as a Member of Technical Staff. He has been engaged in many aspects of the upstream Kubernetes project since 2019.</p> <p>He is now a key contributor to the Testing special interest group. He is also active in the SIG Network community. Lately, Rajas has contributed significantly to the <a href="https://docs.google.com/document/d/1AtWQy2fNa4qXRag9cCp5_HsefD7bxKe3ea2RPn8jnSs/">NetworkPolicy++</a> and <a href="https://github.com/kubernetes-sigs/kpng"><code>kpng</code></a> sub-projects.</p> <p>One of the first challenges he ran across was that he was in a different time zone than the upstream project's regular meeting hours. However, async interactions on community forums progressively corrected that problem.</p> <blockquote> <p><em>I enjoy contributing to Kubernetes not just because I get to work on cutting edge tech but more importantly because I get to work with awesome people and help in solving real world problems.</em></p> </blockquote> <h2 id="rajula-vineet-reddy-https-github-com-rajula96reddy"><a href="https://github.com/rajula96reddy">Rajula Vineet Reddy</a></h2> <p>Rajula Vineet Reddy, a Junior Engineer at CERN, is a member of the Marketing Council team under SIG ContribEx . He also served as a release shadow for SIG Release during the 1.22 and 1.23 Kubernetes release cycles.</p> <p>He started looking at the Kubernetes project as part of a university project with the help of one of his professors. Over time, he spent a significant amount of time reading the project's documentation, Slack discussions, GitHub issues, and blogs, which helped him better grasp the Kubernetes project and piqued his interest in contributing upstream. One of his key contributions was his assistance with automation in the SIG ContribEx Upstream Marketing subproject.</p> <p>According to Rajula, attending project meetings and shadowing various project roles are vital for learning about the community.</p> <blockquote> <p><em>I find the community very helpful and it's always</em> “you get back as much as you contribute”. <em>The more involved you are, the more you will understand, get to learn and contribute new things.</em></p> <p><em>The first step to</em> “come forward and start” <em>is hard. But it's all gonna be smooth after that. Just take that jump.</em></p> </blockquote> <hr> <p>If you have any recommendations/suggestions for who we should interview next, please let us know in #sig-contribex. We're thrilled to have other folks assisting us in reaching out to even more wonderful individuals of the community. Your suggestions would be much appreciated.</p> <p>We'll see you all in the next one. Everyone, till then, have a happy contributing! 👋</p></description></item><item><title>Blog: Kubernetes is Moving on From Dockershim: Commitments and Next Steps</title><link>https://kubernetes.io/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/</link><pubDate>Fri, 07 Jan 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/</guid><description> <p><strong>Authors:</strong> Sergey Kanzhelev (Google), Jim Angel (Google), Davanum Srinivas (VMware), Shannon Kularathna (Google), Chris Short (AWS), Dawn Chen (Google)</p> <p>Kubernetes is removing dockershim in the upcoming v1.24 release. We're excited to reaffirm our community values by supporting open source container runtimes, enabling a smaller kubelet, and increasing engineering velocity for teams using Kubernetes. If you <a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/find-out-runtime-you-use/">use Docker Engine as a container runtime</a> for your Kubernetes cluster, get ready to migrate in 1.24! To check if you're affected, refer to <a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-removal-affects-you/">Check whether dockershim removal affects you</a>.</p> <h2 id="why-we-re-moving-away-from-dockershim">Why we’re moving away from dockershim</h2> <p>Docker was the first container runtime used by Kubernetes. This is one of the reasons why Docker is so familiar to many Kubernetes users and enthusiasts. Docker support was hardcoded into Kubernetes – a component the project refers to as dockershim. As containerization became an industry standard, the Kubernetes project added support for additional runtimes. This culminated in the implementation of the container runtime interface (CRI), letting system components (like the kubelet) talk to container runtimes in a standardized way. As a result, dockershim became an anomaly in the Kubernetes project. Dependencies on Docker and dockershim have crept into various tools and projects in the CNCF ecosystem ecosystem, resulting in fragile code.</p> <p>By removing the dockershim CRI, we're embracing the first value of CNCF: &quot;<a href="https://github.com/cncf/foundation/blob/master/charter.md#3-values">Fast is better than slow</a>&quot;. Stay tuned for future communications on the topic!</p> <h2 id="deprecation-timeline">Deprecation timeline</h2> <p>We <a href="https://kubernetes.io/blog/2020/12/08/kubernetes-1-20-release-announcement/">formally announced</a> the dockershim deprecation in December 2020. Full removal is targeted in Kubernetes 1.24, in April 2022. This timeline aligns with our <a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/#deprecating-a-feature-or-behavior">deprecation policy</a>, which states that deprecated behaviors must function for at least 1 year after their announced deprecation.</p> <p>We'll support Kubernetes version 1.23, which includes dockershim, for another year in the Kubernetes project. For managed Kubernetes providers, vendor support is likely to last even longer, but this is dependent on the companies themselves. Regardless, we're confident all cluster operations will have time to migrate. If you have more questions about the dockershim removal, refer to the <a href="https://kubernetes.io/dockershim">Dockershim Deprecation FAQ</a>.</p> <p>We asked you whether you feel prepared for the migration from dockershim in this survey: <a href="https://kubernetes.io/blog/2021/11/12/are-you-ready-for-dockershim-removal/">Are you ready for Dockershim removal</a>. We had over 600 responses. To everybody who took time filling out the survey, thank you.</p> <p>The results show that we still have a lot of ground to cover to help you to migrate smoothly. Other container runtimes exist, and have been promoted extensively. However, many users told us they still rely on dockershim, and sometimes have dependencies that need to be re-worked. Some of these dependencies are outside of your control. Based on your feedback, here are some of the steps we are taking to help.</p> <h2 id="our-next-steps">Our next steps</h2> <p>Based on the feedback you provided:</p> <ul> <li>CNCF and the 1.24 release team are committed to delivering documentation in time for the 1.24 release. This includes more informative blog posts like this one, updating existing code samples, tutorials, and tasks, and producing a migration guide for cluster operators.</li> <li>We are reaching out to the rest of the CNCF community to help prepare them for this change.</li> </ul> <p>If you're part of a project with dependencies on dockershim, or if you're interested in helping with the migration effort, please join us! There's always room for more contributors, whether to our transition tools or to our documentation. To get started, say hello in the <a href="https://kubernetes.slack.com/archives/C0BP8PW9G">#sig-node</a> channel on <a href="https://slack.kubernetes.io/">Kubernetes Slack</a>!</p> <h2 id="final-thoughts">Final thoughts</h2> <p>As a project, we've already seen cluster operators increasingly adopt other container runtimes through 2021. We believe there are no major blockers to migration. The steps we're taking to improve the migration experience will light the path more clearly for you.</p> <p>We understand that migration from dockershim is yet another action you may need to do to keep your Kubernetes infrastructure up to date. For most of you, this step will be straightforward and transparent. In some cases, you will encounter hiccups or issues. The community has discussed at length whether postponing the dockershim removal would be helpful. For example, we recently talked about it in the <a href="https://docs.google.com/document/d/1Ne57gvidMEWXR70OxxnRkYquAoMpt56o75oZtg-OeBg/edit#bookmark=id.r77y11bgzid">SIG Node discussion on November 11th</a> and in the <a href="https://docs.google.com/document/d/1qazwMIHGeF3iUh5xMJIJ6PDr-S3bNkT8tNLRkSiOkOU/edit#bookmark=id.m0ir406av7jx">Kubernetes Steering committee meeting held on December 6th</a>. We already <a href="https://github.com/kubernetes/enhancements/pull/2481/">postponed</a> it once in 2021 because the adoption rate of other runtimes was lower than we wanted, which also gave us more time to identify potential blocking issues.</p> <p>At this point, we believe that the value that you (and Kubernetes) gain from dockershim removal makes up for the migration effort you'll have. Start planning now to avoid surprises. We'll have more updates and guides before Kubernetes 1.24 is released.</p></description></item></channel></rss>